{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7JhBErVnGnop",
    "outputId": "824df593-61d6-41ad-a786-ab90f42cd0f9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fatal: destination path 'Monotonic-Early-Exit' already exists and is not an empty directory.\n"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/Thiggel/Monotonic-Early-Exit.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6ExIPX0Ql0pC",
    "outputId": "6ec9ddd9-487e-4e4b-b666-e8dcc04584a7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/content/Monotonic-Early-Exit\n"
     ]
    }
   ],
   "source": [
    "%cd Monotonic-Early-Exit/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "u-jXJ-_Sl8wQ",
    "outputId": "36cacfda-3c60-4fb3-f4b1-b48f079486ff"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Already on 'getting_classifiers_to_run'\n",
      "Your branch is up to date with 'origin/getting_classifiers_to_run'.\n"
     ]
    }
   ],
   "source": [
    "!git checkout getting_classifiers_to_run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "eQ_1qyFtm3ZZ",
    "outputId": "32442aa5-30c6-4b28-d713-85a50e3d112f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting accelerate>=0.12.0 (from -r requirements.txt (line 1))\n",
      "  Downloading accelerate-0.30.1-py3-none-any.whl.metadata (18 kB)\n",
      "Requirement already satisfied: torch>=1.13.0 in /opt/miniconda3/lib/python3.12/site-packages (from -r requirements.txt (line 2)) (2.2.2)\n",
      "Collecting datasets>=1.8.0 (from -r requirements.txt (line 3))\n",
      "  Using cached datasets-2.19.1-py3-none-any.whl.metadata (19 kB)\n",
      "Collecting sentencepiece!=0.1.92 (from -r requirements.txt (line 4))\n",
      "  Using cached sentencepiece-0.2.0-cp312-cp312-macosx_10_9_x86_64.whl.metadata (7.7 kB)\n",
      "Requirement already satisfied: protobuf in /opt/miniconda3/lib/python3.12/site-packages (from -r requirements.txt (line 5)) (5.26.1)\n",
      "Collecting evaluate (from -r requirements.txt (line 6))\n",
      "  Downloading evaluate-0.4.2-py3-none-any.whl.metadata (9.3 kB)\n",
      "Requirement already satisfied: scikit-learn in /opt/miniconda3/lib/python3.12/site-packages (from -r requirements.txt (line 7)) (1.4.2)\n",
      "Collecting rouge-score (from -r requirements.txt (line 8))\n",
      "  Using cached rouge_score-0.1.2-py3-none-any.whl\n",
      "Requirement already satisfied: nltk in /opt/miniconda3/lib/python3.12/site-packages (from -r requirements.txt (line 9)) (3.8.1)\n",
      "Collecting py7zr (from -r requirements.txt (line 10))\n",
      "  Using cached py7zr-0.21.0-py3-none-any.whl.metadata (17 kB)\n",
      "Collecting sacrebleu>=1.4.12 (from -r requirements.txt (line 11))\n",
      "  Using cached sacrebleu-2.4.2-py3-none-any.whl.metadata (58 kB)\n",
      "Collecting transformers==4.28.1 (from -r requirements.txt (line 12))\n",
      "  Using cached transformers-4.28.1-py3-none-any.whl.metadata (109 kB)\n",
      "Collecting protobuf (from -r requirements.txt (line 5))\n",
      "  Using cached protobuf-3.20.1-py2.py3-none-any.whl.metadata (720 bytes)\n",
      "Requirement already satisfied: matplotlib in /opt/miniconda3/lib/python3.12/site-packages (from -r requirements.txt (line 14)) (3.8.4)\n",
      "Collecting peft (from -r requirements.txt (line 15))\n",
      "  Downloading peft-0.11.1-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: filelock in /opt/miniconda3/lib/python3.12/site-packages (from transformers==4.28.1->-r requirements.txt (line 12)) (3.13.4)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.11.0 in /opt/miniconda3/lib/python3.12/site-packages (from transformers==4.28.1->-r requirements.txt (line 12)) (0.22.2)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/miniconda3/lib/python3.12/site-packages (from transformers==4.28.1->-r requirements.txt (line 12)) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/miniconda3/lib/python3.12/site-packages (from transformers==4.28.1->-r requirements.txt (line 12)) (23.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/miniconda3/lib/python3.12/site-packages (from transformers==4.28.1->-r requirements.txt (line 12)) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/miniconda3/lib/python3.12/site-packages (from transformers==4.28.1->-r requirements.txt (line 12)) (2024.4.16)\n",
      "Requirement already satisfied: requests in /opt/miniconda3/lib/python3.12/site-packages (from transformers==4.28.1->-r requirements.txt (line 12)) (2.31.0)\n",
      "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1 (from transformers==4.28.1->-r requirements.txt (line 12))\n",
      "  Using cached tokenizers-0.13.3.tar.gz (314 kB)\n",
      "  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /opt/miniconda3/lib/python3.12/site-packages (from transformers==4.28.1->-r requirements.txt (line 12)) (4.65.0)\n",
      "Requirement already satisfied: psutil in /opt/miniconda3/lib/python3.12/site-packages (from accelerate>=0.12.0->-r requirements.txt (line 1)) (5.9.8)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in /opt/miniconda3/lib/python3.12/site-packages (from accelerate>=0.12.0->-r requirements.txt (line 1)) (0.4.3)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /opt/miniconda3/lib/python3.12/site-packages (from torch>=1.13.0->-r requirements.txt (line 2)) (4.11.0)\n",
      "Requirement already satisfied: sympy in /opt/miniconda3/lib/python3.12/site-packages (from torch>=1.13.0->-r requirements.txt (line 2)) (1.12)\n",
      "Requirement already satisfied: networkx in /opt/miniconda3/lib/python3.12/site-packages (from torch>=1.13.0->-r requirements.txt (line 2)) (3.3)\n",
      "Requirement already satisfied: jinja2 in /opt/miniconda3/lib/python3.12/site-packages (from torch>=1.13.0->-r requirements.txt (line 2)) (3.1.3)\n",
      "Requirement already satisfied: fsspec in /opt/miniconda3/lib/python3.12/site-packages (from torch>=1.13.0->-r requirements.txt (line 2)) (2024.3.1)\n",
      "Collecting pyarrow>=12.0.0 (from datasets>=1.8.0->-r requirements.txt (line 3))\n",
      "  Downloading pyarrow-16.1.0-cp312-cp312-macosx_10_15_x86_64.whl.metadata (3.0 kB)\n",
      "Collecting pyarrow-hotfix (from datasets>=1.8.0->-r requirements.txt (line 3))\n",
      "  Using cached pyarrow_hotfix-0.6-py3-none-any.whl.metadata (3.6 kB)\n",
      "Collecting dill<0.3.9,>=0.3.0 (from datasets>=1.8.0->-r requirements.txt (line 3))\n",
      "  Using cached dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
      "Requirement already satisfied: pandas in /opt/miniconda3/lib/python3.12/site-packages (from datasets>=1.8.0->-r requirements.txt (line 3)) (2.2.2)\n",
      "Collecting xxhash (from datasets>=1.8.0->-r requirements.txt (line 3))\n",
      "  Using cached xxhash-3.4.1-cp312-cp312-macosx_10_9_x86_64.whl.metadata (12 kB)\n",
      "Collecting multiprocess (from datasets>=1.8.0->-r requirements.txt (line 3))\n",
      "  Using cached multiprocess-0.70.16-py312-none-any.whl.metadata (7.2 kB)\n",
      "Requirement already satisfied: aiohttp in /opt/miniconda3/lib/python3.12/site-packages (from datasets>=1.8.0->-r requirements.txt (line 3)) (3.9.5)\n",
      "Requirement already satisfied: scipy>=1.6.0 in /opt/miniconda3/lib/python3.12/site-packages (from scikit-learn->-r requirements.txt (line 7)) (1.13.0)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /opt/miniconda3/lib/python3.12/site-packages (from scikit-learn->-r requirements.txt (line 7)) (1.4.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /opt/miniconda3/lib/python3.12/site-packages (from scikit-learn->-r requirements.txt (line 7)) (3.4.0)\n",
      "Requirement already satisfied: absl-py in /opt/miniconda3/lib/python3.12/site-packages (from rouge-score->-r requirements.txt (line 8)) (2.1.0)\n",
      "Requirement already satisfied: six>=1.14.0 in /opt/miniconda3/lib/python3.12/site-packages (from rouge-score->-r requirements.txt (line 8)) (1.16.0)\n",
      "Requirement already satisfied: click in /opt/miniconda3/lib/python3.12/site-packages (from nltk->-r requirements.txt (line 9)) (8.1.7)\n",
      "Collecting texttable (from py7zr->-r requirements.txt (line 10))\n",
      "  Using cached texttable-1.7.0-py2.py3-none-any.whl.metadata (9.8 kB)\n",
      "Collecting pycryptodomex>=3.16.0 (from py7zr->-r requirements.txt (line 10))\n",
      "  Using cached pycryptodomex-3.20.0-cp35-abi3-macosx_10_9_x86_64.whl.metadata (3.4 kB)\n",
      "Collecting pyzstd>=0.15.9 (from py7zr->-r requirements.txt (line 10))\n",
      "  Downloading pyzstd-0.16.0-cp312-cp312-macosx_10_9_x86_64.whl.metadata (2.4 kB)\n",
      "Collecting pyppmd<1.2.0,>=1.1.0 (from py7zr->-r requirements.txt (line 10))\n",
      "  Using cached pyppmd-1.1.0-cp312-cp312-macosx_10_9_x86_64.whl.metadata (5.7 kB)\n",
      "Collecting pybcj<1.1.0,>=1.0.0 (from py7zr->-r requirements.txt (line 10))\n",
      "  Using cached pybcj-1.0.2-cp312-cp312-macosx_10_9_x86_64.whl.metadata (4.0 kB)\n",
      "Collecting multivolumefile>=0.2.3 (from py7zr->-r requirements.txt (line 10))\n",
      "  Using cached multivolumefile-0.2.3-py3-none-any.whl.metadata (6.3 kB)\n",
      "Collecting inflate64<1.1.0,>=1.0.0 (from py7zr->-r requirements.txt (line 10))\n",
      "  Using cached inflate64-1.0.0-cp312-cp312-macosx_10_9_x86_64.whl.metadata (4.0 kB)\n",
      "Collecting brotli>=1.1.0 (from py7zr->-r requirements.txt (line 10))\n",
      "  Using cached Brotli-1.1.0-cp312-cp312-macosx_10_9_x86_64.whl.metadata (5.5 kB)\n",
      "Collecting portalocker (from sacrebleu>=1.4.12->-r requirements.txt (line 11))\n",
      "  Using cached portalocker-2.8.2-py3-none-any.whl.metadata (8.5 kB)\n",
      "Collecting tabulate>=0.8.9 (from sacrebleu>=1.4.12->-r requirements.txt (line 11))\n",
      "  Using cached tabulate-0.9.0-py3-none-any.whl.metadata (34 kB)\n",
      "Collecting colorama (from sacrebleu>=1.4.12->-r requirements.txt (line 11))\n",
      "  Using cached colorama-0.4.6-py2.py3-none-any.whl.metadata (17 kB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting lxml (from sacrebleu>=1.4.12->-r requirements.txt (line 11))\n",
      "  Downloading lxml-5.2.2-cp312-cp312-macosx_10_9_x86_64.whl.metadata (3.4 kB)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /opt/miniconda3/lib/python3.12/site-packages (from matplotlib->-r requirements.txt (line 14)) (1.2.1)\n",
      "Requirement already satisfied: cycler>=0.10 in /opt/miniconda3/lib/python3.12/site-packages (from matplotlib->-r requirements.txt (line 14)) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /opt/miniconda3/lib/python3.12/site-packages (from matplotlib->-r requirements.txt (line 14)) (4.51.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /opt/miniconda3/lib/python3.12/site-packages (from matplotlib->-r requirements.txt (line 14)) (1.4.5)\n",
      "Requirement already satisfied: pillow>=8 in /opt/miniconda3/lib/python3.12/site-packages (from matplotlib->-r requirements.txt (line 14)) (10.3.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /opt/miniconda3/lib/python3.12/site-packages (from matplotlib->-r requirements.txt (line 14)) (3.1.2)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /opt/miniconda3/lib/python3.12/site-packages (from matplotlib->-r requirements.txt (line 14)) (2.9.0.post0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /opt/miniconda3/lib/python3.12/site-packages (from aiohttp->datasets>=1.8.0->-r requirements.txt (line 3)) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/miniconda3/lib/python3.12/site-packages (from aiohttp->datasets>=1.8.0->-r requirements.txt (line 3)) (23.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /opt/miniconda3/lib/python3.12/site-packages (from aiohttp->datasets>=1.8.0->-r requirements.txt (line 3)) (1.4.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/miniconda3/lib/python3.12/site-packages (from aiohttp->datasets>=1.8.0->-r requirements.txt (line 3)) (6.0.5)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /opt/miniconda3/lib/python3.12/site-packages (from aiohttp->datasets>=1.8.0->-r requirements.txt (line 3)) (1.9.4)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/miniconda3/lib/python3.12/site-packages (from requests->transformers==4.28.1->-r requirements.txt (line 12)) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/miniconda3/lib/python3.12/site-packages (from requests->transformers==4.28.1->-r requirements.txt (line 12)) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/miniconda3/lib/python3.12/site-packages (from requests->transformers==4.28.1->-r requirements.txt (line 12)) (2.1.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/miniconda3/lib/python3.12/site-packages (from requests->transformers==4.28.1->-r requirements.txt (line 12)) (2024.2.2)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/miniconda3/lib/python3.12/site-packages (from jinja2->torch>=1.13.0->-r requirements.txt (line 2)) (2.1.5)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/miniconda3/lib/python3.12/site-packages (from pandas->datasets>=1.8.0->-r requirements.txt (line 3)) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /opt/miniconda3/lib/python3.12/site-packages (from pandas->datasets>=1.8.0->-r requirements.txt (line 3)) (2024.1)\n",
      "Requirement already satisfied: mpmath>=0.19 in /opt/miniconda3/lib/python3.12/site-packages (from sympy->torch>=1.13.0->-r requirements.txt (line 2)) (1.3.0)\n",
      "Using cached transformers-4.28.1-py3-none-any.whl (7.0 MB)\n",
      "Downloading accelerate-0.30.1-py3-none-any.whl (302 kB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m302.6/302.6 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m[36m0:00:01\u001b[0m\n",
      "\u001b[?25hUsing cached datasets-2.19.1-py3-none-any.whl (542 kB)\n",
      "Using cached sentencepiece-0.2.0-cp312-cp312-macosx_10_9_x86_64.whl (1.2 MB)\n",
      "Using cached protobuf-3.20.1-py2.py3-none-any.whl (162 kB)\n",
      "Downloading evaluate-0.4.2-py3-none-any.whl (84 kB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.1/84.1 kB\u001b[0m \u001b[31m1.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hUsing cached py7zr-0.21.0-py3-none-any.whl (67 kB)\n",
      "Using cached sacrebleu-2.4.2-py3-none-any.whl (106 kB)\n",
      "Downloading peft-0.11.1-py3-none-any.whl (251 kB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m251.6/251.6 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m[36m0:00:01\u001b[0m[36m0:00:01\u001b[0m\n",
      "\u001b[?25hUsing cached Brotli-1.1.0-cp312-cp312-macosx_10_9_x86_64.whl (446 kB)\n",
      "Using cached dill-0.3.8-py3-none-any.whl (116 kB)\n",
      "Using cached inflate64-1.0.0-cp312-cp312-macosx_10_9_x86_64.whl (36 kB)\n",
      "Using cached multivolumefile-0.2.3-py3-none-any.whl (17 kB)\n",
      "Downloading pyarrow-16.1.0-cp312-cp312-macosx_10_15_x86_64.whl (28.4 MB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m28.4/28.4 MB\u001b[0m \u001b[31m9.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0m eta \u001b[36m0:00:01\u001b[0m[36m0:00:01\u001b[0m\n",
      "\u001b[?25hUsing cached pybcj-1.0.2-cp312-cp312-macosx_10_9_x86_64.whl (23 kB)\n",
      "Using cached pycryptodomex-3.20.0-cp35-abi3-macosx_10_9_x86_64.whl (1.6 MB)\n",
      "Using cached pyppmd-1.1.0-cp312-cp312-macosx_10_9_x86_64.whl (47 kB)\n",
      "Downloading pyzstd-0.16.0-cp312-cp312-macosx_10_9_x86_64.whl (373 kB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m373.1/373.1 kB\u001b[0m \u001b[31m1.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mm eta \u001b[36m0:00:01\u001b[0m0:01\u001b[0m\n",
      "\u001b[?25hUsing cached tabulate-0.9.0-py3-none-any.whl (35 kB)\n",
      "Using cached colorama-0.4.6-py2.py3-none-any.whl (25 kB)\n",
      "Downloading lxml-5.2.2-cp312-cp312-macosx_10_9_x86_64.whl (4.4 MB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.4/4.4 MB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mm eta \u001b[36m0:00:01\u001b[0m0:01\u001b[0m:01\u001b[0m\n",
      "\u001b[?25hUsing cached multiprocess-0.70.16-py312-none-any.whl (146 kB)\n",
      "Using cached portalocker-2.8.2-py3-none-any.whl (17 kB)\n",
      "Using cached pyarrow_hotfix-0.6-py3-none-any.whl (7.9 kB)\n",
      "Using cached texttable-1.7.0-py2.py3-none-any.whl (10 kB)\n",
      "Using cached xxhash-3.4.1-cp312-cp312-macosx_10_9_x86_64.whl (31 kB)\n",
      "Building wheels for collected packages: tokenizers\n",
      "  Building wheel for tokenizers (pyproject.toml) ... \u001b[?25lerror\n",
      "  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
      "  \n",
      "  \u001b[31m×\u001b[0m \u001b[32mBuilding wheel for tokenizers \u001b[0m\u001b[1;32m(\u001b[0m\u001b[32mpyproject.toml\u001b[0m\u001b[1;32m)\u001b[0m did not run successfully.\n",
      "  \u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
      "  \u001b[31m╰─>\u001b[0m \u001b[31m[51 lines of output]\u001b[0m\n",
      "  \u001b[31m   \u001b[0m running bdist_wheel\n",
      "  \u001b[31m   \u001b[0m running build\n",
      "  \u001b[31m   \u001b[0m running build_py\n",
      "  \u001b[31m   \u001b[0m creating build\n",
      "  \u001b[31m   \u001b[0m creating build/lib.macosx-10.9-x86_64-cpython-312\n",
      "  \u001b[31m   \u001b[0m creating build/lib.macosx-10.9-x86_64-cpython-312/tokenizers\n",
      "  \u001b[31m   \u001b[0m copying py_src/tokenizers/__init__.py -> build/lib.macosx-10.9-x86_64-cpython-312/tokenizers\n",
      "  \u001b[31m   \u001b[0m creating build/lib.macosx-10.9-x86_64-cpython-312/tokenizers/models\n",
      "  \u001b[31m   \u001b[0m copying py_src/tokenizers/models/__init__.py -> build/lib.macosx-10.9-x86_64-cpython-312/tokenizers/models\n",
      "  \u001b[31m   \u001b[0m creating build/lib.macosx-10.9-x86_64-cpython-312/tokenizers/decoders\n",
      "  \u001b[31m   \u001b[0m copying py_src/tokenizers/decoders/__init__.py -> build/lib.macosx-10.9-x86_64-cpython-312/tokenizers/decoders\n",
      "  \u001b[31m   \u001b[0m creating build/lib.macosx-10.9-x86_64-cpython-312/tokenizers/normalizers\n",
      "  \u001b[31m   \u001b[0m copying py_src/tokenizers/normalizers/__init__.py -> build/lib.macosx-10.9-x86_64-cpython-312/tokenizers/normalizers\n",
      "  \u001b[31m   \u001b[0m creating build/lib.macosx-10.9-x86_64-cpython-312/tokenizers/pre_tokenizers\n",
      "  \u001b[31m   \u001b[0m copying py_src/tokenizers/pre_tokenizers/__init__.py -> build/lib.macosx-10.9-x86_64-cpython-312/tokenizers/pre_tokenizers\n",
      "  \u001b[31m   \u001b[0m creating build/lib.macosx-10.9-x86_64-cpython-312/tokenizers/processors\n",
      "  \u001b[31m   \u001b[0m copying py_src/tokenizers/processors/__init__.py -> build/lib.macosx-10.9-x86_64-cpython-312/tokenizers/processors\n",
      "  \u001b[31m   \u001b[0m creating build/lib.macosx-10.9-x86_64-cpython-312/tokenizers/trainers\n",
      "  \u001b[31m   \u001b[0m copying py_src/tokenizers/trainers/__init__.py -> build/lib.macosx-10.9-x86_64-cpython-312/tokenizers/trainers\n",
      "  \u001b[31m   \u001b[0m creating build/lib.macosx-10.9-x86_64-cpython-312/tokenizers/implementations\n",
      "  \u001b[31m   \u001b[0m copying py_src/tokenizers/implementations/byte_level_bpe.py -> build/lib.macosx-10.9-x86_64-cpython-312/tokenizers/implementations\n",
      "  \u001b[31m   \u001b[0m copying py_src/tokenizers/implementations/sentencepiece_unigram.py -> build/lib.macosx-10.9-x86_64-cpython-312/tokenizers/implementations\n",
      "  \u001b[31m   \u001b[0m copying py_src/tokenizers/implementations/sentencepiece_bpe.py -> build/lib.macosx-10.9-x86_64-cpython-312/tokenizers/implementations\n",
      "  \u001b[31m   \u001b[0m copying py_src/tokenizers/implementations/base_tokenizer.py -> build/lib.macosx-10.9-x86_64-cpython-312/tokenizers/implementations\n",
      "  \u001b[31m   \u001b[0m copying py_src/tokenizers/implementations/__init__.py -> build/lib.macosx-10.9-x86_64-cpython-312/tokenizers/implementations\n",
      "  \u001b[31m   \u001b[0m copying py_src/tokenizers/implementations/char_level_bpe.py -> build/lib.macosx-10.9-x86_64-cpython-312/tokenizers/implementations\n",
      "  \u001b[31m   \u001b[0m copying py_src/tokenizers/implementations/bert_wordpiece.py -> build/lib.macosx-10.9-x86_64-cpython-312/tokenizers/implementations\n",
      "  \u001b[31m   \u001b[0m creating build/lib.macosx-10.9-x86_64-cpython-312/tokenizers/tools\n",
      "  \u001b[31m   \u001b[0m copying py_src/tokenizers/tools/__init__.py -> build/lib.macosx-10.9-x86_64-cpython-312/tokenizers/tools\n",
      "  \u001b[31m   \u001b[0m copying py_src/tokenizers/tools/visualizer.py -> build/lib.macosx-10.9-x86_64-cpython-312/tokenizers/tools\n",
      "  \u001b[31m   \u001b[0m copying py_src/tokenizers/__init__.pyi -> build/lib.macosx-10.9-x86_64-cpython-312/tokenizers\n",
      "  \u001b[31m   \u001b[0m copying py_src/tokenizers/models/__init__.pyi -> build/lib.macosx-10.9-x86_64-cpython-312/tokenizers/models\n",
      "  \u001b[31m   \u001b[0m copying py_src/tokenizers/decoders/__init__.pyi -> build/lib.macosx-10.9-x86_64-cpython-312/tokenizers/decoders\n",
      "  \u001b[31m   \u001b[0m copying py_src/tokenizers/normalizers/__init__.pyi -> build/lib.macosx-10.9-x86_64-cpython-312/tokenizers/normalizers\n",
      "  \u001b[31m   \u001b[0m copying py_src/tokenizers/pre_tokenizers/__init__.pyi -> build/lib.macosx-10.9-x86_64-cpython-312/tokenizers/pre_tokenizers\n",
      "  \u001b[31m   \u001b[0m copying py_src/tokenizers/processors/__init__.pyi -> build/lib.macosx-10.9-x86_64-cpython-312/tokenizers/processors\n",
      "  \u001b[31m   \u001b[0m copying py_src/tokenizers/trainers/__init__.pyi -> build/lib.macosx-10.9-x86_64-cpython-312/tokenizers/trainers\n",
      "  \u001b[31m   \u001b[0m copying py_src/tokenizers/tools/visualizer-styles.css -> build/lib.macosx-10.9-x86_64-cpython-312/tokenizers/tools\n",
      "  \u001b[31m   \u001b[0m running build_ext\n",
      "  \u001b[31m   \u001b[0m running build_rust\n",
      "  \u001b[31m   \u001b[0m error: can't find Rust compiler\n",
      "  \u001b[31m   \u001b[0m \n",
      "  \u001b[31m   \u001b[0m If you are using an outdated pip version, it is possible a prebuilt wheel is available for this package but pip is not able to install from it. Installing from the wheel would avoid the need for a Rust compiler.\n",
      "  \u001b[31m   \u001b[0m \n",
      "  \u001b[31m   \u001b[0m To update pip, run:\n",
      "  \u001b[31m   \u001b[0m \n",
      "  \u001b[31m   \u001b[0m     pip install --upgrade pip\n",
      "  \u001b[31m   \u001b[0m \n",
      "  \u001b[31m   \u001b[0m and then retry package installation.\n",
      "  \u001b[31m   \u001b[0m \n",
      "  \u001b[31m   \u001b[0m If you did intend to build this package from source, try installing a Rust compiler from your system package manager and ensure it is on the PATH during installation. Alternatively, rustup (available at https://rustup.rs) is the recommended way to download and update the Rust compiler toolchain.\n",
      "  \u001b[31m   \u001b[0m \u001b[31m[end of output]\u001b[0m\n",
      "  \n",
      "  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n",
      "\u001b[31m  ERROR: Failed building wheel for tokenizers\u001b[0m\u001b[31m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0m\u001b[?25hFailed to build tokenizers\r\n",
      "\u001b[31mERROR: Could not build wheels for tokenizers, which is required to install pyproject.toml-based projects\u001b[0m\u001b[31m\r\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zddLa4Dunsdg",
    "outputId": "07a1f54d-06cc-465c-f727-b000f0787740"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /usr/local/share/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "8R6TJx-1mDQ7"
   },
   "outputs": [],
   "source": [
    "CUDA_VISIBLE_DEVICES=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IGozVQw8u5DO"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "RTA9Zb_NmjPu",
    "outputId": "d26e20ef-4f8a-4987-faef-201255cec46f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-05-21 01:49:02.294219: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-05-21 01:49:02.294273: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-05-21 01:49:02.296313: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-05-21 01:49:04.103085: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n",
      "05/21/2024 01:49:07 - WARNING - __main__ - Process rank: 0, device: cuda:0, n_gpu: 1distributed training: True, 16-bits training: False\n",
      "05/21/2024 01:49:07 - INFO - __main__ - Training/evaluation parameters Seq2SeqTrainingArguments(\n",
      "_n_gpu=1,\n",
      "adafactor=False,\n",
      "adam_beta1=0.9,\n",
      "adam_beta2=0.999,\n",
      "adam_epsilon=1e-08,\n",
      "auto_find_batch_size=False,\n",
      "bf16=False,\n",
      "bf16_full_eval=False,\n",
      "data_seed=None,\n",
      "dataloader_drop_last=False,\n",
      "dataloader_num_workers=0,\n",
      "dataloader_pin_memory=True,\n",
      "ddp_bucket_cap_mb=None,\n",
      "ddp_find_unused_parameters=None,\n",
      "ddp_timeout=1800,\n",
      "debug=[],\n",
      "deepspeed=None,\n",
      "disable_tqdm=False,\n",
      "do_eval=True,\n",
      "do_predict=False,\n",
      "do_train=False,\n",
      "eval_accumulation_steps=None,\n",
      "eval_delay=0,\n",
      "eval_steps=None,\n",
      "evaluation_strategy=no,\n",
      "fp16=False,\n",
      "fp16_backend=auto,\n",
      "fp16_full_eval=False,\n",
      "fp16_opt_level=O1,\n",
      "fsdp=[],\n",
      "fsdp_config={'fsdp_min_num_params': 0, 'xla': False, 'xla_fsdp_grad_ckpt': False},\n",
      "fsdp_min_num_params=0,\n",
      "fsdp_transformer_layer_cls_to_wrap=None,\n",
      "full_determinism=False,\n",
      "generation_config=None,\n",
      "generation_max_length=None,\n",
      "generation_num_beams=None,\n",
      "gradient_accumulation_steps=1,\n",
      "gradient_checkpointing=False,\n",
      "greater_is_better=None,\n",
      "group_by_length=False,\n",
      "half_precision_backend=auto,\n",
      "hub_model_id=None,\n",
      "hub_private_repo=False,\n",
      "hub_strategy=every_save,\n",
      "hub_token=<HUB_TOKEN>,\n",
      "ignore_data_skip=False,\n",
      "include_inputs_for_metrics=False,\n",
      "jit_mode_eval=False,\n",
      "label_names=None,\n",
      "label_smoothing_factor=0.0,\n",
      "learning_rate=5e-05,\n",
      "length_column_name=length,\n",
      "load_best_model_at_end=False,\n",
      "local_rank=0,\n",
      "log_level=passive,\n",
      "log_level_replica=warning,\n",
      "log_on_each_node=True,\n",
      "logging_dir=./save/cnndm_t5_large/runs/May21_01-49-06_b56da3cfb72c,\n",
      "logging_first_step=False,\n",
      "logging_nan_inf_filter=True,\n",
      "logging_steps=500,\n",
      "logging_strategy=steps,\n",
      "lr_scheduler_type=linear,\n",
      "max_grad_norm=1.0,\n",
      "max_steps=-1,\n",
      "metric_for_best_model=None,\n",
      "mp_parameters=,\n",
      "no_cuda=False,\n",
      "num_train_epochs=10.0,\n",
      "optim=adamw_hf,\n",
      "optim_args=None,\n",
      "output_dir=./save/cnndm_t5_large/,\n",
      "overwrite_output_dir=True,\n",
      "past_index=-1,\n",
      "per_device_eval_batch_size=1,\n",
      "per_device_train_batch_size=4,\n",
      "predict_with_generate=True,\n",
      "prediction_loss_only=False,\n",
      "push_to_hub=False,\n",
      "push_to_hub_model_id=None,\n",
      "push_to_hub_organization=None,\n",
      "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
      "ray_scope=last,\n",
      "remove_unused_columns=True,\n",
      "report_to=['tensorboard'],\n",
      "resume_from_checkpoint=None,\n",
      "run_name=./save/cnndm_t5_large/,\n",
      "save_on_each_node=False,\n",
      "save_safetensors=False,\n",
      "save_steps=500,\n",
      "save_strategy=steps,\n",
      "save_total_limit=None,\n",
      "seed=42,\n",
      "sharded_ddp=[],\n",
      "skip_memory_metrics=True,\n",
      "sortish_sampler=False,\n",
      "tf32=None,\n",
      "torch_compile=False,\n",
      "torch_compile_backend=None,\n",
      "torch_compile_mode=None,\n",
      "torchdynamo=None,\n",
      "tpu_metrics_debug=False,\n",
      "tpu_num_cores=None,\n",
      "use_ipex=False,\n",
      "use_legacy_prediction_loop=False,\n",
      "use_mps_device=False,\n",
      "warmup_ratio=0.0,\n",
      "warmup_steps=0,\n",
      "weight_decay=0.0,\n",
      "xpu_backend=None,\n",
      ")\n",
      "/usr/local/lib/python3.10/dist-packages/datasets/load.py:2547: FutureWarning: 'use_auth_token' was deprecated in favor of 'token' in version 2.14.0 and will be removed in 3.0.0.\n",
      "You can remove this warning by passing 'token=<use_auth_token>' instead.\n",
      "  warnings.warn(\n",
      "Overwrite dataset info from restored data version if exists.\n",
      "05/21/2024 01:49:08 - INFO - datasets.builder - Overwrite dataset info from restored data version if exists.\n",
      "Loading Dataset info from /root/.cache/huggingface/datasets/cnn_dailymail/3.0.0/0.0.0/96df5e686bee6baa90b8bee7c28b81fa3fa6223d\n",
      "05/21/2024 01:49:08 - INFO - datasets.info - Loading Dataset info from /root/.cache/huggingface/datasets/cnn_dailymail/3.0.0/0.0.0/96df5e686bee6baa90b8bee7c28b81fa3fa6223d\n",
      "Found cached dataset cnn_dailymail (/root/.cache/huggingface/datasets/cnn_dailymail/3.0.0/0.0.0/96df5e686bee6baa90b8bee7c28b81fa3fa6223d)\n",
      "05/21/2024 01:49:08 - INFO - datasets.builder - Found cached dataset cnn_dailymail (/root/.cache/huggingface/datasets/cnn_dailymail/3.0.0/0.0.0/96df5e686bee6baa90b8bee7c28b81fa3fa6223d)\n",
      "Loading Dataset info from /root/.cache/huggingface/datasets/cnn_dailymail/3.0.0/0.0.0/96df5e686bee6baa90b8bee7c28b81fa3fa6223d\n",
      "05/21/2024 01:49:08 - INFO - datasets.info - Loading Dataset info from /root/.cache/huggingface/datasets/cnn_dailymail/3.0.0/0.0.0/96df5e686bee6baa90b8bee7c28b81fa3fa6223d\n",
      "/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "[INFO|configuration_utils.py:668] 2024-05-21 01:49:09,459 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--t5-large/snapshots/150ebc2c4b72291e770f58e6057481c8d2ed331a/config.json\n",
      "[INFO|configuration_utils.py:720] 2024-05-21 01:49:09,460 >> Model config T5Config {\n",
      "  \"_name_or_path\": \"t5-large\",\n",
      "  \"architectures\": [\n",
      "    \"T5ForConditionalGeneration\"\n",
      "  ],\n",
      "  \"d_ff\": 4096,\n",
      "  \"d_kv\": 64,\n",
      "  \"d_model\": 1024,\n",
      "  \"decoder_start_token_id\": 0,\n",
      "  \"dense_act_fn\": \"relu\",\n",
      "  \"dropout_rate\": 0.1,\n",
      "  \"eos_token_id\": 1,\n",
      "  \"feed_forward_proj\": \"relu\",\n",
      "  \"initializer_factor\": 1.0,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"is_gated_act\": false,\n",
      "  \"layer_norm_epsilon\": 1e-06,\n",
      "  \"model_type\": \"t5\",\n",
      "  \"n_positions\": 512,\n",
      "  \"num_decoder_layers\": 24,\n",
      "  \"num_heads\": 16,\n",
      "  \"num_layers\": 24,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"relative_attention_max_distance\": 128,\n",
      "  \"relative_attention_num_buckets\": 32,\n",
      "  \"task_specific_params\": {\n",
      "    \"summarization\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"length_penalty\": 2.0,\n",
      "      \"max_length\": 200,\n",
      "      \"min_length\": 30,\n",
      "      \"no_repeat_ngram_size\": 3,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"summarize: \"\n",
      "    },\n",
      "    \"translation_en_to_de\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"max_length\": 300,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"translate English to German: \"\n",
      "    },\n",
      "    \"translation_en_to_fr\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"max_length\": 300,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"translate English to French: \"\n",
      "    },\n",
      "    \"translation_en_to_ro\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"max_length\": 300,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"translate English to Romanian: \"\n",
      "    }\n",
      "  },\n",
      "  \"transformers_version\": \"4.28.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32128\n",
      "}\n",
      "\n",
      "[INFO|tokenization_auto.py:502] 2024-05-21 01:49:09,510 >> Could not locate the tokenizer configuration file, will try to use the model config instead.\n",
      "[INFO|configuration_utils.py:668] 2024-05-21 01:49:09,683 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--t5-large/snapshots/150ebc2c4b72291e770f58e6057481c8d2ed331a/config.json\n",
      "[INFO|configuration_utils.py:720] 2024-05-21 01:49:09,684 >> Model config T5Config {\n",
      "  \"_name_or_path\": \"t5-large\",\n",
      "  \"architectures\": [\n",
      "    \"T5ForConditionalGeneration\"\n",
      "  ],\n",
      "  \"d_ff\": 4096,\n",
      "  \"d_kv\": 64,\n",
      "  \"d_model\": 1024,\n",
      "  \"decoder_start_token_id\": 0,\n",
      "  \"dense_act_fn\": \"relu\",\n",
      "  \"dropout_rate\": 0.1,\n",
      "  \"eos_token_id\": 1,\n",
      "  \"feed_forward_proj\": \"relu\",\n",
      "  \"initializer_factor\": 1.0,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"is_gated_act\": false,\n",
      "  \"layer_norm_epsilon\": 1e-06,\n",
      "  \"model_type\": \"t5\",\n",
      "  \"n_positions\": 512,\n",
      "  \"num_decoder_layers\": 24,\n",
      "  \"num_heads\": 16,\n",
      "  \"num_layers\": 24,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"relative_attention_max_distance\": 128,\n",
      "  \"relative_attention_num_buckets\": 32,\n",
      "  \"task_specific_params\": {\n",
      "    \"summarization\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"length_penalty\": 2.0,\n",
      "      \"max_length\": 200,\n",
      "      \"min_length\": 30,\n",
      "      \"no_repeat_ngram_size\": 3,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"summarize: \"\n",
      "    },\n",
      "    \"translation_en_to_de\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"max_length\": 300,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"translate English to German: \"\n",
      "    },\n",
      "    \"translation_en_to_fr\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"max_length\": 300,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"translate English to French: \"\n",
      "    },\n",
      "    \"translation_en_to_ro\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"max_length\": 300,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"translate English to Romanian: \"\n",
      "    }\n",
      "  },\n",
      "  \"transformers_version\": \"4.28.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32128\n",
      "}\n",
      "\n",
      "[INFO|tokenization_utils_base.py:1809] 2024-05-21 01:49:09,787 >> loading file spiece.model from cache at /root/.cache/huggingface/hub/models--t5-large/snapshots/150ebc2c4b72291e770f58e6057481c8d2ed331a/spiece.model\n",
      "[INFO|tokenization_utils_base.py:1809] 2024-05-21 01:49:09,787 >> loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--t5-large/snapshots/150ebc2c4b72291e770f58e6057481c8d2ed331a/tokenizer.json\n",
      "[INFO|tokenization_utils_base.py:1809] 2024-05-21 01:49:09,787 >> loading file added_tokens.json from cache at None\n",
      "[INFO|tokenization_utils_base.py:1809] 2024-05-21 01:49:09,787 >> loading file special_tokens_map.json from cache at None\n",
      "[INFO|tokenization_utils_base.py:1809] 2024-05-21 01:49:09,787 >> loading file tokenizer_config.json from cache at None\n",
      "[INFO|configuration_utils.py:668] 2024-05-21 01:49:09,788 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--t5-large/snapshots/150ebc2c4b72291e770f58e6057481c8d2ed331a/config.json\n",
      "[INFO|configuration_utils.py:720] 2024-05-21 01:49:09,789 >> Model config T5Config {\n",
      "  \"_name_or_path\": \"t5-large\",\n",
      "  \"architectures\": [\n",
      "    \"T5ForConditionalGeneration\"\n",
      "  ],\n",
      "  \"d_ff\": 4096,\n",
      "  \"d_kv\": 64,\n",
      "  \"d_model\": 1024,\n",
      "  \"decoder_start_token_id\": 0,\n",
      "  \"dense_act_fn\": \"relu\",\n",
      "  \"dropout_rate\": 0.1,\n",
      "  \"eos_token_id\": 1,\n",
      "  \"feed_forward_proj\": \"relu\",\n",
      "  \"initializer_factor\": 1.0,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"is_gated_act\": false,\n",
      "  \"layer_norm_epsilon\": 1e-06,\n",
      "  \"model_type\": \"t5\",\n",
      "  \"n_positions\": 512,\n",
      "  \"num_decoder_layers\": 24,\n",
      "  \"num_heads\": 16,\n",
      "  \"num_layers\": 24,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"relative_attention_max_distance\": 128,\n",
      "  \"relative_attention_num_buckets\": 32,\n",
      "  \"task_specific_params\": {\n",
      "    \"summarization\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"length_penalty\": 2.0,\n",
      "      \"max_length\": 200,\n",
      "      \"min_length\": 30,\n",
      "      \"no_repeat_ngram_size\": 3,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"summarize: \"\n",
      "    },\n",
      "    \"translation_en_to_de\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"max_length\": 300,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"translate English to German: \"\n",
      "    },\n",
      "    \"translation_en_to_fr\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"max_length\": 300,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"translate English to French: \"\n",
      "    },\n",
      "    \"translation_en_to_ro\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"max_length\": 300,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"translate English to Romanian: \"\n",
      "    }\n",
      "  },\n",
      "  \"transformers_version\": \"4.28.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32128\n",
      "}\n",
      "\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/models/t5/tokenization_t5_fast.py:155: FutureWarning: This tokenizer was incorrectly instantiated with a model max length of 512 which will be corrected in Transformers v5.\n",
      "For now, this behavior is kept to avoid breaking backwards compatibility when padding/encoding with `truncation is True`.\n",
      "- Be aware that you SHOULD NOT rely on t5-large automatically truncating your input to 512 when padding/encoding.\n",
      "- If you want to encode/pad to sequences longer than 512 you can either instantiate this tokenizer with `model_max_length` or pass `max_length` when encoding/padding.\n",
      "- To avoid this warning, please instantiate this tokenizer with `model_max_length` set to your preferred value.\n",
      "  warnings.warn(\n",
      "[INFO|modeling_utils.py:2534] 2024-05-21 01:49:09,844 >> loading weights file model.safetensors from cache at /root/.cache/huggingface/hub/models--t5-large/snapshots/150ebc2c4b72291e770f58e6057481c8d2ed331a/model.safetensors\n",
      "[INFO|configuration_utils.py:575] 2024-05-21 01:49:09,882 >> Generate config GenerationConfig {\n",
      "  \"_from_model_config\": true,\n",
      "  \"decoder_start_token_id\": 0,\n",
      "  \"eos_token_id\": 1,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"transformers_version\": \"4.28.1\"\n",
      "}\n",
      "\n",
      "[INFO|modeling_utils.py:3190] 2024-05-21 01:49:53,563 >> All model checkpoint weights were used when initializing DeployT5ForConditionalGeneration.\n",
      "\n",
      "[INFO|modeling_utils.py:3198] 2024-05-21 01:49:53,564 >> All the weights of DeployT5ForConditionalGeneration were initialized from the model checkpoint at t5-large.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use DeployT5ForConditionalGeneration for predictions without further training.\n",
      "[INFO|configuration_utils.py:537] 2024-05-21 01:49:53,625 >> loading configuration file generation_config.json from cache at /root/.cache/huggingface/hub/models--t5-large/snapshots/150ebc2c4b72291e770f58e6057481c8d2ed331a/generation_config.json\n",
      "[INFO|configuration_utils.py:575] 2024-05-21 01:49:53,625 >> Generate config GenerationConfig {\n",
      "  \"_from_model_config\": true,\n",
      "  \"decoder_start_token_id\": 0,\n",
      "  \"eos_token_id\": 1,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"transformers_version\": \"4.28.1\"\n",
      "}\n",
      "\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/cnn_dailymail/3.0.0/0.0.0/96df5e686bee6baa90b8bee7c28b81fa3fa6223d/cache-6246a45506458069.arrow\n",
      "05/21/2024 01:49:53 - INFO - datasets.arrow_dataset - Loading cached processed dataset at /root/.cache/huggingface/datasets/cnn_dailymail/3.0.0/0.0.0/96df5e686bee6baa90b8bee7c28b81fa3fa6223d/cache-6246a45506458069.arrow\n",
      "05/21/2024 01:49:55 - INFO - __main__ - *** Evaluate ***\n",
      "[WARNING|logging.py:280] 2024-05-21 01:49:55,443 >> You're using a T5TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "[INFO|configuration_utils.py:575] 2024-05-21 01:49:55,455 >> Generate config GenerationConfig {\n",
      "  \"_from_model_config\": true,\n",
      "  \"decoder_start_token_id\": 0,\n",
      "  \"eos_token_id\": 1,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"transformers_version\": \"4.28.1\"\n",
      "}\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/content/Monotonic-Early-Exit/src/run_summarization.py\", line 630, in <module>\n",
      "    main(model_args, data_args, training_args, additional_args, model_cls, trainer_cls)\n",
      "  File \"/content/Monotonic-Early-Exit/src/run_summarization.py\", line 546, in main\n",
      "    metrics = trainer.evaluate(metric_key_prefix=\"eval\")\n",
      "  File \"/content/Monotonic-Early-Exit/src/sum_lib/trainer_sum.py\", line 106, in evaluate\n",
      "    output = eval_loop(\n",
      "  File \"/content/Monotonic-Early-Exit/src/sum_lib/trainer_sum.py\", line 239, in evaluation_loop\n",
      "    loss, logits, labels = self.prediction_step(model, inputs, prediction_loss_only, ignore_keys=ignore_keys)\n",
      "  File \"/content/Monotonic-Early-Exit/src/sum_lib/trainer_sum.py\", line 409, in prediction_step\n",
      "    generated_tokens = gen_model.generate(\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/_contextlib.py\", line 115, in decorate_context\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py\", line 1437, in generate\n",
      "    return self.greedy_search(\n",
      "  File \"/content/Monotonic-Early-Exit/src/models/deploying_t5.py\", line 1334, in greedy_search\n",
      "    outputs = self(\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/content/Monotonic-Early-Exit/src/models/deploying_t5.py\", line 1097, in forward\n",
      "    encoder_outputs, decoder_outputs = self.forward_impl(input_ids, attention_mask, decoder_input_ids, decoder_attention_mask,\n",
      "  File \"/content/Monotonic-Early-Exit/src/models/deploying_t5.py\", line 1217, in forward_impl\n",
      "    decoder_outputs = self.decoder(\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/content/Monotonic-Early-Exit/src/models/deploying_t5.py\", line 921, in forward\n",
      "    skip_mask = get_skip_mask(\n",
      "  File \"/content/Monotonic-Early-Exit/src/util/skip_conf.py\", line 196, in get_skip_mask\n",
      "    conf = conf_measure(\n",
      "  File \"/content/Monotonic-Early-Exit/src/util/skip_conf.py\", line 33, in last_three_hiddens_classifier\n",
      "    assert classifier is not None\n",
      "AssertionError\n",
      "[2024-05-21 01:50:02,314] torch.distributed.elastic.multiprocessing.api: [ERROR] failed (exitcode: 1) local_rank: 0 (pid: 19644) of binary: /usr/bin/python3\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.10/runpy.py\", line 196, in _run_module_as_main\n",
      "    return _run_code(code, main_globals, None,\n",
      "  File \"/usr/lib/python3.10/runpy.py\", line 86, in _run_code\n",
      "    exec(code, run_globals)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/torch/distributed/run.py\", line 816, in <module>\n",
      "    main()\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py\", line 347, in wrapper\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/torch/distributed/run.py\", line 812, in main\n",
      "    run(args)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/torch/distributed/run.py\", line 803, in run\n",
      "    elastic_launch(\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/torch/distributed/launcher/api.py\", line 135, in __call__\n",
      "    return launch_agent(self._config, self._entrypoint, list(args))\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/torch/distributed/launcher/api.py\", line 268, in launch_agent\n",
      "    raise ChildFailedError(\n",
      "torch.distributed.elastic.multiprocessing.errors.ChildFailedError: \n",
      "============================================================\n",
      "src/run_summarization.py FAILED\n",
      "------------------------------------------------------------\n",
      "Failures:\n",
      "  <NO_OTHER_FAILURES>\n",
      "------------------------------------------------------------\n",
      "Root Cause (first observed failure):\n",
      "[0]:\n",
      "  time      : 2024-05-21_01:50:02\n",
      "  host      : b56da3cfb72c\n",
      "  rank      : 0 (local_rank: 0)\n",
      "  exitcode  : 1 (pid: 19644)\n",
      "  error_file: <N/A>\n",
      "  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "!python -m torch.distributed.run --nproc_per_node=1 \\\n",
    "    src/run_summarization.py \\\n",
    "    --model_name_or_path t5-large \\\n",
    "    --do_eval \\\n",
    "    --dataset_name cnn_dailymail \\\n",
    "    --dataset_config_name \"3.0.0\" \\\n",
    "    --output_dir ./save/cnndm_t5_large/ \\\n",
    "    --deploy_scenario True \\\n",
    "    --use_synchronize True \\\n",
    "    --per_device_train_batch_size 4 \\\n",
    "    --per_device_eval_batch_size 1 \\\n",
    "    --overwrite_output_dir \\\n",
    "    --predict_with_generate \\\n",
    "    --source_prefix \"summarize: \" \\\n",
    "    --num_train_epochs 10 \\\n",
    "    --output_hidden_states_decoder True \\\n",
    "    --intermediate_loss_fn weighted_ce \\\n",
    "    --use_early_exit True \\\n",
    "    --exit_conf_type last_three_hiddens_classifier \\\n",
    "    --exit_position_temp 4 \\\n",
    "    --exit_conf_threshold 0.5\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "M6_7Ebu311UI",
    "outputId": "3373c422-5572-485c-9f14-04ba2041c98b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-05-21 01:50:10.410698: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-05-21 01:50:10.410749: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-05-21 01:50:10.417062: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-05-21 01:50:12.402105: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n",
      "05/21/2024 01:50:16 - WARNING - __main__ - Process rank: 0, device: cuda:0, n_gpu: 1distributed training: True, 16-bits training: False\n",
      "05/21/2024 01:50:16 - INFO - __main__ - Training/evaluation parameters Seq2SeqTrainingArguments(\n",
      "_n_gpu=1,\n",
      "adafactor=False,\n",
      "adam_beta1=0.9,\n",
      "adam_beta2=0.999,\n",
      "adam_epsilon=1e-08,\n",
      "auto_find_batch_size=False,\n",
      "bf16=False,\n",
      "bf16_full_eval=False,\n",
      "data_seed=None,\n",
      "dataloader_drop_last=False,\n",
      "dataloader_num_workers=0,\n",
      "dataloader_pin_memory=True,\n",
      "ddp_bucket_cap_mb=None,\n",
      "ddp_find_unused_parameters=None,\n",
      "ddp_timeout=1800,\n",
      "debug=[],\n",
      "deepspeed=None,\n",
      "disable_tqdm=False,\n",
      "do_eval=True,\n",
      "do_predict=False,\n",
      "do_train=False,\n",
      "eval_accumulation_steps=None,\n",
      "eval_delay=0,\n",
      "eval_steps=None,\n",
      "evaluation_strategy=no,\n",
      "fp16=False,\n",
      "fp16_backend=auto,\n",
      "fp16_full_eval=False,\n",
      "fp16_opt_level=O1,\n",
      "fsdp=[],\n",
      "fsdp_config={'fsdp_min_num_params': 0, 'xla': False, 'xla_fsdp_grad_ckpt': False},\n",
      "fsdp_min_num_params=0,\n",
      "fsdp_transformer_layer_cls_to_wrap=None,\n",
      "full_determinism=False,\n",
      "generation_config=None,\n",
      "generation_max_length=None,\n",
      "generation_num_beams=None,\n",
      "gradient_accumulation_steps=1,\n",
      "gradient_checkpointing=False,\n",
      "greater_is_better=None,\n",
      "group_by_length=False,\n",
      "half_precision_backend=auto,\n",
      "hub_model_id=None,\n",
      "hub_private_repo=False,\n",
      "hub_strategy=every_save,\n",
      "hub_token=<HUB_TOKEN>,\n",
      "ignore_data_skip=False,\n",
      "include_inputs_for_metrics=False,\n",
      "jit_mode_eval=False,\n",
      "label_names=None,\n",
      "label_smoothing_factor=0.0,\n",
      "learning_rate=5e-05,\n",
      "length_column_name=length,\n",
      "load_best_model_at_end=False,\n",
      "local_rank=0,\n",
      "log_level=passive,\n",
      "log_level_replica=warning,\n",
      "log_on_each_node=True,\n",
      "logging_dir=./save/cnndm_t5_large/runs/May21_01-50-16_b56da3cfb72c,\n",
      "logging_first_step=False,\n",
      "logging_nan_inf_filter=True,\n",
      "logging_steps=500,\n",
      "logging_strategy=steps,\n",
      "lr_scheduler_type=linear,\n",
      "max_grad_norm=1.0,\n",
      "max_steps=-1,\n",
      "metric_for_best_model=None,\n",
      "mp_parameters=,\n",
      "no_cuda=False,\n",
      "num_train_epochs=10.0,\n",
      "optim=adamw_hf,\n",
      "optim_args=None,\n",
      "output_dir=./save/cnndm_t5_large/,\n",
      "overwrite_output_dir=True,\n",
      "past_index=-1,\n",
      "per_device_eval_batch_size=1,\n",
      "per_device_train_batch_size=4,\n",
      "predict_with_generate=True,\n",
      "prediction_loss_only=False,\n",
      "push_to_hub=False,\n",
      "push_to_hub_model_id=None,\n",
      "push_to_hub_organization=None,\n",
      "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
      "ray_scope=last,\n",
      "remove_unused_columns=True,\n",
      "report_to=['tensorboard'],\n",
      "resume_from_checkpoint=None,\n",
      "run_name=./save/cnndm_t5_large/,\n",
      "save_on_each_node=False,\n",
      "save_safetensors=False,\n",
      "save_steps=500,\n",
      "save_strategy=steps,\n",
      "save_total_limit=None,\n",
      "seed=42,\n",
      "sharded_ddp=[],\n",
      "skip_memory_metrics=True,\n",
      "sortish_sampler=False,\n",
      "tf32=None,\n",
      "torch_compile=False,\n",
      "torch_compile_backend=None,\n",
      "torch_compile_mode=None,\n",
      "torchdynamo=None,\n",
      "tpu_metrics_debug=False,\n",
      "tpu_num_cores=None,\n",
      "use_ipex=False,\n",
      "use_legacy_prediction_loop=False,\n",
      "use_mps_device=False,\n",
      "warmup_ratio=0.0,\n",
      "warmup_steps=0,\n",
      "weight_decay=0.0,\n",
      "xpu_backend=None,\n",
      ")\n",
      "/usr/local/lib/python3.10/dist-packages/datasets/load.py:2547: FutureWarning: 'use_auth_token' was deprecated in favor of 'token' in version 2.14.0 and will be removed in 3.0.0.\n",
      "You can remove this warning by passing 'token=<use_auth_token>' instead.\n",
      "  warnings.warn(\n",
      "Overwrite dataset info from restored data version if exists.\n",
      "05/21/2024 01:50:18 - INFO - datasets.builder - Overwrite dataset info from restored data version if exists.\n",
      "Loading Dataset info from /root/.cache/huggingface/datasets/cnn_dailymail/3.0.0/0.0.0/96df5e686bee6baa90b8bee7c28b81fa3fa6223d\n",
      "05/21/2024 01:50:18 - INFO - datasets.info - Loading Dataset info from /root/.cache/huggingface/datasets/cnn_dailymail/3.0.0/0.0.0/96df5e686bee6baa90b8bee7c28b81fa3fa6223d\n",
      "Found cached dataset cnn_dailymail (/root/.cache/huggingface/datasets/cnn_dailymail/3.0.0/0.0.0/96df5e686bee6baa90b8bee7c28b81fa3fa6223d)\n",
      "05/21/2024 01:50:18 - INFO - datasets.builder - Found cached dataset cnn_dailymail (/root/.cache/huggingface/datasets/cnn_dailymail/3.0.0/0.0.0/96df5e686bee6baa90b8bee7c28b81fa3fa6223d)\n",
      "Loading Dataset info from /root/.cache/huggingface/datasets/cnn_dailymail/3.0.0/0.0.0/96df5e686bee6baa90b8bee7c28b81fa3fa6223d\n",
      "05/21/2024 01:50:18 - INFO - datasets.info - Loading Dataset info from /root/.cache/huggingface/datasets/cnn_dailymail/3.0.0/0.0.0/96df5e686bee6baa90b8bee7c28b81fa3fa6223d\n",
      "/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "[INFO|configuration_utils.py:668] 2024-05-21 01:50:18,839 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--t5-large/snapshots/150ebc2c4b72291e770f58e6057481c8d2ed331a/config.json\n",
      "[INFO|configuration_utils.py:720] 2024-05-21 01:50:18,840 >> Model config T5Config {\n",
      "  \"_name_or_path\": \"t5-large\",\n",
      "  \"architectures\": [\n",
      "    \"T5ForConditionalGeneration\"\n",
      "  ],\n",
      "  \"d_ff\": 4096,\n",
      "  \"d_kv\": 64,\n",
      "  \"d_model\": 1024,\n",
      "  \"decoder_start_token_id\": 0,\n",
      "  \"dense_act_fn\": \"relu\",\n",
      "  \"dropout_rate\": 0.1,\n",
      "  \"eos_token_id\": 1,\n",
      "  \"feed_forward_proj\": \"relu\",\n",
      "  \"initializer_factor\": 1.0,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"is_gated_act\": false,\n",
      "  \"layer_norm_epsilon\": 1e-06,\n",
      "  \"model_type\": \"t5\",\n",
      "  \"n_positions\": 512,\n",
      "  \"num_decoder_layers\": 24,\n",
      "  \"num_heads\": 16,\n",
      "  \"num_layers\": 24,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"relative_attention_max_distance\": 128,\n",
      "  \"relative_attention_num_buckets\": 32,\n",
      "  \"task_specific_params\": {\n",
      "    \"summarization\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"length_penalty\": 2.0,\n",
      "      \"max_length\": 200,\n",
      "      \"min_length\": 30,\n",
      "      \"no_repeat_ngram_size\": 3,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"summarize: \"\n",
      "    },\n",
      "    \"translation_en_to_de\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"max_length\": 300,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"translate English to German: \"\n",
      "    },\n",
      "    \"translation_en_to_fr\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"max_length\": 300,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"translate English to French: \"\n",
      "    },\n",
      "    \"translation_en_to_ro\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"max_length\": 300,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"translate English to Romanian: \"\n",
      "    }\n",
      "  },\n",
      "  \"transformers_version\": \"4.28.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32128\n",
      "}\n",
      "\n",
      "[INFO|tokenization_auto.py:502] 2024-05-21 01:50:18,891 >> Could not locate the tokenizer configuration file, will try to use the model config instead.\n",
      "[INFO|configuration_utils.py:668] 2024-05-21 01:50:18,937 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--t5-large/snapshots/150ebc2c4b72291e770f58e6057481c8d2ed331a/config.json\n",
      "[INFO|configuration_utils.py:720] 2024-05-21 01:50:18,938 >> Model config T5Config {\n",
      "  \"_name_or_path\": \"t5-large\",\n",
      "  \"architectures\": [\n",
      "    \"T5ForConditionalGeneration\"\n",
      "  ],\n",
      "  \"d_ff\": 4096,\n",
      "  \"d_kv\": 64,\n",
      "  \"d_model\": 1024,\n",
      "  \"decoder_start_token_id\": 0,\n",
      "  \"dense_act_fn\": \"relu\",\n",
      "  \"dropout_rate\": 0.1,\n",
      "  \"eos_token_id\": 1,\n",
      "  \"feed_forward_proj\": \"relu\",\n",
      "  \"initializer_factor\": 1.0,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"is_gated_act\": false,\n",
      "  \"layer_norm_epsilon\": 1e-06,\n",
      "  \"model_type\": \"t5\",\n",
      "  \"n_positions\": 512,\n",
      "  \"num_decoder_layers\": 24,\n",
      "  \"num_heads\": 16,\n",
      "  \"num_layers\": 24,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"relative_attention_max_distance\": 128,\n",
      "  \"relative_attention_num_buckets\": 32,\n",
      "  \"task_specific_params\": {\n",
      "    \"summarization\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"length_penalty\": 2.0,\n",
      "      \"max_length\": 200,\n",
      "      \"min_length\": 30,\n",
      "      \"no_repeat_ngram_size\": 3,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"summarize: \"\n",
      "    },\n",
      "    \"translation_en_to_de\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"max_length\": 300,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"translate English to German: \"\n",
      "    },\n",
      "    \"translation_en_to_fr\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"max_length\": 300,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"translate English to French: \"\n",
      "    },\n",
      "    \"translation_en_to_ro\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"max_length\": 300,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"translate English to Romanian: \"\n",
      "    }\n",
      "  },\n",
      "  \"transformers_version\": \"4.28.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32128\n",
      "}\n",
      "\n",
      "[INFO|tokenization_utils_base.py:1809] 2024-05-21 01:50:19,048 >> loading file spiece.model from cache at /root/.cache/huggingface/hub/models--t5-large/snapshots/150ebc2c4b72291e770f58e6057481c8d2ed331a/spiece.model\n",
      "[INFO|tokenization_utils_base.py:1809] 2024-05-21 01:50:19,048 >> loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--t5-large/snapshots/150ebc2c4b72291e770f58e6057481c8d2ed331a/tokenizer.json\n",
      "[INFO|tokenization_utils_base.py:1809] 2024-05-21 01:50:19,048 >> loading file added_tokens.json from cache at None\n",
      "[INFO|tokenization_utils_base.py:1809] 2024-05-21 01:50:19,048 >> loading file special_tokens_map.json from cache at None\n",
      "[INFO|tokenization_utils_base.py:1809] 2024-05-21 01:50:19,048 >> loading file tokenizer_config.json from cache at None\n",
      "[INFO|configuration_utils.py:668] 2024-05-21 01:50:19,049 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--t5-large/snapshots/150ebc2c4b72291e770f58e6057481c8d2ed331a/config.json\n",
      "[INFO|configuration_utils.py:720] 2024-05-21 01:50:19,049 >> Model config T5Config {\n",
      "  \"_name_or_path\": \"t5-large\",\n",
      "  \"architectures\": [\n",
      "    \"T5ForConditionalGeneration\"\n",
      "  ],\n",
      "  \"d_ff\": 4096,\n",
      "  \"d_kv\": 64,\n",
      "  \"d_model\": 1024,\n",
      "  \"decoder_start_token_id\": 0,\n",
      "  \"dense_act_fn\": \"relu\",\n",
      "  \"dropout_rate\": 0.1,\n",
      "  \"eos_token_id\": 1,\n",
      "  \"feed_forward_proj\": \"relu\",\n",
      "  \"initializer_factor\": 1.0,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"is_gated_act\": false,\n",
      "  \"layer_norm_epsilon\": 1e-06,\n",
      "  \"model_type\": \"t5\",\n",
      "  \"n_positions\": 512,\n",
      "  \"num_decoder_layers\": 24,\n",
      "  \"num_heads\": 16,\n",
      "  \"num_layers\": 24,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"relative_attention_max_distance\": 128,\n",
      "  \"relative_attention_num_buckets\": 32,\n",
      "  \"task_specific_params\": {\n",
      "    \"summarization\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"length_penalty\": 2.0,\n",
      "      \"max_length\": 200,\n",
      "      \"min_length\": 30,\n",
      "      \"no_repeat_ngram_size\": 3,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"summarize: \"\n",
      "    },\n",
      "    \"translation_en_to_de\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"max_length\": 300,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"translate English to German: \"\n",
      "    },\n",
      "    \"translation_en_to_fr\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"max_length\": 300,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"translate English to French: \"\n",
      "    },\n",
      "    \"translation_en_to_ro\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"max_length\": 300,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"translate English to Romanian: \"\n",
      "    }\n",
      "  },\n",
      "  \"transformers_version\": \"4.28.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32128\n",
      "}\n",
      "\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/models/t5/tokenization_t5_fast.py:155: FutureWarning: This tokenizer was incorrectly instantiated with a model max length of 512 which will be corrected in Transformers v5.\n",
      "For now, this behavior is kept to avoid breaking backwards compatibility when padding/encoding with `truncation is True`.\n",
      "- Be aware that you SHOULD NOT rely on t5-large automatically truncating your input to 512 when padding/encoding.\n",
      "- If you want to encode/pad to sequences longer than 512 you can either instantiate this tokenizer with `model_max_length` or pass `max_length` when encoding/padding.\n",
      "- To avoid this warning, please instantiate this tokenizer with `model_max_length` set to your preferred value.\n",
      "  warnings.warn(\n",
      "[INFO|modeling_utils.py:2534] 2024-05-21 01:50:19,122 >> loading weights file model.safetensors from cache at /root/.cache/huggingface/hub/models--t5-large/snapshots/150ebc2c4b72291e770f58e6057481c8d2ed331a/model.safetensors\n",
      "[INFO|configuration_utils.py:575] 2024-05-21 01:50:19,502 >> Generate config GenerationConfig {\n",
      "  \"_from_model_config\": true,\n",
      "  \"decoder_start_token_id\": 0,\n",
      "  \"eos_token_id\": 1,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"transformers_version\": \"4.28.1\"\n",
      "}\n",
      "\n",
      "[INFO|modeling_utils.py:3190] 2024-05-21 01:50:58,902 >> All model checkpoint weights were used when initializing DeployT5ForConditionalGeneration.\n",
      "\n",
      "[INFO|modeling_utils.py:3198] 2024-05-21 01:50:58,902 >> All the weights of DeployT5ForConditionalGeneration were initialized from the model checkpoint at t5-large.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use DeployT5ForConditionalGeneration for predictions without further training.\n",
      "[INFO|configuration_utils.py:537] 2024-05-21 01:50:59,152 >> loading configuration file generation_config.json from cache at /root/.cache/huggingface/hub/models--t5-large/snapshots/150ebc2c4b72291e770f58e6057481c8d2ed331a/generation_config.json\n",
      "[INFO|configuration_utils.py:575] 2024-05-21 01:50:59,152 >> Generate config GenerationConfig {\n",
      "  \"_from_model_config\": true,\n",
      "  \"decoder_start_token_id\": 0,\n",
      "  \"eos_token_id\": 1,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"transformers_version\": \"4.28.1\"\n",
      "}\n",
      "\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/cnn_dailymail/3.0.0/0.0.0/96df5e686bee6baa90b8bee7c28b81fa3fa6223d/cache-6246a45506458069.arrow\n",
      "05/21/2024 01:50:59 - INFO - datasets.arrow_dataset - Loading cached processed dataset at /root/.cache/huggingface/datasets/cnn_dailymail/3.0.0/0.0.0/96df5e686bee6baa90b8bee7c28b81fa3fa6223d/cache-6246a45506458069.arrow\n",
      "05/21/2024 01:51:01 - INFO - __main__ - *** Evaluate ***\n",
      "[WARNING|logging.py:280] 2024-05-21 01:51:01,139 >> You're using a T5TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "[INFO|configuration_utils.py:575] 2024-05-21 01:51:01,149 >> Generate config GenerationConfig {\n",
      "  \"_from_model_config\": true,\n",
      "  \"decoder_start_token_id\": 0,\n",
      "  \"eos_token_id\": 1,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"transformers_version\": \"4.28.1\"\n",
      "}\n",
      "\n",
      "100% 40/40 [02:21<00:00,  3.55s/it][WARNING|integrations.py:649] 2024-05-21 01:53:27,933 >> Trainer is attempting to log a value of \"0:00:00\" of type <class 'str'> for key \"train/time_encoder_forward\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n",
      "[WARNING|integrations.py:649] 2024-05-21 01:53:27,933 >> Trainer is attempting to log a value of \"0:02:14\" of type <class 'str'> for key \"train/time_decoder_forward\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n",
      "[WARNING|integrations.py:649] 2024-05-21 01:53:27,933 >> Trainer is attempting to log a value of \"['0:00:01', '0:00:00']\" of type <class 'str'> for key \"train/time_key_value_gen\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n",
      "[WARNING|integrations.py:649] 2024-05-21 01:53:27,934 >> Trainer is attempting to log a value of \"['0:00:05', '0:00:03']\" of type <class 'str'> for key \"train/time_attn\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n",
      "[WARNING|integrations.py:649] 2024-05-21 01:53:27,934 >> Trainer is attempting to log a value of \"0:00:01\" of type <class 'str'> for key \"train/time_ffn\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n",
      "[WARNING|integrations.py:649] 2024-05-21 01:53:27,934 >> Trainer is attempting to log a value of \"0:00:05\" of type <class 'str'> for key \"train/time_confidence\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n",
      "[WARNING|integrations.py:649] 2024-05-21 01:53:27,934 >> Trainer is attempting to log a value of \"['0:00:26', '0:00:03']\" of type <class 'str'> for key \"train/time_exit_key_value_gen\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n",
      "[WARNING|integrations.py:649] 2024-05-21 01:53:27,934 >> Trainer is attempting to log a value of \"['0:00:29', '0:00:10']\" of type <class 'str'> for key \"train/time_exit_attn\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n",
      "[WARNING|integrations.py:649] 2024-05-21 01:53:27,934 >> Trainer is attempting to log a value of \"0:00:03\" of type <class 'str'> for key \"train/time_exit_ffn\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n",
      "[WARNING|integrations.py:649] 2024-05-21 01:53:27,934 >> Trainer is attempting to log a value of \"['0:00:00', '0:00:00']\" of type <class 'str'> for key \"train/time_parallel_key_value_gen\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n",
      "[WARNING|integrations.py:649] 2024-05-21 01:53:27,934 >> Trainer is attempting to log a value of \"['0:00:00', '0:00:00']\" of type <class 'str'> for key \"train/time_parallel_attn\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n",
      "[WARNING|integrations.py:649] 2024-05-21 01:53:27,934 >> Trainer is attempting to log a value of \"0:00:00\" of type <class 'str'> for key \"train/time_parallel_ffn\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n",
      "[WARNING|integrations.py:649] 2024-05-21 01:53:27,934 >> Trainer is attempting to log a value of \"0:00:00\" of type <class 'str'> for key \"train/time_estimate_conf\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n",
      "[WARNING|integrations.py:649] 2024-05-21 01:53:27,934 >> Trainer is attempting to log a value of \"0:00:04\" of type <class 'str'> for key \"train/time_others\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n",
      "100% 40/40 [02:21<00:00,  3.54s/it]\n",
      "***** eval metrics *****\n",
      "  eval_block_avg              =                    1.0\n",
      "  eval_gen_len                =                    0.0\n",
      "  eval_rouge1                 =                    0.0\n",
      "  eval_rouge2                 =                    0.0\n",
      "  eval_rougeL                 =                    0.0\n",
      "  eval_rougeLsum              =                    0.0\n",
      "  eval_runtime                =             0:02:26.80\n",
      "  eval_samples                =                     40\n",
      "  eval_samples_per_second     =                  0.272\n",
      "  eval_steps_per_second       =                  0.272\n",
      "  time_attn                   = ['0:00:05', '0:00:03']\n",
      "  time_confidence             =                0:00:05\n",
      "  time_decoder_forward        =                0:02:14\n",
      "  time_encoder_forward        =                0:00:00\n",
      "  time_estimate_conf          =                0:00:00\n",
      "  time_exit_attn              = ['0:00:29', '0:00:10']\n",
      "  time_exit_ffn               =                0:00:03\n",
      "  time_exit_key_value_gen     = ['0:00:26', '0:00:03']\n",
      "  time_ffn                    =                0:00:01\n",
      "  time_key_value_gen          = ['0:00:01', '0:00:00']\n",
      "  time_others                 =                0:00:04\n",
      "  time_parallel_attn          = ['0:00:00', '0:00:00']\n",
      "  time_parallel_ffn           =                0:00:00\n",
      "  time_parallel_key_value_gen = ['0:00:00', '0:00:00']\n",
      "[INFO|modelcard.py:451] 2024-05-21 01:53:28,035 >> Dropping the following result as it does not have all the necessary fields:\n",
      "{'task': {'name': 'Summarization', 'type': 'summarization'}, 'dataset': {'name': 'cnn_dailymail 3.0.0', 'type': 'cnn_dailymail', 'args': '3.0.0'}}\n"
     ]
    }
   ],
   "source": [
    "!python -m torch.distributed.run --nproc_per_node=1 \\\n",
    "    src/run_summarization.py \\\n",
    "    --model_name_or_path t5-large \\\n",
    "    --do_eval \\\n",
    "    --dataset_name cnn_dailymail \\\n",
    "    --dataset_config_name \"3.0.0\" \\\n",
    "    --output_dir ./save/cnndm_t5_large/ \\\n",
    "    --deploy_scenario True \\\n",
    "    --use_synchronize True \\\n",
    "    --per_device_train_batch_size 4 \\\n",
    "    --per_device_eval_batch_size 1 \\\n",
    "    --overwrite_output_dir \\\n",
    "    --predict_with_generate \\\n",
    "    --source_prefix \"summarize: \" \\\n",
    "    --num_train_epochs 10 \\\n",
    "    --output_hidden_states_decoder True \\\n",
    "    --intermediate_loss_fn weighted_ce \\\n",
    "    --use_early_exit True \\\n",
    "    --exit_conf_type softmax \\\n",
    "    --exit_position_temp 4 \\\n",
    "    --exit_conf_threshold 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TbcqnwQrmuDG",
    "outputId": "961ae651-e765-4783-f723-8bc954b1941e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-05-21 01:53:42.174213: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-05-21 01:53:42.174267: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-05-21 01:53:42.175697: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-05-21 01:53:43.936173: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n",
      "05/21/2024 01:53:47 - WARNING - __main__ - Process rank: 0, device: cuda:0, n_gpu: 1distributed training: True, 16-bits training: False\n",
      "05/21/2024 01:53:47 - INFO - __main__ - Training/evaluation parameters Seq2SeqTrainingArguments(\n",
      "_n_gpu=1,\n",
      "adafactor=False,\n",
      "adam_beta1=0.9,\n",
      "adam_beta2=0.999,\n",
      "adam_epsilon=1e-08,\n",
      "auto_find_batch_size=False,\n",
      "bf16=False,\n",
      "bf16_full_eval=False,\n",
      "data_seed=None,\n",
      "dataloader_drop_last=False,\n",
      "dataloader_num_workers=0,\n",
      "dataloader_pin_memory=True,\n",
      "ddp_bucket_cap_mb=None,\n",
      "ddp_find_unused_parameters=None,\n",
      "ddp_timeout=1800,\n",
      "debug=[],\n",
      "deepspeed=None,\n",
      "disable_tqdm=False,\n",
      "do_eval=True,\n",
      "do_predict=False,\n",
      "do_train=False,\n",
      "eval_accumulation_steps=None,\n",
      "eval_delay=0,\n",
      "eval_steps=None,\n",
      "evaluation_strategy=no,\n",
      "fp16=False,\n",
      "fp16_backend=auto,\n",
      "fp16_full_eval=False,\n",
      "fp16_opt_level=O1,\n",
      "fsdp=[],\n",
      "fsdp_config={'fsdp_min_num_params': 0, 'xla': False, 'xla_fsdp_grad_ckpt': False},\n",
      "fsdp_min_num_params=0,\n",
      "fsdp_transformer_layer_cls_to_wrap=None,\n",
      "full_determinism=False,\n",
      "generation_config=None,\n",
      "generation_max_length=None,\n",
      "generation_num_beams=None,\n",
      "gradient_accumulation_steps=1,\n",
      "gradient_checkpointing=False,\n",
      "greater_is_better=None,\n",
      "group_by_length=False,\n",
      "half_precision_backend=auto,\n",
      "hub_model_id=None,\n",
      "hub_private_repo=False,\n",
      "hub_strategy=every_save,\n",
      "hub_token=<HUB_TOKEN>,\n",
      "ignore_data_skip=False,\n",
      "include_inputs_for_metrics=False,\n",
      "jit_mode_eval=False,\n",
      "label_names=None,\n",
      "label_smoothing_factor=0.0,\n",
      "learning_rate=5e-05,\n",
      "length_column_name=length,\n",
      "load_best_model_at_end=False,\n",
      "local_rank=0,\n",
      "log_level=passive,\n",
      "log_level_replica=warning,\n",
      "log_on_each_node=True,\n",
      "logging_dir=./save/cnndm_t5_large/runs/May21_01-53-47_b56da3cfb72c,\n",
      "logging_first_step=False,\n",
      "logging_nan_inf_filter=True,\n",
      "logging_steps=500,\n",
      "logging_strategy=steps,\n",
      "lr_scheduler_type=linear,\n",
      "max_grad_norm=1.0,\n",
      "max_steps=-1,\n",
      "metric_for_best_model=None,\n",
      "mp_parameters=,\n",
      "no_cuda=False,\n",
      "num_train_epochs=10.0,\n",
      "optim=adamw_hf,\n",
      "optim_args=None,\n",
      "output_dir=./save/cnndm_t5_large/,\n",
      "overwrite_output_dir=True,\n",
      "past_index=-1,\n",
      "per_device_eval_batch_size=1,\n",
      "per_device_train_batch_size=4,\n",
      "predict_with_generate=True,\n",
      "prediction_loss_only=False,\n",
      "push_to_hub=False,\n",
      "push_to_hub_model_id=None,\n",
      "push_to_hub_organization=None,\n",
      "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
      "ray_scope=last,\n",
      "remove_unused_columns=True,\n",
      "report_to=['tensorboard'],\n",
      "resume_from_checkpoint=None,\n",
      "run_name=./save/cnndm_t5_large/,\n",
      "save_on_each_node=False,\n",
      "save_safetensors=False,\n",
      "save_steps=500,\n",
      "save_strategy=steps,\n",
      "save_total_limit=None,\n",
      "seed=42,\n",
      "sharded_ddp=[],\n",
      "skip_memory_metrics=True,\n",
      "sortish_sampler=False,\n",
      "tf32=None,\n",
      "torch_compile=False,\n",
      "torch_compile_backend=None,\n",
      "torch_compile_mode=None,\n",
      "torchdynamo=None,\n",
      "tpu_metrics_debug=False,\n",
      "tpu_num_cores=None,\n",
      "use_ipex=False,\n",
      "use_legacy_prediction_loop=False,\n",
      "use_mps_device=False,\n",
      "warmup_ratio=0.0,\n",
      "warmup_steps=0,\n",
      "weight_decay=0.0,\n",
      "xpu_backend=None,\n",
      ")\n",
      "/usr/local/lib/python3.10/dist-packages/datasets/load.py:2547: FutureWarning: 'use_auth_token' was deprecated in favor of 'token' in version 2.14.0 and will be removed in 3.0.0.\n",
      "You can remove this warning by passing 'token=<use_auth_token>' instead.\n",
      "  warnings.warn(\n",
      "Overwrite dataset info from restored data version if exists.\n",
      "05/21/2024 01:53:49 - INFO - datasets.builder - Overwrite dataset info from restored data version if exists.\n",
      "Loading Dataset info from /root/.cache/huggingface/datasets/cnn_dailymail/3.0.0/0.0.0/96df5e686bee6baa90b8bee7c28b81fa3fa6223d\n",
      "05/21/2024 01:53:49 - INFO - datasets.info - Loading Dataset info from /root/.cache/huggingface/datasets/cnn_dailymail/3.0.0/0.0.0/96df5e686bee6baa90b8bee7c28b81fa3fa6223d\n",
      "Found cached dataset cnn_dailymail (/root/.cache/huggingface/datasets/cnn_dailymail/3.0.0/0.0.0/96df5e686bee6baa90b8bee7c28b81fa3fa6223d)\n",
      "05/21/2024 01:53:49 - INFO - datasets.builder - Found cached dataset cnn_dailymail (/root/.cache/huggingface/datasets/cnn_dailymail/3.0.0/0.0.0/96df5e686bee6baa90b8bee7c28b81fa3fa6223d)\n",
      "Loading Dataset info from /root/.cache/huggingface/datasets/cnn_dailymail/3.0.0/0.0.0/96df5e686bee6baa90b8bee7c28b81fa3fa6223d\n",
      "05/21/2024 01:53:49 - INFO - datasets.info - Loading Dataset info from /root/.cache/huggingface/datasets/cnn_dailymail/3.0.0/0.0.0/96df5e686bee6baa90b8bee7c28b81fa3fa6223d\n",
      "/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "[INFO|configuration_utils.py:668] 2024-05-21 01:53:51,949 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--t5-large/snapshots/150ebc2c4b72291e770f58e6057481c8d2ed331a/config.json\n",
      "[INFO|configuration_utils.py:720] 2024-05-21 01:53:51,950 >> Model config T5Config {\n",
      "  \"_name_or_path\": \"t5-large\",\n",
      "  \"architectures\": [\n",
      "    \"T5ForConditionalGeneration\"\n",
      "  ],\n",
      "  \"d_ff\": 4096,\n",
      "  \"d_kv\": 64,\n",
      "  \"d_model\": 1024,\n",
      "  \"decoder_start_token_id\": 0,\n",
      "  \"dense_act_fn\": \"relu\",\n",
      "  \"dropout_rate\": 0.1,\n",
      "  \"eos_token_id\": 1,\n",
      "  \"feed_forward_proj\": \"relu\",\n",
      "  \"initializer_factor\": 1.0,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"is_gated_act\": false,\n",
      "  \"layer_norm_epsilon\": 1e-06,\n",
      "  \"model_type\": \"t5\",\n",
      "  \"n_positions\": 512,\n",
      "  \"num_decoder_layers\": 24,\n",
      "  \"num_heads\": 16,\n",
      "  \"num_layers\": 24,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"relative_attention_max_distance\": 128,\n",
      "  \"relative_attention_num_buckets\": 32,\n",
      "  \"task_specific_params\": {\n",
      "    \"summarization\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"length_penalty\": 2.0,\n",
      "      \"max_length\": 200,\n",
      "      \"min_length\": 30,\n",
      "      \"no_repeat_ngram_size\": 3,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"summarize: \"\n",
      "    },\n",
      "    \"translation_en_to_de\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"max_length\": 300,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"translate English to German: \"\n",
      "    },\n",
      "    \"translation_en_to_fr\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"max_length\": 300,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"translate English to French: \"\n",
      "    },\n",
      "    \"translation_en_to_ro\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"max_length\": 300,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"translate English to Romanian: \"\n",
      "    }\n",
      "  },\n",
      "  \"transformers_version\": \"4.28.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32128\n",
      "}\n",
      "\n",
      "[INFO|tokenization_auto.py:502] 2024-05-21 01:53:52,009 >> Could not locate the tokenizer configuration file, will try to use the model config instead.\n",
      "[INFO|configuration_utils.py:668] 2024-05-21 01:53:52,068 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--t5-large/snapshots/150ebc2c4b72291e770f58e6057481c8d2ed331a/config.json\n",
      "[INFO|configuration_utils.py:720] 2024-05-21 01:53:52,069 >> Model config T5Config {\n",
      "  \"_name_or_path\": \"t5-large\",\n",
      "  \"architectures\": [\n",
      "    \"T5ForConditionalGeneration\"\n",
      "  ],\n",
      "  \"d_ff\": 4096,\n",
      "  \"d_kv\": 64,\n",
      "  \"d_model\": 1024,\n",
      "  \"decoder_start_token_id\": 0,\n",
      "  \"dense_act_fn\": \"relu\",\n",
      "  \"dropout_rate\": 0.1,\n",
      "  \"eos_token_id\": 1,\n",
      "  \"feed_forward_proj\": \"relu\",\n",
      "  \"initializer_factor\": 1.0,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"is_gated_act\": false,\n",
      "  \"layer_norm_epsilon\": 1e-06,\n",
      "  \"model_type\": \"t5\",\n",
      "  \"n_positions\": 512,\n",
      "  \"num_decoder_layers\": 24,\n",
      "  \"num_heads\": 16,\n",
      "  \"num_layers\": 24,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"relative_attention_max_distance\": 128,\n",
      "  \"relative_attention_num_buckets\": 32,\n",
      "  \"task_specific_params\": {\n",
      "    \"summarization\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"length_penalty\": 2.0,\n",
      "      \"max_length\": 200,\n",
      "      \"min_length\": 30,\n",
      "      \"no_repeat_ngram_size\": 3,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"summarize: \"\n",
      "    },\n",
      "    \"translation_en_to_de\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"max_length\": 300,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"translate English to German: \"\n",
      "    },\n",
      "    \"translation_en_to_fr\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"max_length\": 300,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"translate English to French: \"\n",
      "    },\n",
      "    \"translation_en_to_ro\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"max_length\": 300,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"translate English to Romanian: \"\n",
      "    }\n",
      "  },\n",
      "  \"transformers_version\": \"4.28.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32128\n",
      "}\n",
      "\n",
      "[INFO|tokenization_utils_base.py:1809] 2024-05-21 01:53:52,171 >> loading file spiece.model from cache at /root/.cache/huggingface/hub/models--t5-large/snapshots/150ebc2c4b72291e770f58e6057481c8d2ed331a/spiece.model\n",
      "[INFO|tokenization_utils_base.py:1809] 2024-05-21 01:53:52,171 >> loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--t5-large/snapshots/150ebc2c4b72291e770f58e6057481c8d2ed331a/tokenizer.json\n",
      "[INFO|tokenization_utils_base.py:1809] 2024-05-21 01:53:52,171 >> loading file added_tokens.json from cache at None\n",
      "[INFO|tokenization_utils_base.py:1809] 2024-05-21 01:53:52,171 >> loading file special_tokens_map.json from cache at None\n",
      "[INFO|tokenization_utils_base.py:1809] 2024-05-21 01:53:52,171 >> loading file tokenizer_config.json from cache at None\n",
      "[INFO|configuration_utils.py:668] 2024-05-21 01:53:52,171 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--t5-large/snapshots/150ebc2c4b72291e770f58e6057481c8d2ed331a/config.json\n",
      "[INFO|configuration_utils.py:720] 2024-05-21 01:53:52,172 >> Model config T5Config {\n",
      "  \"_name_or_path\": \"t5-large\",\n",
      "  \"architectures\": [\n",
      "    \"T5ForConditionalGeneration\"\n",
      "  ],\n",
      "  \"d_ff\": 4096,\n",
      "  \"d_kv\": 64,\n",
      "  \"d_model\": 1024,\n",
      "  \"decoder_start_token_id\": 0,\n",
      "  \"dense_act_fn\": \"relu\",\n",
      "  \"dropout_rate\": 0.1,\n",
      "  \"eos_token_id\": 1,\n",
      "  \"feed_forward_proj\": \"relu\",\n",
      "  \"initializer_factor\": 1.0,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"is_gated_act\": false,\n",
      "  \"layer_norm_epsilon\": 1e-06,\n",
      "  \"model_type\": \"t5\",\n",
      "  \"n_positions\": 512,\n",
      "  \"num_decoder_layers\": 24,\n",
      "  \"num_heads\": 16,\n",
      "  \"num_layers\": 24,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"relative_attention_max_distance\": 128,\n",
      "  \"relative_attention_num_buckets\": 32,\n",
      "  \"task_specific_params\": {\n",
      "    \"summarization\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"length_penalty\": 2.0,\n",
      "      \"max_length\": 200,\n",
      "      \"min_length\": 30,\n",
      "      \"no_repeat_ngram_size\": 3,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"summarize: \"\n",
      "    },\n",
      "    \"translation_en_to_de\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"max_length\": 300,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"translate English to German: \"\n",
      "    },\n",
      "    \"translation_en_to_fr\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"max_length\": 300,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"translate English to French: \"\n",
      "    },\n",
      "    \"translation_en_to_ro\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"max_length\": 300,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"translate English to Romanian: \"\n",
      "    }\n",
      "  },\n",
      "  \"transformers_version\": \"4.28.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32128\n",
      "}\n",
      "\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/models/t5/tokenization_t5_fast.py:155: FutureWarning: This tokenizer was incorrectly instantiated with a model max length of 512 which will be corrected in Transformers v5.\n",
      "For now, this behavior is kept to avoid breaking backwards compatibility when padding/encoding with `truncation is True`.\n",
      "- Be aware that you SHOULD NOT rely on t5-large automatically truncating your input to 512 when padding/encoding.\n",
      "- If you want to encode/pad to sequences longer than 512 you can either instantiate this tokenizer with `model_max_length` or pass `max_length` when encoding/padding.\n",
      "- To avoid this warning, please instantiate this tokenizer with `model_max_length` set to your preferred value.\n",
      "  warnings.warn(\n",
      "[INFO|modeling_utils.py:2534] 2024-05-21 01:53:52,229 >> loading weights file model.safetensors from cache at /root/.cache/huggingface/hub/models--t5-large/snapshots/150ebc2c4b72291e770f58e6057481c8d2ed331a/model.safetensors\n",
      "[INFO|configuration_utils.py:575] 2024-05-21 01:53:52,523 >> Generate config GenerationConfig {\n",
      "  \"_from_model_config\": true,\n",
      "  \"decoder_start_token_id\": 0,\n",
      "  \"eos_token_id\": 1,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"transformers_version\": \"4.28.1\"\n",
      "}\n",
      "\n",
      "[INFO|modeling_utils.py:3190] 2024-05-21 01:54:33,171 >> All model checkpoint weights were used when initializing DeployT5ForConditionalGeneration.\n",
      "\n",
      "[INFO|modeling_utils.py:3198] 2024-05-21 01:54:33,174 >> All the weights of DeployT5ForConditionalGeneration were initialized from the model checkpoint at t5-large.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use DeployT5ForConditionalGeneration for predictions without further training.\n",
      "[INFO|configuration_utils.py:537] 2024-05-21 01:54:33,238 >> loading configuration file generation_config.json from cache at /root/.cache/huggingface/hub/models--t5-large/snapshots/150ebc2c4b72291e770f58e6057481c8d2ed331a/generation_config.json\n",
      "[INFO|configuration_utils.py:575] 2024-05-21 01:54:33,238 >> Generate config GenerationConfig {\n",
      "  \"_from_model_config\": true,\n",
      "  \"decoder_start_token_id\": 0,\n",
      "  \"eos_token_id\": 1,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"transformers_version\": \"4.28.1\"\n",
      "}\n",
      "\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/cnn_dailymail/3.0.0/0.0.0/96df5e686bee6baa90b8bee7c28b81fa3fa6223d/cache-6246a45506458069.arrow\n",
      "05/21/2024 01:54:33 - INFO - datasets.arrow_dataset - Loading cached processed dataset at /root/.cache/huggingface/datasets/cnn_dailymail/3.0.0/0.0.0/96df5e686bee6baa90b8bee7c28b81fa3fa6223d/cache-6246a45506458069.arrow\n",
      "05/21/2024 01:54:35 - INFO - __main__ - *** Evaluate ***\n",
      "[WARNING|logging.py:280] 2024-05-21 01:54:35,029 >> You're using a T5TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "[INFO|configuration_utils.py:575] 2024-05-21 01:54:35,099 >> Generate config GenerationConfig {\n",
      "  \"_from_model_config\": true,\n",
      "  \"decoder_start_token_id\": 0,\n",
      "  \"eos_token_id\": 1,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"transformers_version\": \"4.28.1\"\n",
      "}\n",
      "\n",
      "100% 40/40 [02:19<00:00,  3.49s/it][WARNING|integrations.py:649] 2024-05-21 01:57:01,095 >> Trainer is attempting to log a value of \"0:00:00\" of type <class 'str'> for key \"train/time_encoder_forward\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n",
      "[WARNING|integrations.py:649] 2024-05-21 01:57:01,095 >> Trainer is attempting to log a value of \"0:02:17\" of type <class 'str'> for key \"train/time_decoder_forward\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n",
      "[WARNING|integrations.py:649] 2024-05-21 01:57:01,095 >> Trainer is attempting to log a value of \"['0:00:10', '0:00:01']\" of type <class 'str'> for key \"train/time_key_value_gen\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n",
      "[WARNING|integrations.py:649] 2024-05-21 01:57:01,095 >> Trainer is attempting to log a value of \"['0:00:26', '0:00:25']\" of type <class 'str'> for key \"train/time_attn\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n",
      "[WARNING|integrations.py:649] 2024-05-21 01:57:01,095 >> Trainer is attempting to log a value of \"0:00:17\" of type <class 'str'> for key \"train/time_ffn\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n",
      "[WARNING|integrations.py:649] 2024-05-21 01:57:01,095 >> Trainer is attempting to log a value of \"0:00:33\" of type <class 'str'> for key \"train/time_confidence\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n",
      "[WARNING|integrations.py:649] 2024-05-21 01:57:01,095 >> Trainer is attempting to log a value of \"['0:00:00', '0:00:00']\" of type <class 'str'> for key \"train/time_exit_key_value_gen\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n",
      "[WARNING|integrations.py:649] 2024-05-21 01:57:01,095 >> Trainer is attempting to log a value of \"['0:00:00', '0:00:00']\" of type <class 'str'> for key \"train/time_exit_attn\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n",
      "[WARNING|integrations.py:649] 2024-05-21 01:57:01,095 >> Trainer is attempting to log a value of \"0:00:00\" of type <class 'str'> for key \"train/time_exit_ffn\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n",
      "[WARNING|integrations.py:649] 2024-05-21 01:57:01,095 >> Trainer is attempting to log a value of \"['0:00:00', '0:00:00']\" of type <class 'str'> for key \"train/time_parallel_key_value_gen\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n",
      "[WARNING|integrations.py:649] 2024-05-21 01:57:01,096 >> Trainer is attempting to log a value of \"['0:00:00', '0:00:00']\" of type <class 'str'> for key \"train/time_parallel_attn\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n",
      "[WARNING|integrations.py:649] 2024-05-21 01:57:01,096 >> Trainer is attempting to log a value of \"0:00:00\" of type <class 'str'> for key \"train/time_parallel_ffn\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n",
      "[WARNING|integrations.py:649] 2024-05-21 01:57:01,096 >> Trainer is attempting to log a value of \"0:00:00\" of type <class 'str'> for key \"train/time_estimate_conf\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n",
      "[WARNING|integrations.py:649] 2024-05-21 01:57:01,096 >> Trainer is attempting to log a value of \"0:00:03\" of type <class 'str'> for key \"train/time_others\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n",
      "100% 40/40 [02:20<00:00,  3.50s/it]\n",
      "***** eval metrics *****\n",
      "  eval_block_avg              =                   24.0\n",
      "  eval_gen_len                =                 49.225\n",
      "  eval_rouge1                 =                 39.682\n",
      "  eval_rouge2                 =                18.3808\n",
      "  eval_rougeL                 =                30.3773\n",
      "  eval_rougeLsum              =                36.6978\n",
      "  eval_runtime                =             0:02:26.07\n",
      "  eval_samples                =                     40\n",
      "  eval_samples_per_second     =                  0.274\n",
      "  eval_steps_per_second       =                  0.274\n",
      "  time_attn                   = ['0:00:26', '0:00:25']\n",
      "  time_confidence             =                0:00:33\n",
      "  time_decoder_forward        =                0:02:17\n",
      "  time_encoder_forward        =                0:00:00\n",
      "  time_estimate_conf          =                0:00:00\n",
      "  time_exit_attn              = ['0:00:00', '0:00:00']\n",
      "  time_exit_ffn               =                0:00:00\n",
      "  time_exit_key_value_gen     = ['0:00:00', '0:00:00']\n",
      "  time_ffn                    =                0:00:17\n",
      "  time_key_value_gen          = ['0:00:10', '0:00:01']\n",
      "  time_others                 =                0:00:03\n",
      "  time_parallel_attn          = ['0:00:00', '0:00:00']\n",
      "  time_parallel_ffn           =                0:00:00\n",
      "  time_parallel_key_value_gen = ['0:00:00', '0:00:00']\n",
      "[INFO|modelcard.py:451] 2024-05-21 01:57:01,196 >> Dropping the following result as it does not have all the necessary fields:\n",
      "{'task': {'name': 'Summarization', 'type': 'summarization'}, 'dataset': {'name': 'cnn_dailymail 3.0.0', 'type': 'cnn_dailymail', 'args': '3.0.0'}}\n"
     ]
    }
   ],
   "source": [
    "!python -m torch.distributed.run --nproc_per_node=1 \\\n",
    "    src/run_summarization.py \\\n",
    "    --model_name_or_path t5-large \\\n",
    "    --do_eval \\\n",
    "    --dataset_name cnn_dailymail \\\n",
    "    --dataset_config_name \"3.0.0\" \\\n",
    "    --output_dir ./save/cnndm_t5_large/ \\\n",
    "    --deploy_scenario True \\\n",
    "    --use_synchronize True \\\n",
    "    --per_device_train_batch_size 4 \\\n",
    "    --per_device_eval_batch_size 1 \\\n",
    "    --overwrite_output_dir \\\n",
    "    --predict_with_generate \\\n",
    "    --source_prefix \"summarize: \" \\\n",
    "    --num_train_epochs 10 \\\n",
    "    --output_hidden_states_decoder True \\\n",
    "    --intermediate_loss_fn weighted_ce \\\n",
    "    --use_early_exit True \\\n",
    "    --exit_conf_type hidden_state_saturation \\\n",
    "    --exit_position_temp 4 \\\n",
    "    --exit_conf_threshold 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Ca1iEhAa19g4",
    "outputId": "0d002755-a9bc-4972-af65-15b76d068008"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-05-21 01:57:15.961545: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-05-21 01:57:15.961596: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-05-21 01:57:15.963523: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-05-21 01:57:17.177638: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n",
      "05/21/2024 01:57:20 - WARNING - __main__ - Process rank: 0, device: cuda:0, n_gpu: 1distributed training: True, 16-bits training: False\n",
      "05/21/2024 01:57:20 - INFO - __main__ - Training/evaluation parameters Seq2SeqTrainingArguments(\n",
      "_n_gpu=1,\n",
      "adafactor=False,\n",
      "adam_beta1=0.9,\n",
      "adam_beta2=0.999,\n",
      "adam_epsilon=1e-08,\n",
      "auto_find_batch_size=False,\n",
      "bf16=False,\n",
      "bf16_full_eval=False,\n",
      "data_seed=None,\n",
      "dataloader_drop_last=False,\n",
      "dataloader_num_workers=0,\n",
      "dataloader_pin_memory=True,\n",
      "ddp_bucket_cap_mb=None,\n",
      "ddp_find_unused_parameters=None,\n",
      "ddp_timeout=1800,\n",
      "debug=[],\n",
      "deepspeed=None,\n",
      "disable_tqdm=False,\n",
      "do_eval=True,\n",
      "do_predict=False,\n",
      "do_train=False,\n",
      "eval_accumulation_steps=None,\n",
      "eval_delay=0,\n",
      "eval_steps=None,\n",
      "evaluation_strategy=no,\n",
      "fp16=False,\n",
      "fp16_backend=auto,\n",
      "fp16_full_eval=False,\n",
      "fp16_opt_level=O1,\n",
      "fsdp=[],\n",
      "fsdp_config={'fsdp_min_num_params': 0, 'xla': False, 'xla_fsdp_grad_ckpt': False},\n",
      "fsdp_min_num_params=0,\n",
      "fsdp_transformer_layer_cls_to_wrap=None,\n",
      "full_determinism=False,\n",
      "generation_config=None,\n",
      "generation_max_length=None,\n",
      "generation_num_beams=None,\n",
      "gradient_accumulation_steps=1,\n",
      "gradient_checkpointing=False,\n",
      "greater_is_better=None,\n",
      "group_by_length=False,\n",
      "half_precision_backend=auto,\n",
      "hub_model_id=None,\n",
      "hub_private_repo=False,\n",
      "hub_strategy=every_save,\n",
      "hub_token=<HUB_TOKEN>,\n",
      "ignore_data_skip=False,\n",
      "include_inputs_for_metrics=False,\n",
      "jit_mode_eval=False,\n",
      "label_names=None,\n",
      "label_smoothing_factor=0.0,\n",
      "learning_rate=5e-05,\n",
      "length_column_name=length,\n",
      "load_best_model_at_end=False,\n",
      "local_rank=0,\n",
      "log_level=passive,\n",
      "log_level_replica=warning,\n",
      "log_on_each_node=True,\n",
      "logging_dir=./save/cnndm_t5_large/runs/May21_01-57-20_b56da3cfb72c,\n",
      "logging_first_step=False,\n",
      "logging_nan_inf_filter=True,\n",
      "logging_steps=500,\n",
      "logging_strategy=steps,\n",
      "lr_scheduler_type=linear,\n",
      "max_grad_norm=1.0,\n",
      "max_steps=-1,\n",
      "metric_for_best_model=None,\n",
      "mp_parameters=,\n",
      "no_cuda=False,\n",
      "num_train_epochs=10.0,\n",
      "optim=adamw_hf,\n",
      "optim_args=None,\n",
      "output_dir=./save/cnndm_t5_large/,\n",
      "overwrite_output_dir=True,\n",
      "past_index=-1,\n",
      "per_device_eval_batch_size=1,\n",
      "per_device_train_batch_size=4,\n",
      "predict_with_generate=True,\n",
      "prediction_loss_only=False,\n",
      "push_to_hub=False,\n",
      "push_to_hub_model_id=None,\n",
      "push_to_hub_organization=None,\n",
      "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
      "ray_scope=last,\n",
      "remove_unused_columns=True,\n",
      "report_to=['tensorboard'],\n",
      "resume_from_checkpoint=None,\n",
      "run_name=./save/cnndm_t5_large/,\n",
      "save_on_each_node=False,\n",
      "save_safetensors=False,\n",
      "save_steps=500,\n",
      "save_strategy=steps,\n",
      "save_total_limit=None,\n",
      "seed=42,\n",
      "sharded_ddp=[],\n",
      "skip_memory_metrics=True,\n",
      "sortish_sampler=False,\n",
      "tf32=None,\n",
      "torch_compile=False,\n",
      "torch_compile_backend=None,\n",
      "torch_compile_mode=None,\n",
      "torchdynamo=None,\n",
      "tpu_metrics_debug=False,\n",
      "tpu_num_cores=None,\n",
      "use_ipex=False,\n",
      "use_legacy_prediction_loop=False,\n",
      "use_mps_device=False,\n",
      "warmup_ratio=0.0,\n",
      "warmup_steps=0,\n",
      "weight_decay=0.0,\n",
      "xpu_backend=None,\n",
      ")\n",
      "/usr/local/lib/python3.10/dist-packages/datasets/load.py:2547: FutureWarning: 'use_auth_token' was deprecated in favor of 'token' in version 2.14.0 and will be removed in 3.0.0.\n",
      "You can remove this warning by passing 'token=<use_auth_token>' instead.\n",
      "  warnings.warn(\n",
      "Overwrite dataset info from restored data version if exists.\n",
      "05/21/2024 01:57:22 - INFO - datasets.builder - Overwrite dataset info from restored data version if exists.\n",
      "Loading Dataset info from /root/.cache/huggingface/datasets/cnn_dailymail/3.0.0/0.0.0/96df5e686bee6baa90b8bee7c28b81fa3fa6223d\n",
      "05/21/2024 01:57:22 - INFO - datasets.info - Loading Dataset info from /root/.cache/huggingface/datasets/cnn_dailymail/3.0.0/0.0.0/96df5e686bee6baa90b8bee7c28b81fa3fa6223d\n",
      "Found cached dataset cnn_dailymail (/root/.cache/huggingface/datasets/cnn_dailymail/3.0.0/0.0.0/96df5e686bee6baa90b8bee7c28b81fa3fa6223d)\n",
      "05/21/2024 01:57:22 - INFO - datasets.builder - Found cached dataset cnn_dailymail (/root/.cache/huggingface/datasets/cnn_dailymail/3.0.0/0.0.0/96df5e686bee6baa90b8bee7c28b81fa3fa6223d)\n",
      "Loading Dataset info from /root/.cache/huggingface/datasets/cnn_dailymail/3.0.0/0.0.0/96df5e686bee6baa90b8bee7c28b81fa3fa6223d\n",
      "05/21/2024 01:57:22 - INFO - datasets.info - Loading Dataset info from /root/.cache/huggingface/datasets/cnn_dailymail/3.0.0/0.0.0/96df5e686bee6baa90b8bee7c28b81fa3fa6223d\n",
      "/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "[INFO|configuration_utils.py:668] 2024-05-21 01:57:22,945 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--t5-large/snapshots/150ebc2c4b72291e770f58e6057481c8d2ed331a/config.json\n",
      "[INFO|configuration_utils.py:720] 2024-05-21 01:57:22,946 >> Model config T5Config {\n",
      "  \"_name_or_path\": \"t5-large\",\n",
      "  \"architectures\": [\n",
      "    \"T5ForConditionalGeneration\"\n",
      "  ],\n",
      "  \"d_ff\": 4096,\n",
      "  \"d_kv\": 64,\n",
      "  \"d_model\": 1024,\n",
      "  \"decoder_start_token_id\": 0,\n",
      "  \"dense_act_fn\": \"relu\",\n",
      "  \"dropout_rate\": 0.1,\n",
      "  \"eos_token_id\": 1,\n",
      "  \"feed_forward_proj\": \"relu\",\n",
      "  \"initializer_factor\": 1.0,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"is_gated_act\": false,\n",
      "  \"layer_norm_epsilon\": 1e-06,\n",
      "  \"model_type\": \"t5\",\n",
      "  \"n_positions\": 512,\n",
      "  \"num_decoder_layers\": 24,\n",
      "  \"num_heads\": 16,\n",
      "  \"num_layers\": 24,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"relative_attention_max_distance\": 128,\n",
      "  \"relative_attention_num_buckets\": 32,\n",
      "  \"task_specific_params\": {\n",
      "    \"summarization\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"length_penalty\": 2.0,\n",
      "      \"max_length\": 200,\n",
      "      \"min_length\": 30,\n",
      "      \"no_repeat_ngram_size\": 3,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"summarize: \"\n",
      "    },\n",
      "    \"translation_en_to_de\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"max_length\": 300,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"translate English to German: \"\n",
      "    },\n",
      "    \"translation_en_to_fr\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"max_length\": 300,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"translate English to French: \"\n",
      "    },\n",
      "    \"translation_en_to_ro\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"max_length\": 300,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"translate English to Romanian: \"\n",
      "    }\n",
      "  },\n",
      "  \"transformers_version\": \"4.28.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32128\n",
      "}\n",
      "\n",
      "[INFO|tokenization_auto.py:502] 2024-05-21 01:57:22,999 >> Could not locate the tokenizer configuration file, will try to use the model config instead.\n",
      "[INFO|configuration_utils.py:668] 2024-05-21 01:57:23,076 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--t5-large/snapshots/150ebc2c4b72291e770f58e6057481c8d2ed331a/config.json\n",
      "[INFO|configuration_utils.py:720] 2024-05-21 01:57:23,077 >> Model config T5Config {\n",
      "  \"_name_or_path\": \"t5-large\",\n",
      "  \"architectures\": [\n",
      "    \"T5ForConditionalGeneration\"\n",
      "  ],\n",
      "  \"d_ff\": 4096,\n",
      "  \"d_kv\": 64,\n",
      "  \"d_model\": 1024,\n",
      "  \"decoder_start_token_id\": 0,\n",
      "  \"dense_act_fn\": \"relu\",\n",
      "  \"dropout_rate\": 0.1,\n",
      "  \"eos_token_id\": 1,\n",
      "  \"feed_forward_proj\": \"relu\",\n",
      "  \"initializer_factor\": 1.0,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"is_gated_act\": false,\n",
      "  \"layer_norm_epsilon\": 1e-06,\n",
      "  \"model_type\": \"t5\",\n",
      "  \"n_positions\": 512,\n",
      "  \"num_decoder_layers\": 24,\n",
      "  \"num_heads\": 16,\n",
      "  \"num_layers\": 24,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"relative_attention_max_distance\": 128,\n",
      "  \"relative_attention_num_buckets\": 32,\n",
      "  \"task_specific_params\": {\n",
      "    \"summarization\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"length_penalty\": 2.0,\n",
      "      \"max_length\": 200,\n",
      "      \"min_length\": 30,\n",
      "      \"no_repeat_ngram_size\": 3,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"summarize: \"\n",
      "    },\n",
      "    \"translation_en_to_de\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"max_length\": 300,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"translate English to German: \"\n",
      "    },\n",
      "    \"translation_en_to_fr\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"max_length\": 300,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"translate English to French: \"\n",
      "    },\n",
      "    \"translation_en_to_ro\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"max_length\": 300,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"translate English to Romanian: \"\n",
      "    }\n",
      "  },\n",
      "  \"transformers_version\": \"4.28.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32128\n",
      "}\n",
      "\n",
      "[INFO|tokenization_utils_base.py:1809] 2024-05-21 01:57:23,180 >> loading file spiece.model from cache at /root/.cache/huggingface/hub/models--t5-large/snapshots/150ebc2c4b72291e770f58e6057481c8d2ed331a/spiece.model\n",
      "[INFO|tokenization_utils_base.py:1809] 2024-05-21 01:57:23,180 >> loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--t5-large/snapshots/150ebc2c4b72291e770f58e6057481c8d2ed331a/tokenizer.json\n",
      "[INFO|tokenization_utils_base.py:1809] 2024-05-21 01:57:23,180 >> loading file added_tokens.json from cache at None\n",
      "[INFO|tokenization_utils_base.py:1809] 2024-05-21 01:57:23,180 >> loading file special_tokens_map.json from cache at None\n",
      "[INFO|tokenization_utils_base.py:1809] 2024-05-21 01:57:23,180 >> loading file tokenizer_config.json from cache at None\n",
      "[INFO|configuration_utils.py:668] 2024-05-21 01:57:23,180 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--t5-large/snapshots/150ebc2c4b72291e770f58e6057481c8d2ed331a/config.json\n",
      "[INFO|configuration_utils.py:720] 2024-05-21 01:57:23,181 >> Model config T5Config {\n",
      "  \"_name_or_path\": \"t5-large\",\n",
      "  \"architectures\": [\n",
      "    \"T5ForConditionalGeneration\"\n",
      "  ],\n",
      "  \"d_ff\": 4096,\n",
      "  \"d_kv\": 64,\n",
      "  \"d_model\": 1024,\n",
      "  \"decoder_start_token_id\": 0,\n",
      "  \"dense_act_fn\": \"relu\",\n",
      "  \"dropout_rate\": 0.1,\n",
      "  \"eos_token_id\": 1,\n",
      "  \"feed_forward_proj\": \"relu\",\n",
      "  \"initializer_factor\": 1.0,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"is_gated_act\": false,\n",
      "  \"layer_norm_epsilon\": 1e-06,\n",
      "  \"model_type\": \"t5\",\n",
      "  \"n_positions\": 512,\n",
      "  \"num_decoder_layers\": 24,\n",
      "  \"num_heads\": 16,\n",
      "  \"num_layers\": 24,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"relative_attention_max_distance\": 128,\n",
      "  \"relative_attention_num_buckets\": 32,\n",
      "  \"task_specific_params\": {\n",
      "    \"summarization\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"length_penalty\": 2.0,\n",
      "      \"max_length\": 200,\n",
      "      \"min_length\": 30,\n",
      "      \"no_repeat_ngram_size\": 3,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"summarize: \"\n",
      "    },\n",
      "    \"translation_en_to_de\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"max_length\": 300,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"translate English to German: \"\n",
      "    },\n",
      "    \"translation_en_to_fr\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"max_length\": 300,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"translate English to French: \"\n",
      "    },\n",
      "    \"translation_en_to_ro\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"max_length\": 300,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"translate English to Romanian: \"\n",
      "    }\n",
      "  },\n",
      "  \"transformers_version\": \"4.28.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32128\n",
      "}\n",
      "\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/models/t5/tokenization_t5_fast.py:155: FutureWarning: This tokenizer was incorrectly instantiated with a model max length of 512 which will be corrected in Transformers v5.\n",
      "For now, this behavior is kept to avoid breaking backwards compatibility when padding/encoding with `truncation is True`.\n",
      "- Be aware that you SHOULD NOT rely on t5-large automatically truncating your input to 512 when padding/encoding.\n",
      "- If you want to encode/pad to sequences longer than 512 you can either instantiate this tokenizer with `model_max_length` or pass `max_length` when encoding/padding.\n",
      "- To avoid this warning, please instantiate this tokenizer with `model_max_length` set to your preferred value.\n",
      "  warnings.warn(\n",
      "[INFO|modeling_utils.py:2534] 2024-05-21 01:57:23,242 >> loading weights file model.safetensors from cache at /root/.cache/huggingface/hub/models--t5-large/snapshots/150ebc2c4b72291e770f58e6057481c8d2ed331a/model.safetensors\n",
      "[INFO|configuration_utils.py:575] 2024-05-21 01:57:23,260 >> Generate config GenerationConfig {\n",
      "  \"_from_model_config\": true,\n",
      "  \"decoder_start_token_id\": 0,\n",
      "  \"eos_token_id\": 1,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"transformers_version\": \"4.28.1\"\n",
      "}\n",
      "\n",
      "[INFO|modeling_utils.py:3190] 2024-05-21 01:58:04,770 >> All model checkpoint weights were used when initializing DeployT5ForConditionalGeneration.\n",
      "\n",
      "[INFO|modeling_utils.py:3198] 2024-05-21 01:58:04,770 >> All the weights of DeployT5ForConditionalGeneration were initialized from the model checkpoint at t5-large.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use DeployT5ForConditionalGeneration for predictions without further training.\n",
      "[INFO|configuration_utils.py:537] 2024-05-21 01:58:04,848 >> loading configuration file generation_config.json from cache at /root/.cache/huggingface/hub/models--t5-large/snapshots/150ebc2c4b72291e770f58e6057481c8d2ed331a/generation_config.json\n",
      "[INFO|configuration_utils.py:575] 2024-05-21 01:58:04,848 >> Generate config GenerationConfig {\n",
      "  \"_from_model_config\": true,\n",
      "  \"decoder_start_token_id\": 0,\n",
      "  \"eos_token_id\": 1,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"transformers_version\": \"4.28.1\"\n",
      "}\n",
      "\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/cnn_dailymail/3.0.0/0.0.0/96df5e686bee6baa90b8bee7c28b81fa3fa6223d/cache-6246a45506458069.arrow\n",
      "05/21/2024 01:58:04 - INFO - datasets.arrow_dataset - Loading cached processed dataset at /root/.cache/huggingface/datasets/cnn_dailymail/3.0.0/0.0.0/96df5e686bee6baa90b8bee7c28b81fa3fa6223d/cache-6246a45506458069.arrow\n",
      "05/21/2024 01:58:06 - INFO - __main__ - *** Evaluate ***\n",
      "[WARNING|logging.py:280] 2024-05-21 01:58:06,436 >> You're using a T5TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "[INFO|configuration_utils.py:575] 2024-05-21 01:58:06,455 >> Generate config GenerationConfig {\n",
      "  \"_from_model_config\": true,\n",
      "  \"decoder_start_token_id\": 0,\n",
      "  \"eos_token_id\": 1,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"transformers_version\": \"4.28.1\"\n",
      "}\n",
      "\n",
      "100% 40/40 [02:21<00:00,  3.66s/it][WARNING|integrations.py:649] 2024-05-21 02:00:33,269 >> Trainer is attempting to log a value of \"0:00:00\" of type <class 'str'> for key \"train/time_encoder_forward\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n",
      "[WARNING|integrations.py:649] 2024-05-21 02:00:33,270 >> Trainer is attempting to log a value of \"0:02:18\" of type <class 'str'> for key \"train/time_decoder_forward\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n",
      "[WARNING|integrations.py:649] 2024-05-21 02:00:33,270 >> Trainer is attempting to log a value of \"['0:00:10', '0:00:01']\" of type <class 'str'> for key \"train/time_key_value_gen\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n",
      "[WARNING|integrations.py:649] 2024-05-21 02:00:33,270 >> Trainer is attempting to log a value of \"['0:00:26', '0:00:25']\" of type <class 'str'> for key \"train/time_attn\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n",
      "[WARNING|integrations.py:649] 2024-05-21 02:00:33,270 >> Trainer is attempting to log a value of \"0:00:17\" of type <class 'str'> for key \"train/time_ffn\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n",
      "[WARNING|integrations.py:649] 2024-05-21 02:00:33,270 >> Trainer is attempting to log a value of \"0:00:33\" of type <class 'str'> for key \"train/time_confidence\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n",
      "[WARNING|integrations.py:649] 2024-05-21 02:00:33,270 >> Trainer is attempting to log a value of \"['0:00:00', '0:00:00']\" of type <class 'str'> for key \"train/time_exit_key_value_gen\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n",
      "[WARNING|integrations.py:649] 2024-05-21 02:00:33,270 >> Trainer is attempting to log a value of \"['0:00:00', '0:00:00']\" of type <class 'str'> for key \"train/time_exit_attn\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n",
      "[WARNING|integrations.py:649] 2024-05-21 02:00:33,270 >> Trainer is attempting to log a value of \"0:00:00\" of type <class 'str'> for key \"train/time_exit_ffn\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n",
      "[WARNING|integrations.py:649] 2024-05-21 02:00:33,270 >> Trainer is attempting to log a value of \"['0:00:00', '0:00:00']\" of type <class 'str'> for key \"train/time_parallel_key_value_gen\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n",
      "[WARNING|integrations.py:649] 2024-05-21 02:00:33,270 >> Trainer is attempting to log a value of \"['0:00:00', '0:00:00']\" of type <class 'str'> for key \"train/time_parallel_attn\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n",
      "[WARNING|integrations.py:649] 2024-05-21 02:00:33,270 >> Trainer is attempting to log a value of \"0:00:00\" of type <class 'str'> for key \"train/time_parallel_ffn\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n",
      "[WARNING|integrations.py:649] 2024-05-21 02:00:33,270 >> Trainer is attempting to log a value of \"0:00:00\" of type <class 'str'> for key \"train/time_estimate_conf\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n",
      "[WARNING|integrations.py:649] 2024-05-21 02:00:33,270 >> Trainer is attempting to log a value of \"0:00:03\" of type <class 'str'> for key \"train/time_others\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n",
      "100% 40/40 [02:21<00:00,  3.55s/it]\n",
      "***** eval metrics *****\n",
      "  eval_block_avg              =                   24.0\n",
      "  eval_gen_len                =                 49.225\n",
      "  eval_rouge1                 =                 39.682\n",
      "  eval_rouge2                 =                18.3808\n",
      "  eval_rougeL                 =                30.3773\n",
      "  eval_rougeLsum              =                36.6978\n",
      "  eval_runtime                =             0:02:26.83\n",
      "  eval_samples                =                     40\n",
      "  eval_samples_per_second     =                  0.272\n",
      "  eval_steps_per_second       =                  0.272\n",
      "  time_attn                   = ['0:00:26', '0:00:25']\n",
      "  time_confidence             =                0:00:33\n",
      "  time_decoder_forward        =                0:02:18\n",
      "  time_encoder_forward        =                0:00:00\n",
      "  time_estimate_conf          =                0:00:00\n",
      "  time_exit_attn              = ['0:00:00', '0:00:00']\n",
      "  time_exit_ffn               =                0:00:00\n",
      "  time_exit_key_value_gen     = ['0:00:00', '0:00:00']\n",
      "  time_ffn                    =                0:00:17\n",
      "  time_key_value_gen          = ['0:00:10', '0:00:01']\n",
      "  time_others                 =                0:00:03\n",
      "  time_parallel_attn          = ['0:00:00', '0:00:00']\n",
      "  time_parallel_ffn           =                0:00:00\n",
      "  time_parallel_key_value_gen = ['0:00:00', '0:00:00']\n",
      "[INFO|modelcard.py:451] 2024-05-21 02:00:33,380 >> Dropping the following result as it does not have all the necessary fields:\n",
      "{'task': {'name': 'Summarization', 'type': 'summarization'}, 'dataset': {'name': 'cnn_dailymail 3.0.0', 'type': 'cnn_dailymail', 'args': '3.0.0'}}\n"
     ]
    }
   ],
   "source": [
    "!python -m torch.distributed.run --nproc_per_node=1 \\\n",
    "    src/run_summarization.py \\\n",
    "    --model_name_or_path t5-large \\\n",
    "    --do_eval \\\n",
    "    --dataset_name cnn_dailymail \\\n",
    "    --dataset_config_name \"3.0.0\" \\\n",
    "    --output_dir ./save/cnndm_t5_large/ \\\n",
    "    --deploy_scenario True \\\n",
    "    --use_synchronize True \\\n",
    "    --per_device_train_batch_size 4 \\\n",
    "    --per_device_eval_batch_size 1 \\\n",
    "    --overwrite_output_dir \\\n",
    "    --predict_with_generate \\\n",
    "    --source_prefix \"summarize: \" \\\n",
    "    --num_train_epochs 10 \\\n",
    "    --output_hidden_states_decoder True \\\n",
    "    --intermediate_loss_fn weighted_ce \\\n",
    "    --use_early_exit True \\\n",
    "    --exit_conf_type last_three_top_prob_heuristic \\\n",
    "    --exit_position_temp 4 \\\n",
    "    --exit_conf_threshold 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CXldsrcF_Sg_",
    "outputId": "86b5042c-75ab-4ff5-bebd-c589655ece2f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-05-21 03:15:12.586579: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-05-21 03:15:12.586631: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-05-21 03:15:12.588065: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-05-21 03:15:13.819236: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n",
      "05/21/2024 03:15:18 - WARNING - __main__ - Process rank: 0, device: cuda:0, n_gpu: 1distributed training: True, 16-bits training: False\n",
      "05/21/2024 03:15:18 - INFO - __main__ - Training/evaluation parameters Seq2SeqTrainingArguments(\n",
      "_n_gpu=1,\n",
      "adafactor=False,\n",
      "adam_beta1=0.9,\n",
      "adam_beta2=0.999,\n",
      "adam_epsilon=1e-08,\n",
      "auto_find_batch_size=False,\n",
      "bf16=False,\n",
      "bf16_full_eval=False,\n",
      "data_seed=None,\n",
      "dataloader_drop_last=False,\n",
      "dataloader_num_workers=0,\n",
      "dataloader_pin_memory=True,\n",
      "ddp_bucket_cap_mb=None,\n",
      "ddp_find_unused_parameters=None,\n",
      "ddp_timeout=1800,\n",
      "debug=[],\n",
      "deepspeed=None,\n",
      "disable_tqdm=False,\n",
      "do_eval=True,\n",
      "do_predict=False,\n",
      "do_train=False,\n",
      "eval_accumulation_steps=None,\n",
      "eval_delay=0,\n",
      "eval_steps=None,\n",
      "evaluation_strategy=no,\n",
      "fp16=False,\n",
      "fp16_backend=auto,\n",
      "fp16_full_eval=False,\n",
      "fp16_opt_level=O1,\n",
      "fsdp=[],\n",
      "fsdp_config={'fsdp_min_num_params': 0, 'xla': False, 'xla_fsdp_grad_ckpt': False},\n",
      "fsdp_min_num_params=0,\n",
      "fsdp_transformer_layer_cls_to_wrap=None,\n",
      "full_determinism=False,\n",
      "generation_config=None,\n",
      "generation_max_length=None,\n",
      "generation_num_beams=None,\n",
      "gradient_accumulation_steps=1,\n",
      "gradient_checkpointing=False,\n",
      "greater_is_better=None,\n",
      "group_by_length=False,\n",
      "half_precision_backend=auto,\n",
      "hub_model_id=None,\n",
      "hub_private_repo=False,\n",
      "hub_strategy=every_save,\n",
      "hub_token=<HUB_TOKEN>,\n",
      "ignore_data_skip=False,\n",
      "include_inputs_for_metrics=False,\n",
      "jit_mode_eval=False,\n",
      "label_names=None,\n",
      "label_smoothing_factor=0.0,\n",
      "learning_rate=5e-05,\n",
      "length_column_name=length,\n",
      "load_best_model_at_end=False,\n",
      "local_rank=0,\n",
      "log_level=passive,\n",
      "log_level_replica=warning,\n",
      "log_on_each_node=True,\n",
      "logging_dir=./save/iwslt_t5_large/runs/May21_03-15-18_b56da3cfb72c,\n",
      "logging_first_step=False,\n",
      "logging_nan_inf_filter=True,\n",
      "logging_steps=500,\n",
      "logging_strategy=steps,\n",
      "lr_scheduler_type=linear,\n",
      "max_grad_norm=1.0,\n",
      "max_steps=-1,\n",
      "metric_for_best_model=None,\n",
      "mp_parameters=,\n",
      "no_cuda=False,\n",
      "num_train_epochs=3.0,\n",
      "optim=adamw_hf,\n",
      "optim_args=None,\n",
      "output_dir=./save/iwslt_t5_large/,\n",
      "overwrite_output_dir=True,\n",
      "past_index=-1,\n",
      "per_device_eval_batch_size=1,\n",
      "per_device_train_batch_size=8,\n",
      "predict_with_generate=True,\n",
      "prediction_loss_only=False,\n",
      "push_to_hub=False,\n",
      "push_to_hub_model_id=None,\n",
      "push_to_hub_organization=None,\n",
      "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
      "ray_scope=last,\n",
      "remove_unused_columns=True,\n",
      "report_to=['tensorboard'],\n",
      "resume_from_checkpoint=None,\n",
      "run_name=./save/iwslt_t5_large/,\n",
      "save_on_each_node=False,\n",
      "save_safetensors=False,\n",
      "save_steps=500,\n",
      "save_strategy=steps,\n",
      "save_total_limit=None,\n",
      "seed=42,\n",
      "sharded_ddp=[],\n",
      "skip_memory_metrics=True,\n",
      "sortish_sampler=False,\n",
      "tf32=None,\n",
      "torch_compile=False,\n",
      "torch_compile_backend=None,\n",
      "torch_compile_mode=None,\n",
      "torchdynamo=None,\n",
      "tpu_metrics_debug=False,\n",
      "tpu_num_cores=None,\n",
      "use_ipex=False,\n",
      "use_legacy_prediction_loop=False,\n",
      "use_mps_device=False,\n",
      "warmup_ratio=0.0,\n",
      "warmup_steps=0,\n",
      "weight_decay=0.0,\n",
      "xpu_backend=None,\n",
      ")\n",
      "/usr/local/lib/python3.10/dist-packages/datasets/load.py:2547: FutureWarning: 'use_auth_token' was deprecated in favor of 'token' in version 2.14.0 and will be removed in 3.0.0.\n",
      "You can remove this warning by passing 'token=<use_auth_token>' instead.\n",
      "  warnings.warn(\n",
      "Overwrite dataset info from restored data version if exists.\n",
      "05/21/2024 03:15:19 - INFO - datasets.builder - Overwrite dataset info from restored data version if exists.\n",
      "Loading Dataset info from /root/.cache/huggingface/datasets/iwslt2017/iwslt2017-de-en/1.0.0/c18a4f81a47ae6fa079fe9d32db288ddde38451d\n",
      "05/21/2024 03:15:19 - INFO - datasets.info - Loading Dataset info from /root/.cache/huggingface/datasets/iwslt2017/iwslt2017-de-en/1.0.0/c18a4f81a47ae6fa079fe9d32db288ddde38451d\n",
      "Found cached dataset iwslt2017 (/root/.cache/huggingface/datasets/iwslt2017/iwslt2017-de-en/1.0.0/c18a4f81a47ae6fa079fe9d32db288ddde38451d)\n",
      "05/21/2024 03:15:20 - INFO - datasets.builder - Found cached dataset iwslt2017 (/root/.cache/huggingface/datasets/iwslt2017/iwslt2017-de-en/1.0.0/c18a4f81a47ae6fa079fe9d32db288ddde38451d)\n",
      "Loading Dataset info from /root/.cache/huggingface/datasets/iwslt2017/iwslt2017-de-en/1.0.0/c18a4f81a47ae6fa079fe9d32db288ddde38451d\n",
      "05/21/2024 03:15:20 - INFO - datasets.info - Loading Dataset info from /root/.cache/huggingface/datasets/iwslt2017/iwslt2017-de-en/1.0.0/c18a4f81a47ae6fa079fe9d32db288ddde38451d\n",
      "/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "[INFO|configuration_utils.py:668] 2024-05-21 03:15:20,608 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--t5-large/snapshots/150ebc2c4b72291e770f58e6057481c8d2ed331a/config.json\n",
      "[INFO|configuration_utils.py:720] 2024-05-21 03:15:20,609 >> Model config T5Config {\n",
      "  \"_name_or_path\": \"t5-large\",\n",
      "  \"architectures\": [\n",
      "    \"T5ForConditionalGeneration\"\n",
      "  ],\n",
      "  \"d_ff\": 4096,\n",
      "  \"d_kv\": 64,\n",
      "  \"d_model\": 1024,\n",
      "  \"decoder_start_token_id\": 0,\n",
      "  \"dense_act_fn\": \"relu\",\n",
      "  \"dropout_rate\": 0.1,\n",
      "  \"eos_token_id\": 1,\n",
      "  \"feed_forward_proj\": \"relu\",\n",
      "  \"initializer_factor\": 1.0,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"is_gated_act\": false,\n",
      "  \"layer_norm_epsilon\": 1e-06,\n",
      "  \"model_type\": \"t5\",\n",
      "  \"n_positions\": 512,\n",
      "  \"num_decoder_layers\": 24,\n",
      "  \"num_heads\": 16,\n",
      "  \"num_layers\": 24,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"relative_attention_max_distance\": 128,\n",
      "  \"relative_attention_num_buckets\": 32,\n",
      "  \"task_specific_params\": {\n",
      "    \"summarization\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"length_penalty\": 2.0,\n",
      "      \"max_length\": 200,\n",
      "      \"min_length\": 30,\n",
      "      \"no_repeat_ngram_size\": 3,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"summarize: \"\n",
      "    },\n",
      "    \"translation_en_to_de\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"max_length\": 300,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"translate English to German: \"\n",
      "    },\n",
      "    \"translation_en_to_fr\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"max_length\": 300,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"translate English to French: \"\n",
      "    },\n",
      "    \"translation_en_to_ro\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"max_length\": 300,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"translate English to Romanian: \"\n",
      "    }\n",
      "  },\n",
      "  \"transformers_version\": \"4.28.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32128\n",
      "}\n",
      "\n",
      "[INFO|tokenization_auto.py:502] 2024-05-21 03:15:20,665 >> Could not locate the tokenizer configuration file, will try to use the model config instead.\n",
      "[INFO|configuration_utils.py:668] 2024-05-21 03:15:20,722 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--t5-large/snapshots/150ebc2c4b72291e770f58e6057481c8d2ed331a/config.json\n",
      "[INFO|configuration_utils.py:720] 2024-05-21 03:15:20,723 >> Model config T5Config {\n",
      "  \"_name_or_path\": \"t5-large\",\n",
      "  \"architectures\": [\n",
      "    \"T5ForConditionalGeneration\"\n",
      "  ],\n",
      "  \"d_ff\": 4096,\n",
      "  \"d_kv\": 64,\n",
      "  \"d_model\": 1024,\n",
      "  \"decoder_start_token_id\": 0,\n",
      "  \"dense_act_fn\": \"relu\",\n",
      "  \"dropout_rate\": 0.1,\n",
      "  \"eos_token_id\": 1,\n",
      "  \"feed_forward_proj\": \"relu\",\n",
      "  \"initializer_factor\": 1.0,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"is_gated_act\": false,\n",
      "  \"layer_norm_epsilon\": 1e-06,\n",
      "  \"model_type\": \"t5\",\n",
      "  \"n_positions\": 512,\n",
      "  \"num_decoder_layers\": 24,\n",
      "  \"num_heads\": 16,\n",
      "  \"num_layers\": 24,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"relative_attention_max_distance\": 128,\n",
      "  \"relative_attention_num_buckets\": 32,\n",
      "  \"task_specific_params\": {\n",
      "    \"summarization\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"length_penalty\": 2.0,\n",
      "      \"max_length\": 200,\n",
      "      \"min_length\": 30,\n",
      "      \"no_repeat_ngram_size\": 3,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"summarize: \"\n",
      "    },\n",
      "    \"translation_en_to_de\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"max_length\": 300,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"translate English to German: \"\n",
      "    },\n",
      "    \"translation_en_to_fr\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"max_length\": 300,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"translate English to French: \"\n",
      "    },\n",
      "    \"translation_en_to_ro\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"max_length\": 300,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"translate English to Romanian: \"\n",
      "    }\n",
      "  },\n",
      "  \"transformers_version\": \"4.28.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32128\n",
      "}\n",
      "\n",
      "[INFO|tokenization_utils_base.py:1809] 2024-05-21 03:15:20,879 >> loading file spiece.model from cache at /root/.cache/huggingface/hub/models--t5-large/snapshots/150ebc2c4b72291e770f58e6057481c8d2ed331a/spiece.model\n",
      "[INFO|tokenization_utils_base.py:1809] 2024-05-21 03:15:20,879 >> loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--t5-large/snapshots/150ebc2c4b72291e770f58e6057481c8d2ed331a/tokenizer.json\n",
      "[INFO|tokenization_utils_base.py:1809] 2024-05-21 03:15:20,879 >> loading file added_tokens.json from cache at None\n",
      "[INFO|tokenization_utils_base.py:1809] 2024-05-21 03:15:20,879 >> loading file special_tokens_map.json from cache at None\n",
      "[INFO|tokenization_utils_base.py:1809] 2024-05-21 03:15:20,879 >> loading file tokenizer_config.json from cache at None\n",
      "[INFO|configuration_utils.py:668] 2024-05-21 03:15:20,879 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--t5-large/snapshots/150ebc2c4b72291e770f58e6057481c8d2ed331a/config.json\n",
      "[INFO|configuration_utils.py:720] 2024-05-21 03:15:20,880 >> Model config T5Config {\n",
      "  \"_name_or_path\": \"t5-large\",\n",
      "  \"architectures\": [\n",
      "    \"T5ForConditionalGeneration\"\n",
      "  ],\n",
      "  \"d_ff\": 4096,\n",
      "  \"d_kv\": 64,\n",
      "  \"d_model\": 1024,\n",
      "  \"decoder_start_token_id\": 0,\n",
      "  \"dense_act_fn\": \"relu\",\n",
      "  \"dropout_rate\": 0.1,\n",
      "  \"eos_token_id\": 1,\n",
      "  \"feed_forward_proj\": \"relu\",\n",
      "  \"initializer_factor\": 1.0,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"is_gated_act\": false,\n",
      "  \"layer_norm_epsilon\": 1e-06,\n",
      "  \"model_type\": \"t5\",\n",
      "  \"n_positions\": 512,\n",
      "  \"num_decoder_layers\": 24,\n",
      "  \"num_heads\": 16,\n",
      "  \"num_layers\": 24,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"relative_attention_max_distance\": 128,\n",
      "  \"relative_attention_num_buckets\": 32,\n",
      "  \"task_specific_params\": {\n",
      "    \"summarization\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"length_penalty\": 2.0,\n",
      "      \"max_length\": 200,\n",
      "      \"min_length\": 30,\n",
      "      \"no_repeat_ngram_size\": 3,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"summarize: \"\n",
      "    },\n",
      "    \"translation_en_to_de\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"max_length\": 300,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"translate English to German: \"\n",
      "    },\n",
      "    \"translation_en_to_fr\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"max_length\": 300,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"translate English to French: \"\n",
      "    },\n",
      "    \"translation_en_to_ro\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"max_length\": 300,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"translate English to Romanian: \"\n",
      "    }\n",
      "  },\n",
      "  \"transformers_version\": \"4.28.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32128\n",
      "}\n",
      "\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/models/t5/tokenization_t5_fast.py:155: FutureWarning: This tokenizer was incorrectly instantiated with a model max length of 512 which will be corrected in Transformers v5.\n",
      "For now, this behavior is kept to avoid breaking backwards compatibility when padding/encoding with `truncation is True`.\n",
      "- Be aware that you SHOULD NOT rely on t5-large automatically truncating your input to 512 when padding/encoding.\n",
      "- If you want to encode/pad to sequences longer than 512 you can either instantiate this tokenizer with `model_max_length` or pass `max_length` when encoding/padding.\n",
      "- To avoid this warning, please instantiate this tokenizer with `model_max_length` set to your preferred value.\n",
      "  warnings.warn(\n",
      "[INFO|modeling_utils.py:2534] 2024-05-21 03:15:20,935 >> loading weights file model.safetensors from cache at /root/.cache/huggingface/hub/models--t5-large/snapshots/150ebc2c4b72291e770f58e6057481c8d2ed331a/model.safetensors\n",
      "[INFO|configuration_utils.py:575] 2024-05-21 03:15:20,953 >> Generate config GenerationConfig {\n",
      "  \"_from_model_config\": true,\n",
      "  \"decoder_start_token_id\": 0,\n",
      "  \"eos_token_id\": 1,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"transformers_version\": \"4.28.1\"\n",
      "}\n",
      "\n",
      "[INFO|modeling_utils.py:3190] 2024-05-21 03:15:57,729 >> All model checkpoint weights were used when initializing DeployT5ForConditionalGeneration.\n",
      "\n",
      "[INFO|modeling_utils.py:3198] 2024-05-21 03:15:57,729 >> All the weights of DeployT5ForConditionalGeneration were initialized from the model checkpoint at t5-large.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use DeployT5ForConditionalGeneration for predictions without further training.\n",
      "[INFO|configuration_utils.py:537] 2024-05-21 03:15:57,791 >> loading configuration file generation_config.json from cache at /root/.cache/huggingface/hub/models--t5-large/snapshots/150ebc2c4b72291e770f58e6057481c8d2ed331a/generation_config.json\n",
      "[INFO|configuration_utils.py:575] 2024-05-21 03:15:57,791 >> Generate config GenerationConfig {\n",
      "  \"_from_model_config\": true,\n",
      "  \"decoder_start_token_id\": 0,\n",
      "  \"eos_token_id\": 1,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"transformers_version\": \"4.28.1\"\n",
      "}\n",
      "\n",
      "Running tokenizer on validation dataset:   0% 0/40 [00:00<?, ? examples/s]Caching processed dataset at /root/.cache/huggingface/datasets/iwslt2017/iwslt2017-de-en/1.0.0/c18a4f81a47ae6fa079fe9d32db288ddde38451d/cache-4b87423bce09feab.arrow\n",
      "05/21/2024 03:15:57 - INFO - datasets.arrow_dataset - Caching processed dataset at /root/.cache/huggingface/datasets/iwslt2017/iwslt2017-de-en/1.0.0/c18a4f81a47ae6fa079fe9d32db288ddde38451d/cache-4b87423bce09feab.arrow\n",
      "Running tokenizer on validation dataset: 100% 40/40 [00:00<00:00, 902.02 examples/s]\n",
      "Downloading builder script: 100% 8.15k/8.15k [00:00<00:00, 26.5MB/s]\n",
      "05/21/2024 03:15:59 - INFO - __main__ - *** Evaluate ***\n",
      "[WARNING|logging.py:280] 2024-05-21 03:15:59,780 >> You're using a T5TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "[INFO|configuration_utils.py:575] 2024-05-21 03:15:59,798 >> Generate config GenerationConfig {\n",
      "  \"_from_model_config\": true,\n",
      "  \"decoder_start_token_id\": 0,\n",
      "  \"eos_token_id\": 1,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"transformers_version\": \"4.28.1\"\n",
      "}\n",
      "\n",
      "100% 40/40 [00:54<00:00,  1.97s/it][WARNING|integrations.py:649] 2024-05-21 03:17:00,169 >> Trainer is attempting to log a value of \"0:00:00\" of type <class 'str'> for key \"train/time_encoder_forward\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n",
      "[WARNING|integrations.py:649] 2024-05-21 03:17:00,170 >> Trainer is attempting to log a value of \"0:00:55\" of type <class 'str'> for key \"train/time_decoder_forward\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n",
      "[WARNING|integrations.py:649] 2024-05-21 03:17:00,170 >> Trainer is attempting to log a value of \"['0:00:05', '0:00:00']\" of type <class 'str'> for key \"train/time_key_value_gen\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n",
      "[WARNING|integrations.py:649] 2024-05-21 03:17:00,170 >> Trainer is attempting to log a value of \"['0:00:13', '0:00:13']\" of type <class 'str'> for key \"train/time_attn\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n",
      "[WARNING|integrations.py:649] 2024-05-21 03:17:00,170 >> Trainer is attempting to log a value of \"0:00:09\" of type <class 'str'> for key \"train/time_ffn\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n",
      "[WARNING|integrations.py:649] 2024-05-21 03:17:00,170 >> Trainer is attempting to log a value of \"0:00:00\" of type <class 'str'> for key \"train/time_confidence\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n",
      "[WARNING|integrations.py:649] 2024-05-21 03:17:00,170 >> Trainer is attempting to log a value of \"['0:00:00', '0:00:00']\" of type <class 'str'> for key \"train/time_exit_key_value_gen\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n",
      "[WARNING|integrations.py:649] 2024-05-21 03:17:00,170 >> Trainer is attempting to log a value of \"['0:00:00', '0:00:00']\" of type <class 'str'> for key \"train/time_exit_attn\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n",
      "[WARNING|integrations.py:649] 2024-05-21 03:17:00,170 >> Trainer is attempting to log a value of \"0:00:00\" of type <class 'str'> for key \"train/time_exit_ffn\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n",
      "[WARNING|integrations.py:649] 2024-05-21 03:17:00,170 >> Trainer is attempting to log a value of \"['0:00:00', '0:00:00']\" of type <class 'str'> for key \"train/time_parallel_key_value_gen\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n",
      "[WARNING|integrations.py:649] 2024-05-21 03:17:00,170 >> Trainer is attempting to log a value of \"['0:00:00', '0:00:00']\" of type <class 'str'> for key \"train/time_parallel_attn\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n",
      "[WARNING|integrations.py:649] 2024-05-21 03:17:00,170 >> Trainer is attempting to log a value of \"0:00:00\" of type <class 'str'> for key \"train/time_parallel_ffn\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n",
      "[WARNING|integrations.py:649] 2024-05-21 03:17:00,170 >> Trainer is attempting to log a value of \"0:00:00\" of type <class 'str'> for key \"train/time_estimate_conf\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n",
      "[WARNING|integrations.py:649] 2024-05-21 03:17:00,170 >> Trainer is attempting to log a value of \"0:00:02\" of type <class 'str'> for key \"train/time_others\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n",
      "100% 40/40 [00:54<00:00,  1.37s/it]\n",
      "***** eval metrics *****\n",
      "  eval_bleu                   =                 0.8452\n",
      "  eval_block_avg              =                   24.0\n",
      "  eval_gen_len                =                   27.3\n",
      "  eval_runtime                =             0:01:00.33\n",
      "  eval_samples                =                     40\n",
      "  eval_samples_per_second     =                  0.663\n",
      "  eval_steps_per_second       =                  0.663\n",
      "  time_attn                   = ['0:00:13', '0:00:13']\n",
      "  time_confidence             =                0:00:00\n",
      "  time_decoder_forward        =                0:00:55\n",
      "  time_encoder_forward        =                0:00:00\n",
      "  time_estimate_conf          =                0:00:00\n",
      "  time_exit_attn              = ['0:00:00', '0:00:00']\n",
      "  time_exit_ffn               =                0:00:00\n",
      "  time_exit_key_value_gen     = ['0:00:00', '0:00:00']\n",
      "  time_ffn                    =                0:00:09\n",
      "  time_key_value_gen          = ['0:00:05', '0:00:00']\n",
      "  time_others                 =                0:00:02\n",
      "  time_parallel_attn          = ['0:00:00', '0:00:00']\n",
      "  time_parallel_ffn           =                0:00:00\n",
      "  time_parallel_key_value_gen = ['0:00:00', '0:00:00']\n",
      "[INFO|modelcard.py:451] 2024-05-21 03:17:00,264 >> Dropping the following result as it does not have all the necessary fields:\n",
      "{'task': {'name': 'Translation', 'type': 'translation'}, 'dataset': {'name': 'iwslt2017 iwslt2017-de-en', 'type': 'iwslt2017', 'args': 'iwslt2017-de-en'}}\n"
     ]
    }
   ],
   "source": [
    "!python -m torch.distributed.run --nproc_per_node=1 \\\n",
    "    src/run_translation.py \\\n",
    "    --model_name_or_path t5-large \\\n",
    "    --do_eval \\\n",
    "    --dataset_name iwslt2017 \\\n",
    "    --dataset_config_name iwslt2017-de-en \\\n",
    "    --output_dir ./save/iwslt_t5_large/ \\\n",
    "    --per_device_eval_batch_size 1 \\\n",
    "    --deploy_scenario True \\\n",
    "    --use_synchronize True \\\n",
    "    --overwrite_output_dir \\\n",
    "    --predict_with_generate \\\n",
    "    --source_prefix \"translate German to English: \" \\\n",
    "    --output_hidden_states_decoder True \\\n",
    "    --use_early_exit False \\\n",
    "    --source_lang \"de\" \\\n",
    "    --target_lang \"en\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Qr_nfpTUKB1X",
    "outputId": "20d4dea7-cc51-47f7-8c8a-3381c5db0a18"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-05-21 03:19:30.834268: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-05-21 03:19:30.834321: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-05-21 03:19:30.835923: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-05-21 03:19:32.056077: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n",
      "05/21/2024 03:19:36 - WARNING - __main__ - Process rank: 0, device: cuda:0, n_gpu: 1distributed training: True, 16-bits training: False\n",
      "05/21/2024 03:19:36 - INFO - __main__ - Training/evaluation parameters Seq2SeqTrainingArguments(\n",
      "_n_gpu=1,\n",
      "adafactor=False,\n",
      "adam_beta1=0.9,\n",
      "adam_beta2=0.999,\n",
      "adam_epsilon=1e-08,\n",
      "auto_find_batch_size=False,\n",
      "bf16=False,\n",
      "bf16_full_eval=False,\n",
      "data_seed=None,\n",
      "dataloader_drop_last=False,\n",
      "dataloader_num_workers=0,\n",
      "dataloader_pin_memory=True,\n",
      "ddp_bucket_cap_mb=None,\n",
      "ddp_find_unused_parameters=None,\n",
      "ddp_timeout=1800,\n",
      "debug=[],\n",
      "deepspeed=None,\n",
      "disable_tqdm=False,\n",
      "do_eval=True,\n",
      "do_predict=False,\n",
      "do_train=False,\n",
      "eval_accumulation_steps=None,\n",
      "eval_delay=0,\n",
      "eval_steps=None,\n",
      "evaluation_strategy=no,\n",
      "fp16=False,\n",
      "fp16_backend=auto,\n",
      "fp16_full_eval=False,\n",
      "fp16_opt_level=O1,\n",
      "fsdp=[],\n",
      "fsdp_config={'fsdp_min_num_params': 0, 'xla': False, 'xla_fsdp_grad_ckpt': False},\n",
      "fsdp_min_num_params=0,\n",
      "fsdp_transformer_layer_cls_to_wrap=None,\n",
      "full_determinism=False,\n",
      "generation_config=None,\n",
      "generation_max_length=None,\n",
      "generation_num_beams=None,\n",
      "gradient_accumulation_steps=1,\n",
      "gradient_checkpointing=False,\n",
      "greater_is_better=None,\n",
      "group_by_length=False,\n",
      "half_precision_backend=auto,\n",
      "hub_model_id=None,\n",
      "hub_private_repo=False,\n",
      "hub_strategy=every_save,\n",
      "hub_token=<HUB_TOKEN>,\n",
      "ignore_data_skip=False,\n",
      "include_inputs_for_metrics=False,\n",
      "jit_mode_eval=False,\n",
      "label_names=None,\n",
      "label_smoothing_factor=0.0,\n",
      "learning_rate=5e-05,\n",
      "length_column_name=length,\n",
      "load_best_model_at_end=False,\n",
      "local_rank=0,\n",
      "log_level=passive,\n",
      "log_level_replica=warning,\n",
      "log_on_each_node=True,\n",
      "logging_dir=./save/iwslt_t5_large/runs/May21_03-19-36_b56da3cfb72c,\n",
      "logging_first_step=False,\n",
      "logging_nan_inf_filter=True,\n",
      "logging_steps=500,\n",
      "logging_strategy=steps,\n",
      "lr_scheduler_type=linear,\n",
      "max_grad_norm=1.0,\n",
      "max_steps=-1,\n",
      "metric_for_best_model=None,\n",
      "mp_parameters=,\n",
      "no_cuda=False,\n",
      "num_train_epochs=3.0,\n",
      "optim=adamw_hf,\n",
      "optim_args=None,\n",
      "output_dir=./save/iwslt_t5_large/,\n",
      "overwrite_output_dir=True,\n",
      "past_index=-1,\n",
      "per_device_eval_batch_size=1,\n",
      "per_device_train_batch_size=8,\n",
      "predict_with_generate=True,\n",
      "prediction_loss_only=False,\n",
      "push_to_hub=False,\n",
      "push_to_hub_model_id=None,\n",
      "push_to_hub_organization=None,\n",
      "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
      "ray_scope=last,\n",
      "remove_unused_columns=True,\n",
      "report_to=['tensorboard'],\n",
      "resume_from_checkpoint=None,\n",
      "run_name=./save/iwslt_t5_large/,\n",
      "save_on_each_node=False,\n",
      "save_safetensors=False,\n",
      "save_steps=500,\n",
      "save_strategy=steps,\n",
      "save_total_limit=None,\n",
      "seed=42,\n",
      "sharded_ddp=[],\n",
      "skip_memory_metrics=True,\n",
      "sortish_sampler=False,\n",
      "tf32=None,\n",
      "torch_compile=False,\n",
      "torch_compile_backend=None,\n",
      "torch_compile_mode=None,\n",
      "torchdynamo=None,\n",
      "tpu_metrics_debug=False,\n",
      "tpu_num_cores=None,\n",
      "use_ipex=False,\n",
      "use_legacy_prediction_loop=False,\n",
      "use_mps_device=False,\n",
      "warmup_ratio=0.0,\n",
      "warmup_steps=0,\n",
      "weight_decay=0.0,\n",
      "xpu_backend=None,\n",
      ")\n",
      "/usr/local/lib/python3.10/dist-packages/datasets/load.py:2547: FutureWarning: 'use_auth_token' was deprecated in favor of 'token' in version 2.14.0 and will be removed in 3.0.0.\n",
      "You can remove this warning by passing 'token=<use_auth_token>' instead.\n",
      "  warnings.warn(\n",
      "Overwrite dataset info from restored data version if exists.\n",
      "05/21/2024 03:19:38 - INFO - datasets.builder - Overwrite dataset info from restored data version if exists.\n",
      "Loading Dataset info from /root/.cache/huggingface/datasets/iwslt2017/iwslt2017-de-en/1.0.0/c18a4f81a47ae6fa079fe9d32db288ddde38451d\n",
      "05/21/2024 03:19:38 - INFO - datasets.info - Loading Dataset info from /root/.cache/huggingface/datasets/iwslt2017/iwslt2017-de-en/1.0.0/c18a4f81a47ae6fa079fe9d32db288ddde38451d\n",
      "Found cached dataset iwslt2017 (/root/.cache/huggingface/datasets/iwslt2017/iwslt2017-de-en/1.0.0/c18a4f81a47ae6fa079fe9d32db288ddde38451d)\n",
      "05/21/2024 03:19:38 - INFO - datasets.builder - Found cached dataset iwslt2017 (/root/.cache/huggingface/datasets/iwslt2017/iwslt2017-de-en/1.0.0/c18a4f81a47ae6fa079fe9d32db288ddde38451d)\n",
      "Loading Dataset info from /root/.cache/huggingface/datasets/iwslt2017/iwslt2017-de-en/1.0.0/c18a4f81a47ae6fa079fe9d32db288ddde38451d\n",
      "05/21/2024 03:19:38 - INFO - datasets.info - Loading Dataset info from /root/.cache/huggingface/datasets/iwslt2017/iwslt2017-de-en/1.0.0/c18a4f81a47ae6fa079fe9d32db288ddde38451d\n",
      "/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "[INFO|configuration_utils.py:668] 2024-05-21 03:19:38,547 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--t5-large/snapshots/150ebc2c4b72291e770f58e6057481c8d2ed331a/config.json\n",
      "[INFO|configuration_utils.py:720] 2024-05-21 03:19:38,548 >> Model config T5Config {\n",
      "  \"_name_or_path\": \"t5-large\",\n",
      "  \"architectures\": [\n",
      "    \"T5ForConditionalGeneration\"\n",
      "  ],\n",
      "  \"d_ff\": 4096,\n",
      "  \"d_kv\": 64,\n",
      "  \"d_model\": 1024,\n",
      "  \"decoder_start_token_id\": 0,\n",
      "  \"dense_act_fn\": \"relu\",\n",
      "  \"dropout_rate\": 0.1,\n",
      "  \"eos_token_id\": 1,\n",
      "  \"feed_forward_proj\": \"relu\",\n",
      "  \"initializer_factor\": 1.0,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"is_gated_act\": false,\n",
      "  \"layer_norm_epsilon\": 1e-06,\n",
      "  \"model_type\": \"t5\",\n",
      "  \"n_positions\": 512,\n",
      "  \"num_decoder_layers\": 24,\n",
      "  \"num_heads\": 16,\n",
      "  \"num_layers\": 24,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"relative_attention_max_distance\": 128,\n",
      "  \"relative_attention_num_buckets\": 32,\n",
      "  \"task_specific_params\": {\n",
      "    \"summarization\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"length_penalty\": 2.0,\n",
      "      \"max_length\": 200,\n",
      "      \"min_length\": 30,\n",
      "      \"no_repeat_ngram_size\": 3,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"summarize: \"\n",
      "    },\n",
      "    \"translation_en_to_de\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"max_length\": 300,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"translate English to German: \"\n",
      "    },\n",
      "    \"translation_en_to_fr\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"max_length\": 300,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"translate English to French: \"\n",
      "    },\n",
      "    \"translation_en_to_ro\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"max_length\": 300,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"translate English to Romanian: \"\n",
      "    }\n",
      "  },\n",
      "  \"transformers_version\": \"4.28.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32128\n",
      "}\n",
      "\n",
      "[INFO|tokenization_auto.py:502] 2024-05-21 03:19:38,602 >> Could not locate the tokenizer configuration file, will try to use the model config instead.\n",
      "[INFO|configuration_utils.py:668] 2024-05-21 03:19:38,656 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--t5-large/snapshots/150ebc2c4b72291e770f58e6057481c8d2ed331a/config.json\n",
      "[INFO|configuration_utils.py:720] 2024-05-21 03:19:38,657 >> Model config T5Config {\n",
      "  \"_name_or_path\": \"t5-large\",\n",
      "  \"architectures\": [\n",
      "    \"T5ForConditionalGeneration\"\n",
      "  ],\n",
      "  \"d_ff\": 4096,\n",
      "  \"d_kv\": 64,\n",
      "  \"d_model\": 1024,\n",
      "  \"decoder_start_token_id\": 0,\n",
      "  \"dense_act_fn\": \"relu\",\n",
      "  \"dropout_rate\": 0.1,\n",
      "  \"eos_token_id\": 1,\n",
      "  \"feed_forward_proj\": \"relu\",\n",
      "  \"initializer_factor\": 1.0,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"is_gated_act\": false,\n",
      "  \"layer_norm_epsilon\": 1e-06,\n",
      "  \"model_type\": \"t5\",\n",
      "  \"n_positions\": 512,\n",
      "  \"num_decoder_layers\": 24,\n",
      "  \"num_heads\": 16,\n",
      "  \"num_layers\": 24,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"relative_attention_max_distance\": 128,\n",
      "  \"relative_attention_num_buckets\": 32,\n",
      "  \"task_specific_params\": {\n",
      "    \"summarization\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"length_penalty\": 2.0,\n",
      "      \"max_length\": 200,\n",
      "      \"min_length\": 30,\n",
      "      \"no_repeat_ngram_size\": 3,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"summarize: \"\n",
      "    },\n",
      "    \"translation_en_to_de\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"max_length\": 300,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"translate English to German: \"\n",
      "    },\n",
      "    \"translation_en_to_fr\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"max_length\": 300,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"translate English to French: \"\n",
      "    },\n",
      "    \"translation_en_to_ro\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"max_length\": 300,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"translate English to Romanian: \"\n",
      "    }\n",
      "  },\n",
      "  \"transformers_version\": \"4.28.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32128\n",
      "}\n",
      "\n",
      "[INFO|tokenization_utils_base.py:1809] 2024-05-21 03:19:38,757 >> loading file spiece.model from cache at /root/.cache/huggingface/hub/models--t5-large/snapshots/150ebc2c4b72291e770f58e6057481c8d2ed331a/spiece.model\n",
      "[INFO|tokenization_utils_base.py:1809] 2024-05-21 03:19:38,757 >> loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--t5-large/snapshots/150ebc2c4b72291e770f58e6057481c8d2ed331a/tokenizer.json\n",
      "[INFO|tokenization_utils_base.py:1809] 2024-05-21 03:19:38,757 >> loading file added_tokens.json from cache at None\n",
      "[INFO|tokenization_utils_base.py:1809] 2024-05-21 03:19:38,757 >> loading file special_tokens_map.json from cache at None\n",
      "[INFO|tokenization_utils_base.py:1809] 2024-05-21 03:19:38,757 >> loading file tokenizer_config.json from cache at None\n",
      "[INFO|configuration_utils.py:668] 2024-05-21 03:19:38,757 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--t5-large/snapshots/150ebc2c4b72291e770f58e6057481c8d2ed331a/config.json\n",
      "[INFO|configuration_utils.py:720] 2024-05-21 03:19:38,758 >> Model config T5Config {\n",
      "  \"_name_or_path\": \"t5-large\",\n",
      "  \"architectures\": [\n",
      "    \"T5ForConditionalGeneration\"\n",
      "  ],\n",
      "  \"d_ff\": 4096,\n",
      "  \"d_kv\": 64,\n",
      "  \"d_model\": 1024,\n",
      "  \"decoder_start_token_id\": 0,\n",
      "  \"dense_act_fn\": \"relu\",\n",
      "  \"dropout_rate\": 0.1,\n",
      "  \"eos_token_id\": 1,\n",
      "  \"feed_forward_proj\": \"relu\",\n",
      "  \"initializer_factor\": 1.0,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"is_gated_act\": false,\n",
      "  \"layer_norm_epsilon\": 1e-06,\n",
      "  \"model_type\": \"t5\",\n",
      "  \"n_positions\": 512,\n",
      "  \"num_decoder_layers\": 24,\n",
      "  \"num_heads\": 16,\n",
      "  \"num_layers\": 24,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"relative_attention_max_distance\": 128,\n",
      "  \"relative_attention_num_buckets\": 32,\n",
      "  \"task_specific_params\": {\n",
      "    \"summarization\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"length_penalty\": 2.0,\n",
      "      \"max_length\": 200,\n",
      "      \"min_length\": 30,\n",
      "      \"no_repeat_ngram_size\": 3,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"summarize: \"\n",
      "    },\n",
      "    \"translation_en_to_de\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"max_length\": 300,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"translate English to German: \"\n",
      "    },\n",
      "    \"translation_en_to_fr\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"max_length\": 300,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"translate English to French: \"\n",
      "    },\n",
      "    \"translation_en_to_ro\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"max_length\": 300,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"translate English to Romanian: \"\n",
      "    }\n",
      "  },\n",
      "  \"transformers_version\": \"4.28.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32128\n",
      "}\n",
      "\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/models/t5/tokenization_t5_fast.py:155: FutureWarning: This tokenizer was incorrectly instantiated with a model max length of 512 which will be corrected in Transformers v5.\n",
      "For now, this behavior is kept to avoid breaking backwards compatibility when padding/encoding with `truncation is True`.\n",
      "- Be aware that you SHOULD NOT rely on t5-large automatically truncating your input to 512 when padding/encoding.\n",
      "- If you want to encode/pad to sequences longer than 512 you can either instantiate this tokenizer with `model_max_length` or pass `max_length` when encoding/padding.\n",
      "- To avoid this warning, please instantiate this tokenizer with `model_max_length` set to your preferred value.\n",
      "  warnings.warn(\n",
      "[INFO|modeling_utils.py:2534] 2024-05-21 03:19:38,820 >> loading weights file model.safetensors from cache at /root/.cache/huggingface/hub/models--t5-large/snapshots/150ebc2c4b72291e770f58e6057481c8d2ed331a/model.safetensors\n",
      "[INFO|configuration_utils.py:575] 2024-05-21 03:19:39,029 >> Generate config GenerationConfig {\n",
      "  \"_from_model_config\": true,\n",
      "  \"decoder_start_token_id\": 0,\n",
      "  \"eos_token_id\": 1,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"transformers_version\": \"4.28.1\"\n",
      "}\n",
      "\n",
      "[INFO|modeling_utils.py:3190] 2024-05-21 03:20:18,838 >> All model checkpoint weights were used when initializing DeployT5ForConditionalGeneration.\n",
      "\n",
      "[WARNING|modeling_utils.py:3192] 2024-05-21 03:20:18,838 >> Some weights of DeployT5ForConditionalGeneration were not initialized from the model checkpoint at t5-large and are newly initialized: ['cm_head.2.weight', 'cm_head.2.bias', 'cm_head.0.bias', 'cm_head.0.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "[INFO|configuration_utils.py:537] 2024-05-21 03:20:18,898 >> loading configuration file generation_config.json from cache at /root/.cache/huggingface/hub/models--t5-large/snapshots/150ebc2c4b72291e770f58e6057481c8d2ed331a/generation_config.json\n",
      "[INFO|configuration_utils.py:575] 2024-05-21 03:20:18,898 >> Generate config GenerationConfig {\n",
      "  \"_from_model_config\": true,\n",
      "  \"decoder_start_token_id\": 0,\n",
      "  \"eos_token_id\": 1,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"transformers_version\": \"4.28.1\"\n",
      "}\n",
      "\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/iwslt2017/iwslt2017-de-en/1.0.0/c18a4f81a47ae6fa079fe9d32db288ddde38451d/cache-4b87423bce09feab.arrow\n",
      "05/21/2024 03:20:19 - INFO - datasets.arrow_dataset - Loading cached processed dataset at /root/.cache/huggingface/datasets/iwslt2017/iwslt2017-de-en/1.0.0/c18a4f81a47ae6fa079fe9d32db288ddde38451d/cache-4b87423bce09feab.arrow\n",
      "05/21/2024 03:20:20 - INFO - __main__ - *** Evaluate ***\n",
      "[WARNING|logging.py:280] 2024-05-21 03:20:20,666 >> You're using a T5TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "[INFO|configuration_utils.py:575] 2024-05-21 03:20:20,687 >> Generate config GenerationConfig {\n",
      "  \"_from_model_config\": true,\n",
      "  \"decoder_start_token_id\": 0,\n",
      "  \"eos_token_id\": 1,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"transformers_version\": \"4.28.1\"\n",
      "}\n",
      "\n",
      "100% 40/40 [02:17<00:00,  3.46s/it][WARNING|integrations.py:649] 2024-05-21 03:22:43,511 >> Trainer is attempting to log a value of \"0:00:00\" of type <class 'str'> for key \"train/time_encoder_forward\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n",
      "[WARNING|integrations.py:649] 2024-05-21 03:22:43,511 >> Trainer is attempting to log a value of \"0:02:15\" of type <class 'str'> for key \"train/time_decoder_forward\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n",
      "[WARNING|integrations.py:649] 2024-05-21 03:22:43,511 >> Trainer is attempting to log a value of \"['0:00:01', '0:00:00']\" of type <class 'str'> for key \"train/time_key_value_gen\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n",
      "[WARNING|integrations.py:649] 2024-05-21 03:22:43,511 >> Trainer is attempting to log a value of \"['0:00:05', '0:00:03']\" of type <class 'str'> for key \"train/time_attn\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n",
      "[WARNING|integrations.py:649] 2024-05-21 03:22:43,511 >> Trainer is attempting to log a value of \"0:00:02\" of type <class 'str'> for key \"train/time_ffn\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n",
      "[WARNING|integrations.py:649] 2024-05-21 03:22:43,512 >> Trainer is attempting to log a value of \"0:00:05\" of type <class 'str'> for key \"train/time_confidence\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n",
      "[WARNING|integrations.py:649] 2024-05-21 03:22:43,512 >> Trainer is attempting to log a value of \"['0:00:25', '0:00:02']\" of type <class 'str'> for key \"train/time_exit_key_value_gen\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n",
      "[WARNING|integrations.py:649] 2024-05-21 03:22:43,512 >> Trainer is attempting to log a value of \"['0:00:28', '0:00:10']\" of type <class 'str'> for key \"train/time_exit_attn\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n",
      "[WARNING|integrations.py:649] 2024-05-21 03:22:43,512 >> Trainer is attempting to log a value of \"0:00:03\" of type <class 'str'> for key \"train/time_exit_ffn\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n",
      "[WARNING|integrations.py:649] 2024-05-21 03:22:43,512 >> Trainer is attempting to log a value of \"['0:00:00', '0:00:00']\" of type <class 'str'> for key \"train/time_parallel_key_value_gen\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n",
      "[WARNING|integrations.py:649] 2024-05-21 03:22:43,512 >> Trainer is attempting to log a value of \"['0:00:00', '0:00:00']\" of type <class 'str'> for key \"train/time_parallel_attn\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n",
      "[WARNING|integrations.py:649] 2024-05-21 03:22:43,512 >> Trainer is attempting to log a value of \"0:00:00\" of type <class 'str'> for key \"train/time_parallel_ffn\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n",
      "[WARNING|integrations.py:649] 2024-05-21 03:22:43,512 >> Trainer is attempting to log a value of \"0:00:00\" of type <class 'str'> for key \"train/time_estimate_conf\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n",
      "[WARNING|integrations.py:649] 2024-05-21 03:22:43,512 >> Trainer is attempting to log a value of \"0:00:04\" of type <class 'str'> for key \"train/time_others\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n",
      "100% 40/40 [02:17<00:00,  3.45s/it]\n",
      "***** eval metrics *****\n",
      "  eval_bleu                   =                 0.0113\n",
      "  eval_block_avg              =                 1.1825\n",
      "  eval_gen_len                =                  127.0\n",
      "  eval_runtime                =             0:02:22.84\n",
      "  eval_samples                =                     40\n",
      "  eval_samples_per_second     =                   0.28\n",
      "  eval_steps_per_second       =                   0.28\n",
      "  time_attn                   = ['0:00:05', '0:00:03']\n",
      "  time_confidence             =                0:00:05\n",
      "  time_decoder_forward        =                0:02:15\n",
      "  time_encoder_forward        =                0:00:00\n",
      "  time_estimate_conf          =                0:00:00\n",
      "  time_exit_attn              = ['0:00:28', '0:00:10']\n",
      "  time_exit_ffn               =                0:00:03\n",
      "  time_exit_key_value_gen     = ['0:00:25', '0:00:02']\n",
      "  time_ffn                    =                0:00:02\n",
      "  time_key_value_gen          = ['0:00:01', '0:00:00']\n",
      "  time_others                 =                0:00:04\n",
      "  time_parallel_attn          = ['0:00:00', '0:00:00']\n",
      "  time_parallel_ffn           =                0:00:00\n",
      "  time_parallel_key_value_gen = ['0:00:00', '0:00:00']\n",
      "[INFO|modelcard.py:451] 2024-05-21 03:22:43,612 >> Dropping the following result as it does not have all the necessary fields:\n",
      "{'task': {'name': 'Translation', 'type': 'translation'}, 'dataset': {'name': 'iwslt2017 iwslt2017-de-en', 'type': 'iwslt2017', 'args': 'iwslt2017-de-en'}}\n"
     ]
    }
   ],
   "source": [
    "!python -m torch.distributed.run --nproc_per_node=1 \\\n",
    "    src/run_translation.py \\\n",
    "    --model_name_or_path t5-large \\\n",
    "    --do_eval \\\n",
    "    --dataset_name iwslt2017 \\\n",
    "    --dataset_config_name iwslt2017-de-en \\\n",
    "    --output_dir ./save/iwslt_t5_large/ \\\n",
    "    --per_device_eval_batch_size 1 \\\n",
    "    --deploy_scenario True \\\n",
    "    --use_synchronize True \\\n",
    "    --overwrite_output_dir \\\n",
    "    --predict_with_generate \\\n",
    "    --source_prefix \"translate German to English: \" \\\n",
    "    --output_hidden_states_decoder True \\\n",
    "    --use_early_exit True \\\n",
    "    --source_lang \"de\" \\\n",
    "    --target_lang \"en\" \\\n",
    "    --intermediate_loss_fn weighted_ce \\\n",
    "    --exit_conf_type meta \\\n",
    "    --exit_position_temp 4 \\\n",
    "    --exit_conf_threshold 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "uxxD1gCkKI7o",
    "outputId": "fa100e09-0584-4539-bec6-f4b8fd7a2692"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-05-21 03:22:56.915156: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-05-21 03:22:56.915216: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-05-21 03:22:56.917120: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-05-21 03:22:58.802893: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n",
      "05/21/2024 03:23:03 - WARNING - __main__ - Process rank: 0, device: cuda:0, n_gpu: 1distributed training: True, 16-bits training: False\n",
      "05/21/2024 03:23:03 - INFO - __main__ - Training/evaluation parameters Seq2SeqTrainingArguments(\n",
      "_n_gpu=1,\n",
      "adafactor=False,\n",
      "adam_beta1=0.9,\n",
      "adam_beta2=0.999,\n",
      "adam_epsilon=1e-08,\n",
      "auto_find_batch_size=False,\n",
      "bf16=False,\n",
      "bf16_full_eval=False,\n",
      "data_seed=None,\n",
      "dataloader_drop_last=False,\n",
      "dataloader_num_workers=0,\n",
      "dataloader_pin_memory=True,\n",
      "ddp_bucket_cap_mb=None,\n",
      "ddp_find_unused_parameters=None,\n",
      "ddp_timeout=1800,\n",
      "debug=[],\n",
      "deepspeed=None,\n",
      "disable_tqdm=False,\n",
      "do_eval=True,\n",
      "do_predict=False,\n",
      "do_train=False,\n",
      "eval_accumulation_steps=None,\n",
      "eval_delay=0,\n",
      "eval_steps=None,\n",
      "evaluation_strategy=no,\n",
      "fp16=False,\n",
      "fp16_backend=auto,\n",
      "fp16_full_eval=False,\n",
      "fp16_opt_level=O1,\n",
      "fsdp=[],\n",
      "fsdp_config={'fsdp_min_num_params': 0, 'xla': False, 'xla_fsdp_grad_ckpt': False},\n",
      "fsdp_min_num_params=0,\n",
      "fsdp_transformer_layer_cls_to_wrap=None,\n",
      "full_determinism=False,\n",
      "generation_config=None,\n",
      "generation_max_length=None,\n",
      "generation_num_beams=None,\n",
      "gradient_accumulation_steps=1,\n",
      "gradient_checkpointing=False,\n",
      "greater_is_better=None,\n",
      "group_by_length=False,\n",
      "half_precision_backend=auto,\n",
      "hub_model_id=None,\n",
      "hub_private_repo=False,\n",
      "hub_strategy=every_save,\n",
      "hub_token=<HUB_TOKEN>,\n",
      "ignore_data_skip=False,\n",
      "include_inputs_for_metrics=False,\n",
      "jit_mode_eval=False,\n",
      "label_names=None,\n",
      "label_smoothing_factor=0.0,\n",
      "learning_rate=5e-05,\n",
      "length_column_name=length,\n",
      "load_best_model_at_end=False,\n",
      "local_rank=0,\n",
      "log_level=passive,\n",
      "log_level_replica=warning,\n",
      "log_on_each_node=True,\n",
      "logging_dir=./save/iwslt_t5_large/runs/May21_03-23-03_b56da3cfb72c,\n",
      "logging_first_step=False,\n",
      "logging_nan_inf_filter=True,\n",
      "logging_steps=500,\n",
      "logging_strategy=steps,\n",
      "lr_scheduler_type=linear,\n",
      "max_grad_norm=1.0,\n",
      "max_steps=-1,\n",
      "metric_for_best_model=None,\n",
      "mp_parameters=,\n",
      "no_cuda=False,\n",
      "num_train_epochs=3.0,\n",
      "optim=adamw_hf,\n",
      "optim_args=None,\n",
      "output_dir=./save/iwslt_t5_large/,\n",
      "overwrite_output_dir=True,\n",
      "past_index=-1,\n",
      "per_device_eval_batch_size=1,\n",
      "per_device_train_batch_size=8,\n",
      "predict_with_generate=True,\n",
      "prediction_loss_only=False,\n",
      "push_to_hub=False,\n",
      "push_to_hub_model_id=None,\n",
      "push_to_hub_organization=None,\n",
      "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
      "ray_scope=last,\n",
      "remove_unused_columns=True,\n",
      "report_to=['tensorboard'],\n",
      "resume_from_checkpoint=None,\n",
      "run_name=./save/iwslt_t5_large/,\n",
      "save_on_each_node=False,\n",
      "save_safetensors=False,\n",
      "save_steps=500,\n",
      "save_strategy=steps,\n",
      "save_total_limit=None,\n",
      "seed=42,\n",
      "sharded_ddp=[],\n",
      "skip_memory_metrics=True,\n",
      "sortish_sampler=False,\n",
      "tf32=None,\n",
      "torch_compile=False,\n",
      "torch_compile_backend=None,\n",
      "torch_compile_mode=None,\n",
      "torchdynamo=None,\n",
      "tpu_metrics_debug=False,\n",
      "tpu_num_cores=None,\n",
      "use_ipex=False,\n",
      "use_legacy_prediction_loop=False,\n",
      "use_mps_device=False,\n",
      "warmup_ratio=0.0,\n",
      "warmup_steps=0,\n",
      "weight_decay=0.0,\n",
      "xpu_backend=None,\n",
      ")\n",
      "/usr/local/lib/python3.10/dist-packages/datasets/load.py:2547: FutureWarning: 'use_auth_token' was deprecated in favor of 'token' in version 2.14.0 and will be removed in 3.0.0.\n",
      "You can remove this warning by passing 'token=<use_auth_token>' instead.\n",
      "  warnings.warn(\n",
      "Overwrite dataset info from restored data version if exists.\n",
      "05/21/2024 03:23:04 - INFO - datasets.builder - Overwrite dataset info from restored data version if exists.\n",
      "Loading Dataset info from /root/.cache/huggingface/datasets/iwslt2017/iwslt2017-de-en/1.0.0/c18a4f81a47ae6fa079fe9d32db288ddde38451d\n",
      "05/21/2024 03:23:04 - INFO - datasets.info - Loading Dataset info from /root/.cache/huggingface/datasets/iwslt2017/iwslt2017-de-en/1.0.0/c18a4f81a47ae6fa079fe9d32db288ddde38451d\n",
      "Found cached dataset iwslt2017 (/root/.cache/huggingface/datasets/iwslt2017/iwslt2017-de-en/1.0.0/c18a4f81a47ae6fa079fe9d32db288ddde38451d)\n",
      "05/21/2024 03:23:04 - INFO - datasets.builder - Found cached dataset iwslt2017 (/root/.cache/huggingface/datasets/iwslt2017/iwslt2017-de-en/1.0.0/c18a4f81a47ae6fa079fe9d32db288ddde38451d)\n",
      "Loading Dataset info from /root/.cache/huggingface/datasets/iwslt2017/iwslt2017-de-en/1.0.0/c18a4f81a47ae6fa079fe9d32db288ddde38451d\n",
      "05/21/2024 03:23:04 - INFO - datasets.info - Loading Dataset info from /root/.cache/huggingface/datasets/iwslt2017/iwslt2017-de-en/1.0.0/c18a4f81a47ae6fa079fe9d32db288ddde38451d\n",
      "/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "[INFO|configuration_utils.py:668] 2024-05-21 03:23:05,545 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--t5-large/snapshots/150ebc2c4b72291e770f58e6057481c8d2ed331a/config.json\n",
      "[INFO|configuration_utils.py:720] 2024-05-21 03:23:05,546 >> Model config T5Config {\n",
      "  \"_name_or_path\": \"t5-large\",\n",
      "  \"architectures\": [\n",
      "    \"T5ForConditionalGeneration\"\n",
      "  ],\n",
      "  \"d_ff\": 4096,\n",
      "  \"d_kv\": 64,\n",
      "  \"d_model\": 1024,\n",
      "  \"decoder_start_token_id\": 0,\n",
      "  \"dense_act_fn\": \"relu\",\n",
      "  \"dropout_rate\": 0.1,\n",
      "  \"eos_token_id\": 1,\n",
      "  \"feed_forward_proj\": \"relu\",\n",
      "  \"initializer_factor\": 1.0,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"is_gated_act\": false,\n",
      "  \"layer_norm_epsilon\": 1e-06,\n",
      "  \"model_type\": \"t5\",\n",
      "  \"n_positions\": 512,\n",
      "  \"num_decoder_layers\": 24,\n",
      "  \"num_heads\": 16,\n",
      "  \"num_layers\": 24,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"relative_attention_max_distance\": 128,\n",
      "  \"relative_attention_num_buckets\": 32,\n",
      "  \"task_specific_params\": {\n",
      "    \"summarization\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"length_penalty\": 2.0,\n",
      "      \"max_length\": 200,\n",
      "      \"min_length\": 30,\n",
      "      \"no_repeat_ngram_size\": 3,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"summarize: \"\n",
      "    },\n",
      "    \"translation_en_to_de\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"max_length\": 300,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"translate English to German: \"\n",
      "    },\n",
      "    \"translation_en_to_fr\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"max_length\": 300,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"translate English to French: \"\n",
      "    },\n",
      "    \"translation_en_to_ro\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"max_length\": 300,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"translate English to Romanian: \"\n",
      "    }\n",
      "  },\n",
      "  \"transformers_version\": \"4.28.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32128\n",
      "}\n",
      "\n",
      "[INFO|tokenization_auto.py:502] 2024-05-21 03:23:05,601 >> Could not locate the tokenizer configuration file, will try to use the model config instead.\n",
      "[INFO|configuration_utils.py:668] 2024-05-21 03:23:05,649 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--t5-large/snapshots/150ebc2c4b72291e770f58e6057481c8d2ed331a/config.json\n",
      "[INFO|configuration_utils.py:720] 2024-05-21 03:23:05,650 >> Model config T5Config {\n",
      "  \"_name_or_path\": \"t5-large\",\n",
      "  \"architectures\": [\n",
      "    \"T5ForConditionalGeneration\"\n",
      "  ],\n",
      "  \"d_ff\": 4096,\n",
      "  \"d_kv\": 64,\n",
      "  \"d_model\": 1024,\n",
      "  \"decoder_start_token_id\": 0,\n",
      "  \"dense_act_fn\": \"relu\",\n",
      "  \"dropout_rate\": 0.1,\n",
      "  \"eos_token_id\": 1,\n",
      "  \"feed_forward_proj\": \"relu\",\n",
      "  \"initializer_factor\": 1.0,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"is_gated_act\": false,\n",
      "  \"layer_norm_epsilon\": 1e-06,\n",
      "  \"model_type\": \"t5\",\n",
      "  \"n_positions\": 512,\n",
      "  \"num_decoder_layers\": 24,\n",
      "  \"num_heads\": 16,\n",
      "  \"num_layers\": 24,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"relative_attention_max_distance\": 128,\n",
      "  \"relative_attention_num_buckets\": 32,\n",
      "  \"task_specific_params\": {\n",
      "    \"summarization\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"length_penalty\": 2.0,\n",
      "      \"max_length\": 200,\n",
      "      \"min_length\": 30,\n",
      "      \"no_repeat_ngram_size\": 3,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"summarize: \"\n",
      "    },\n",
      "    \"translation_en_to_de\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"max_length\": 300,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"translate English to German: \"\n",
      "    },\n",
      "    \"translation_en_to_fr\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"max_length\": 300,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"translate English to French: \"\n",
      "    },\n",
      "    \"translation_en_to_ro\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"max_length\": 300,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"translate English to Romanian: \"\n",
      "    }\n",
      "  },\n",
      "  \"transformers_version\": \"4.28.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32128\n",
      "}\n",
      "\n",
      "[INFO|tokenization_utils_base.py:1809] 2024-05-21 03:23:05,785 >> loading file spiece.model from cache at /root/.cache/huggingface/hub/models--t5-large/snapshots/150ebc2c4b72291e770f58e6057481c8d2ed331a/spiece.model\n",
      "[INFO|tokenization_utils_base.py:1809] 2024-05-21 03:23:05,785 >> loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--t5-large/snapshots/150ebc2c4b72291e770f58e6057481c8d2ed331a/tokenizer.json\n",
      "[INFO|tokenization_utils_base.py:1809] 2024-05-21 03:23:05,785 >> loading file added_tokens.json from cache at None\n",
      "[INFO|tokenization_utils_base.py:1809] 2024-05-21 03:23:05,785 >> loading file special_tokens_map.json from cache at None\n",
      "[INFO|tokenization_utils_base.py:1809] 2024-05-21 03:23:05,785 >> loading file tokenizer_config.json from cache at None\n",
      "[INFO|configuration_utils.py:668] 2024-05-21 03:23:05,785 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--t5-large/snapshots/150ebc2c4b72291e770f58e6057481c8d2ed331a/config.json\n",
      "[INFO|configuration_utils.py:720] 2024-05-21 03:23:05,786 >> Model config T5Config {\n",
      "  \"_name_or_path\": \"t5-large\",\n",
      "  \"architectures\": [\n",
      "    \"T5ForConditionalGeneration\"\n",
      "  ],\n",
      "  \"d_ff\": 4096,\n",
      "  \"d_kv\": 64,\n",
      "  \"d_model\": 1024,\n",
      "  \"decoder_start_token_id\": 0,\n",
      "  \"dense_act_fn\": \"relu\",\n",
      "  \"dropout_rate\": 0.1,\n",
      "  \"eos_token_id\": 1,\n",
      "  \"feed_forward_proj\": \"relu\",\n",
      "  \"initializer_factor\": 1.0,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"is_gated_act\": false,\n",
      "  \"layer_norm_epsilon\": 1e-06,\n",
      "  \"model_type\": \"t5\",\n",
      "  \"n_positions\": 512,\n",
      "  \"num_decoder_layers\": 24,\n",
      "  \"num_heads\": 16,\n",
      "  \"num_layers\": 24,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"relative_attention_max_distance\": 128,\n",
      "  \"relative_attention_num_buckets\": 32,\n",
      "  \"task_specific_params\": {\n",
      "    \"summarization\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"length_penalty\": 2.0,\n",
      "      \"max_length\": 200,\n",
      "      \"min_length\": 30,\n",
      "      \"no_repeat_ngram_size\": 3,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"summarize: \"\n",
      "    },\n",
      "    \"translation_en_to_de\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"max_length\": 300,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"translate English to German: \"\n",
      "    },\n",
      "    \"translation_en_to_fr\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"max_length\": 300,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"translate English to French: \"\n",
      "    },\n",
      "    \"translation_en_to_ro\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"max_length\": 300,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"translate English to Romanian: \"\n",
      "    }\n",
      "  },\n",
      "  \"transformers_version\": \"4.28.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32128\n",
      "}\n",
      "\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/models/t5/tokenization_t5_fast.py:155: FutureWarning: This tokenizer was incorrectly instantiated with a model max length of 512 which will be corrected in Transformers v5.\n",
      "For now, this behavior is kept to avoid breaking backwards compatibility when padding/encoding with `truncation is True`.\n",
      "- Be aware that you SHOULD NOT rely on t5-large automatically truncating your input to 512 when padding/encoding.\n",
      "- If you want to encode/pad to sequences longer than 512 you can either instantiate this tokenizer with `model_max_length` or pass `max_length` when encoding/padding.\n",
      "- To avoid this warning, please instantiate this tokenizer with `model_max_length` set to your preferred value.\n",
      "  warnings.warn(\n",
      "[INFO|modeling_utils.py:2534] 2024-05-21 03:23:05,861 >> loading weights file model.safetensors from cache at /root/.cache/huggingface/hub/models--t5-large/snapshots/150ebc2c4b72291e770f58e6057481c8d2ed331a/model.safetensors\n",
      "[INFO|configuration_utils.py:575] 2024-05-21 03:23:05,990 >> Generate config GenerationConfig {\n",
      "  \"_from_model_config\": true,\n",
      "  \"decoder_start_token_id\": 0,\n",
      "  \"eos_token_id\": 1,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"transformers_version\": \"4.28.1\"\n",
      "}\n",
      "\n",
      "[INFO|modeling_utils.py:3190] 2024-05-21 03:23:45,865 >> All model checkpoint weights were used when initializing DeployT5ForConditionalGeneration.\n",
      "\n",
      "[INFO|modeling_utils.py:3198] 2024-05-21 03:23:45,882 >> All the weights of DeployT5ForConditionalGeneration were initialized from the model checkpoint at t5-large.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use DeployT5ForConditionalGeneration for predictions without further training.\n",
      "[INFO|configuration_utils.py:537] 2024-05-21 03:23:45,952 >> loading configuration file generation_config.json from cache at /root/.cache/huggingface/hub/models--t5-large/snapshots/150ebc2c4b72291e770f58e6057481c8d2ed331a/generation_config.json\n",
      "[INFO|configuration_utils.py:575] 2024-05-21 03:23:45,952 >> Generate config GenerationConfig {\n",
      "  \"_from_model_config\": true,\n",
      "  \"decoder_start_token_id\": 0,\n",
      "  \"eos_token_id\": 1,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"transformers_version\": \"4.28.1\"\n",
      "}\n",
      "\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/iwslt2017/iwslt2017-de-en/1.0.0/c18a4f81a47ae6fa079fe9d32db288ddde38451d/cache-4b87423bce09feab.arrow\n",
      "05/21/2024 03:23:46 - INFO - datasets.arrow_dataset - Loading cached processed dataset at /root/.cache/huggingface/datasets/iwslt2017/iwslt2017-de-en/1.0.0/c18a4f81a47ae6fa079fe9d32db288ddde38451d/cache-4b87423bce09feab.arrow\n",
      "05/21/2024 03:23:48 - INFO - __main__ - *** Evaluate ***\n",
      "[WARNING|logging.py:280] 2024-05-21 03:23:48,043 >> You're using a T5TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "[INFO|configuration_utils.py:575] 2024-05-21 03:23:48,133 >> Generate config GenerationConfig {\n",
      "  \"_from_model_config\": true,\n",
      "  \"decoder_start_token_id\": 0,\n",
      "  \"eos_token_id\": 1,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"transformers_version\": \"4.28.1\"\n",
      "}\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/content/Monotonic-Early-Exit/src/run_translation.py\", line 510, in <module>\n",
      "    main(model_args, data_args, training_args, additional_args, model_cls, trainer_cls)\n",
      "  File \"/content/Monotonic-Early-Exit/src/run_translation.py\", line 435, in main\n",
      "    metrics = trainer.evaluate(max_length=max_length, num_beams=num_beams, metric_key_prefix=\"eval\")\n",
      "  File \"/content/Monotonic-Early-Exit/src/tr_lib/trainer_tr.py\", line 106, in evaluate\n",
      "    output = eval_loop(\n",
      "  File \"/content/Monotonic-Early-Exit/src/tr_lib/trainer_tr.py\", line 239, in evaluation_loop\n",
      "    loss, logits, labels = self.prediction_step(model, inputs, prediction_loss_only, ignore_keys=ignore_keys)\n",
      "  File \"/content/Monotonic-Early-Exit/src/tr_lib/trainer_tr.py\", line 409, in prediction_step\n",
      "    generated_tokens = gen_model.generate(\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/_contextlib.py\", line 115, in decorate_context\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py\", line 1437, in generate\n",
      "    return self.greedy_search(\n",
      "  File \"/content/Monotonic-Early-Exit/src/models/deploying_t5.py\", line 1334, in greedy_search\n",
      "    outputs = self(\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/content/Monotonic-Early-Exit/src/models/deploying_t5.py\", line 1097, in forward\n",
      "    encoder_outputs, decoder_outputs = self.forward_impl(input_ids, attention_mask, decoder_input_ids, decoder_attention_mask,\n",
      "  File \"/content/Monotonic-Early-Exit/src/models/deploying_t5.py\", line 1217, in forward_impl\n",
      "    decoder_outputs = self.decoder(\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/content/Monotonic-Early-Exit/src/models/deploying_t5.py\", line 921, in forward\n",
      "    skip_mask = get_skip_mask(\n",
      "  File \"/content/Monotonic-Early-Exit/src/util/skip_conf.py\", line 196, in get_skip_mask\n",
      "    conf = conf_measure(\n",
      "  File \"/content/Monotonic-Early-Exit/src/util/skip_conf.py\", line 16, in recurrent_classifier\n",
      "    assert classifier is not None\n",
      "AssertionError\n",
      "[2024-05-21 03:23:53,600] torch.distributed.elastic.multiprocessing.api: [ERROR] failed (exitcode: 1) local_rank: 0 (pid: 43258) of binary: /usr/bin/python3\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.10/runpy.py\", line 196, in _run_module_as_main\n",
      "    return _run_code(code, main_globals, None,\n",
      "  File \"/usr/lib/python3.10/runpy.py\", line 86, in _run_code\n",
      "    exec(code, run_globals)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/torch/distributed/run.py\", line 816, in <module>\n",
      "    main()\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py\", line 347, in wrapper\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/torch/distributed/run.py\", line 812, in main\n",
      "    run(args)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/torch/distributed/run.py\", line 803, in run\n",
      "    elastic_launch(\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/torch/distributed/launcher/api.py\", line 135, in __call__\n",
      "    return launch_agent(self._config, self._entrypoint, list(args))\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/torch/distributed/launcher/api.py\", line 268, in launch_agent\n",
      "    raise ChildFailedError(\n",
      "torch.distributed.elastic.multiprocessing.errors.ChildFailedError: \n",
      "============================================================\n",
      "src/run_translation.py FAILED\n",
      "------------------------------------------------------------\n",
      "Failures:\n",
      "  <NO_OTHER_FAILURES>\n",
      "------------------------------------------------------------\n",
      "Root Cause (first observed failure):\n",
      "[0]:\n",
      "  time      : 2024-05-21_03:23:53\n",
      "  host      : b56da3cfb72c\n",
      "  rank      : 0 (local_rank: 0)\n",
      "  exitcode  : 1 (pid: 43258)\n",
      "  error_file: <N/A>\n",
      "  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "!python -m torch.distributed.run --nproc_per_node=1 \\\n",
    "    src/run_translation.py \\\n",
    "    --model_name_or_path t5-large \\\n",
    "    --do_eval \\\n",
    "    --dataset_name iwslt2017 \\\n",
    "    --dataset_config_name iwslt2017-de-en \\\n",
    "    --output_dir ./save/iwslt_t5_large/ \\\n",
    "    --per_device_eval_batch_size 1 \\\n",
    "    --deploy_scenario True \\\n",
    "    --use_synchronize True \\\n",
    "    --overwrite_output_dir \\\n",
    "    --predict_with_generate \\\n",
    "    --source_prefix \"translate German to English: \" \\\n",
    "    --output_hidden_states_decoder True \\\n",
    "    --use_early_exit True \\\n",
    "    --source_lang \"de\" \\\n",
    "    --target_lang \"en\" \\\n",
    "    --intermediate_loss_fn weighted_ce \\\n",
    "    --exit_conf_type recurrent_classifier \\\n",
    "    --exit_position_temp 4 \\\n",
    "    --exit_conf_threshold 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "BLA17YX-KOko",
    "outputId": "44677ce0-5bed-4be0-e5ad-59779ecd2b64"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-05-21 03:24:01.008129: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-05-21 03:24:01.008180: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-05-21 03:24:01.009965: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-05-21 03:24:03.297848: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n",
      "05/21/2024 03:24:08 - WARNING - __main__ - Process rank: 0, device: cuda:0, n_gpu: 1distributed training: True, 16-bits training: False\n",
      "05/21/2024 03:24:08 - INFO - __main__ - Training/evaluation parameters Seq2SeqTrainingArguments(\n",
      "_n_gpu=1,\n",
      "adafactor=False,\n",
      "adam_beta1=0.9,\n",
      "adam_beta2=0.999,\n",
      "adam_epsilon=1e-08,\n",
      "auto_find_batch_size=False,\n",
      "bf16=False,\n",
      "bf16_full_eval=False,\n",
      "data_seed=None,\n",
      "dataloader_drop_last=False,\n",
      "dataloader_num_workers=0,\n",
      "dataloader_pin_memory=True,\n",
      "ddp_bucket_cap_mb=None,\n",
      "ddp_find_unused_parameters=None,\n",
      "ddp_timeout=1800,\n",
      "debug=[],\n",
      "deepspeed=None,\n",
      "disable_tqdm=False,\n",
      "do_eval=True,\n",
      "do_predict=False,\n",
      "do_train=False,\n",
      "eval_accumulation_steps=None,\n",
      "eval_delay=0,\n",
      "eval_steps=None,\n",
      "evaluation_strategy=no,\n",
      "fp16=False,\n",
      "fp16_backend=auto,\n",
      "fp16_full_eval=False,\n",
      "fp16_opt_level=O1,\n",
      "fsdp=[],\n",
      "fsdp_config={'fsdp_min_num_params': 0, 'xla': False, 'xla_fsdp_grad_ckpt': False},\n",
      "fsdp_min_num_params=0,\n",
      "fsdp_transformer_layer_cls_to_wrap=None,\n",
      "full_determinism=False,\n",
      "generation_config=None,\n",
      "generation_max_length=None,\n",
      "generation_num_beams=None,\n",
      "gradient_accumulation_steps=1,\n",
      "gradient_checkpointing=False,\n",
      "greater_is_better=None,\n",
      "group_by_length=False,\n",
      "half_precision_backend=auto,\n",
      "hub_model_id=None,\n",
      "hub_private_repo=False,\n",
      "hub_strategy=every_save,\n",
      "hub_token=<HUB_TOKEN>,\n",
      "ignore_data_skip=False,\n",
      "include_inputs_for_metrics=False,\n",
      "jit_mode_eval=False,\n",
      "label_names=None,\n",
      "label_smoothing_factor=0.0,\n",
      "learning_rate=5e-05,\n",
      "length_column_name=length,\n",
      "load_best_model_at_end=False,\n",
      "local_rank=0,\n",
      "log_level=passive,\n",
      "log_level_replica=warning,\n",
      "log_on_each_node=True,\n",
      "logging_dir=./save/iwslt_t5_large/runs/May21_03-24-08_b56da3cfb72c,\n",
      "logging_first_step=False,\n",
      "logging_nan_inf_filter=True,\n",
      "logging_steps=500,\n",
      "logging_strategy=steps,\n",
      "lr_scheduler_type=linear,\n",
      "max_grad_norm=1.0,\n",
      "max_steps=-1,\n",
      "metric_for_best_model=None,\n",
      "mp_parameters=,\n",
      "no_cuda=False,\n",
      "num_train_epochs=3.0,\n",
      "optim=adamw_hf,\n",
      "optim_args=None,\n",
      "output_dir=./save/iwslt_t5_large/,\n",
      "overwrite_output_dir=True,\n",
      "past_index=-1,\n",
      "per_device_eval_batch_size=1,\n",
      "per_device_train_batch_size=8,\n",
      "predict_with_generate=True,\n",
      "prediction_loss_only=False,\n",
      "push_to_hub=False,\n",
      "push_to_hub_model_id=None,\n",
      "push_to_hub_organization=None,\n",
      "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
      "ray_scope=last,\n",
      "remove_unused_columns=True,\n",
      "report_to=['tensorboard'],\n",
      "resume_from_checkpoint=None,\n",
      "run_name=./save/iwslt_t5_large/,\n",
      "save_on_each_node=False,\n",
      "save_safetensors=False,\n",
      "save_steps=500,\n",
      "save_strategy=steps,\n",
      "save_total_limit=None,\n",
      "seed=42,\n",
      "sharded_ddp=[],\n",
      "skip_memory_metrics=True,\n",
      "sortish_sampler=False,\n",
      "tf32=None,\n",
      "torch_compile=False,\n",
      "torch_compile_backend=None,\n",
      "torch_compile_mode=None,\n",
      "torchdynamo=None,\n",
      "tpu_metrics_debug=False,\n",
      "tpu_num_cores=None,\n",
      "use_ipex=False,\n",
      "use_legacy_prediction_loop=False,\n",
      "use_mps_device=False,\n",
      "warmup_ratio=0.0,\n",
      "warmup_steps=0,\n",
      "weight_decay=0.0,\n",
      "xpu_backend=None,\n",
      ")\n",
      "/usr/local/lib/python3.10/dist-packages/datasets/load.py:2547: FutureWarning: 'use_auth_token' was deprecated in favor of 'token' in version 2.14.0 and will be removed in 3.0.0.\n",
      "You can remove this warning by passing 'token=<use_auth_token>' instead.\n",
      "  warnings.warn(\n",
      "Overwrite dataset info from restored data version if exists.\n",
      "05/21/2024 03:24:10 - INFO - datasets.builder - Overwrite dataset info from restored data version if exists.\n",
      "Loading Dataset info from /root/.cache/huggingface/datasets/iwslt2017/iwslt2017-de-en/1.0.0/c18a4f81a47ae6fa079fe9d32db288ddde38451d\n",
      "05/21/2024 03:24:10 - INFO - datasets.info - Loading Dataset info from /root/.cache/huggingface/datasets/iwslt2017/iwslt2017-de-en/1.0.0/c18a4f81a47ae6fa079fe9d32db288ddde38451d\n",
      "Found cached dataset iwslt2017 (/root/.cache/huggingface/datasets/iwslt2017/iwslt2017-de-en/1.0.0/c18a4f81a47ae6fa079fe9d32db288ddde38451d)\n",
      "05/21/2024 03:24:10 - INFO - datasets.builder - Found cached dataset iwslt2017 (/root/.cache/huggingface/datasets/iwslt2017/iwslt2017-de-en/1.0.0/c18a4f81a47ae6fa079fe9d32db288ddde38451d)\n",
      "Loading Dataset info from /root/.cache/huggingface/datasets/iwslt2017/iwslt2017-de-en/1.0.0/c18a4f81a47ae6fa079fe9d32db288ddde38451d\n",
      "05/21/2024 03:24:10 - INFO - datasets.info - Loading Dataset info from /root/.cache/huggingface/datasets/iwslt2017/iwslt2017-de-en/1.0.0/c18a4f81a47ae6fa079fe9d32db288ddde38451d\n",
      "/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "[INFO|configuration_utils.py:668] 2024-05-21 03:24:10,926 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--t5-large/snapshots/150ebc2c4b72291e770f58e6057481c8d2ed331a/config.json\n",
      "[INFO|configuration_utils.py:720] 2024-05-21 03:24:10,927 >> Model config T5Config {\n",
      "  \"_name_or_path\": \"t5-large\",\n",
      "  \"architectures\": [\n",
      "    \"T5ForConditionalGeneration\"\n",
      "  ],\n",
      "  \"d_ff\": 4096,\n",
      "  \"d_kv\": 64,\n",
      "  \"d_model\": 1024,\n",
      "  \"decoder_start_token_id\": 0,\n",
      "  \"dense_act_fn\": \"relu\",\n",
      "  \"dropout_rate\": 0.1,\n",
      "  \"eos_token_id\": 1,\n",
      "  \"feed_forward_proj\": \"relu\",\n",
      "  \"initializer_factor\": 1.0,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"is_gated_act\": false,\n",
      "  \"layer_norm_epsilon\": 1e-06,\n",
      "  \"model_type\": \"t5\",\n",
      "  \"n_positions\": 512,\n",
      "  \"num_decoder_layers\": 24,\n",
      "  \"num_heads\": 16,\n",
      "  \"num_layers\": 24,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"relative_attention_max_distance\": 128,\n",
      "  \"relative_attention_num_buckets\": 32,\n",
      "  \"task_specific_params\": {\n",
      "    \"summarization\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"length_penalty\": 2.0,\n",
      "      \"max_length\": 200,\n",
      "      \"min_length\": 30,\n",
      "      \"no_repeat_ngram_size\": 3,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"summarize: \"\n",
      "    },\n",
      "    \"translation_en_to_de\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"max_length\": 300,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"translate English to German: \"\n",
      "    },\n",
      "    \"translation_en_to_fr\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"max_length\": 300,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"translate English to French: \"\n",
      "    },\n",
      "    \"translation_en_to_ro\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"max_length\": 300,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"translate English to Romanian: \"\n",
      "    }\n",
      "  },\n",
      "  \"transformers_version\": \"4.28.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32128\n",
      "}\n",
      "\n",
      "[INFO|tokenization_auto.py:502] 2024-05-21 03:24:10,979 >> Could not locate the tokenizer configuration file, will try to use the model config instead.\n",
      "[INFO|configuration_utils.py:668] 2024-05-21 03:24:11,030 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--t5-large/snapshots/150ebc2c4b72291e770f58e6057481c8d2ed331a/config.json\n",
      "[INFO|configuration_utils.py:720] 2024-05-21 03:24:11,031 >> Model config T5Config {\n",
      "  \"_name_or_path\": \"t5-large\",\n",
      "  \"architectures\": [\n",
      "    \"T5ForConditionalGeneration\"\n",
      "  ],\n",
      "  \"d_ff\": 4096,\n",
      "  \"d_kv\": 64,\n",
      "  \"d_model\": 1024,\n",
      "  \"decoder_start_token_id\": 0,\n",
      "  \"dense_act_fn\": \"relu\",\n",
      "  \"dropout_rate\": 0.1,\n",
      "  \"eos_token_id\": 1,\n",
      "  \"feed_forward_proj\": \"relu\",\n",
      "  \"initializer_factor\": 1.0,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"is_gated_act\": false,\n",
      "  \"layer_norm_epsilon\": 1e-06,\n",
      "  \"model_type\": \"t5\",\n",
      "  \"n_positions\": 512,\n",
      "  \"num_decoder_layers\": 24,\n",
      "  \"num_heads\": 16,\n",
      "  \"num_layers\": 24,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"relative_attention_max_distance\": 128,\n",
      "  \"relative_attention_num_buckets\": 32,\n",
      "  \"task_specific_params\": {\n",
      "    \"summarization\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"length_penalty\": 2.0,\n",
      "      \"max_length\": 200,\n",
      "      \"min_length\": 30,\n",
      "      \"no_repeat_ngram_size\": 3,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"summarize: \"\n",
      "    },\n",
      "    \"translation_en_to_de\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"max_length\": 300,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"translate English to German: \"\n",
      "    },\n",
      "    \"translation_en_to_fr\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"max_length\": 300,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"translate English to French: \"\n",
      "    },\n",
      "    \"translation_en_to_ro\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"max_length\": 300,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"translate English to Romanian: \"\n",
      "    }\n",
      "  },\n",
      "  \"transformers_version\": \"4.28.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32128\n",
      "}\n",
      "\n",
      "[INFO|tokenization_utils_base.py:1809] 2024-05-21 03:24:11,143 >> loading file spiece.model from cache at /root/.cache/huggingface/hub/models--t5-large/snapshots/150ebc2c4b72291e770f58e6057481c8d2ed331a/spiece.model\n",
      "[INFO|tokenization_utils_base.py:1809] 2024-05-21 03:24:11,143 >> loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--t5-large/snapshots/150ebc2c4b72291e770f58e6057481c8d2ed331a/tokenizer.json\n",
      "[INFO|tokenization_utils_base.py:1809] 2024-05-21 03:24:11,143 >> loading file added_tokens.json from cache at None\n",
      "[INFO|tokenization_utils_base.py:1809] 2024-05-21 03:24:11,143 >> loading file special_tokens_map.json from cache at None\n",
      "[INFO|tokenization_utils_base.py:1809] 2024-05-21 03:24:11,143 >> loading file tokenizer_config.json from cache at None\n",
      "[INFO|configuration_utils.py:668] 2024-05-21 03:24:11,144 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--t5-large/snapshots/150ebc2c4b72291e770f58e6057481c8d2ed331a/config.json\n",
      "[INFO|configuration_utils.py:720] 2024-05-21 03:24:11,144 >> Model config T5Config {\n",
      "  \"_name_or_path\": \"t5-large\",\n",
      "  \"architectures\": [\n",
      "    \"T5ForConditionalGeneration\"\n",
      "  ],\n",
      "  \"d_ff\": 4096,\n",
      "  \"d_kv\": 64,\n",
      "  \"d_model\": 1024,\n",
      "  \"decoder_start_token_id\": 0,\n",
      "  \"dense_act_fn\": \"relu\",\n",
      "  \"dropout_rate\": 0.1,\n",
      "  \"eos_token_id\": 1,\n",
      "  \"feed_forward_proj\": \"relu\",\n",
      "  \"initializer_factor\": 1.0,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"is_gated_act\": false,\n",
      "  \"layer_norm_epsilon\": 1e-06,\n",
      "  \"model_type\": \"t5\",\n",
      "  \"n_positions\": 512,\n",
      "  \"num_decoder_layers\": 24,\n",
      "  \"num_heads\": 16,\n",
      "  \"num_layers\": 24,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"relative_attention_max_distance\": 128,\n",
      "  \"relative_attention_num_buckets\": 32,\n",
      "  \"task_specific_params\": {\n",
      "    \"summarization\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"length_penalty\": 2.0,\n",
      "      \"max_length\": 200,\n",
      "      \"min_length\": 30,\n",
      "      \"no_repeat_ngram_size\": 3,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"summarize: \"\n",
      "    },\n",
      "    \"translation_en_to_de\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"max_length\": 300,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"translate English to German: \"\n",
      "    },\n",
      "    \"translation_en_to_fr\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"max_length\": 300,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"translate English to French: \"\n",
      "    },\n",
      "    \"translation_en_to_ro\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"max_length\": 300,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"translate English to Romanian: \"\n",
      "    }\n",
      "  },\n",
      "  \"transformers_version\": \"4.28.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32128\n",
      "}\n",
      "\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/models/t5/tokenization_t5_fast.py:155: FutureWarning: This tokenizer was incorrectly instantiated with a model max length of 512 which will be corrected in Transformers v5.\n",
      "For now, this behavior is kept to avoid breaking backwards compatibility when padding/encoding with `truncation is True`.\n",
      "- Be aware that you SHOULD NOT rely on t5-large automatically truncating your input to 512 when padding/encoding.\n",
      "- If you want to encode/pad to sequences longer than 512 you can either instantiate this tokenizer with `model_max_length` or pass `max_length` when encoding/padding.\n",
      "- To avoid this warning, please instantiate this tokenizer with `model_max_length` set to your preferred value.\n",
      "  warnings.warn(\n",
      "[INFO|modeling_utils.py:2534] 2024-05-21 03:24:11,221 >> loading weights file model.safetensors from cache at /root/.cache/huggingface/hub/models--t5-large/snapshots/150ebc2c4b72291e770f58e6057481c8d2ed331a/model.safetensors\n",
      "[INFO|configuration_utils.py:575] 2024-05-21 03:24:11,440 >> Generate config GenerationConfig {\n",
      "  \"_from_model_config\": true,\n",
      "  \"decoder_start_token_id\": 0,\n",
      "  \"eos_token_id\": 1,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"transformers_version\": \"4.28.1\"\n",
      "}\n",
      "\n",
      "[INFO|modeling_utils.py:3190] 2024-05-21 03:24:53,624 >> All model checkpoint weights were used when initializing DeployT5ForConditionalGeneration.\n",
      "\n",
      "[INFO|modeling_utils.py:3198] 2024-05-21 03:24:53,624 >> All the weights of DeployT5ForConditionalGeneration were initialized from the model checkpoint at t5-large.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use DeployT5ForConditionalGeneration for predictions without further training.\n",
      "[INFO|configuration_utils.py:537] 2024-05-21 03:24:53,855 >> loading configuration file generation_config.json from cache at /root/.cache/huggingface/hub/models--t5-large/snapshots/150ebc2c4b72291e770f58e6057481c8d2ed331a/generation_config.json\n",
      "[INFO|configuration_utils.py:575] 2024-05-21 03:24:53,855 >> Generate config GenerationConfig {\n",
      "  \"_from_model_config\": true,\n",
      "  \"decoder_start_token_id\": 0,\n",
      "  \"eos_token_id\": 1,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"transformers_version\": \"4.28.1\"\n",
      "}\n",
      "\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/iwslt2017/iwslt2017-de-en/1.0.0/c18a4f81a47ae6fa079fe9d32db288ddde38451d/cache-4b87423bce09feab.arrow\n",
      "05/21/2024 03:24:53 - INFO - datasets.arrow_dataset - Loading cached processed dataset at /root/.cache/huggingface/datasets/iwslt2017/iwslt2017-de-en/1.0.0/c18a4f81a47ae6fa079fe9d32db288ddde38451d/cache-4b87423bce09feab.arrow\n",
      "05/21/2024 03:24:55 - INFO - __main__ - *** Evaluate ***\n",
      "[WARNING|logging.py:280] 2024-05-21 03:24:55,870 >> You're using a T5TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "[INFO|configuration_utils.py:575] 2024-05-21 03:24:55,893 >> Generate config GenerationConfig {\n",
      "  \"_from_model_config\": true,\n",
      "  \"decoder_start_token_id\": 0,\n",
      "  \"eos_token_id\": 1,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"transformers_version\": \"4.28.1\"\n",
      "}\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/content/Monotonic-Early-Exit/src/run_translation.py\", line 510, in <module>\n",
      "    main(model_args, data_args, training_args, additional_args, model_cls, trainer_cls)\n",
      "  File \"/content/Monotonic-Early-Exit/src/run_translation.py\", line 435, in main\n",
      "    metrics = trainer.evaluate(max_length=max_length, num_beams=num_beams, metric_key_prefix=\"eval\")\n",
      "  File \"/content/Monotonic-Early-Exit/src/tr_lib/trainer_tr.py\", line 106, in evaluate\n",
      "    output = eval_loop(\n",
      "  File \"/content/Monotonic-Early-Exit/src/tr_lib/trainer_tr.py\", line 239, in evaluation_loop\n",
      "    loss, logits, labels = self.prediction_step(model, inputs, prediction_loss_only, ignore_keys=ignore_keys)\n",
      "  File \"/content/Monotonic-Early-Exit/src/tr_lib/trainer_tr.py\", line 409, in prediction_step\n",
      "    generated_tokens = gen_model.generate(\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/_contextlib.py\", line 115, in decorate_context\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py\", line 1437, in generate\n",
      "    return self.greedy_search(\n",
      "  File \"/content/Monotonic-Early-Exit/src/models/deploying_t5.py\", line 1334, in greedy_search\n",
      "    outputs = self(\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/content/Monotonic-Early-Exit/src/models/deploying_t5.py\", line 1097, in forward\n",
      "    encoder_outputs, decoder_outputs = self.forward_impl(input_ids, attention_mask, decoder_input_ids, decoder_attention_mask,\n",
      "  File \"/content/Monotonic-Early-Exit/src/models/deploying_t5.py\", line 1217, in forward_impl\n",
      "    decoder_outputs = self.decoder(\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/content/Monotonic-Early-Exit/src/models/deploying_t5.py\", line 921, in forward\n",
      "    skip_mask = get_skip_mask(\n",
      "  File \"/content/Monotonic-Early-Exit/src/util/skip_conf.py\", line 196, in get_skip_mask\n",
      "    conf = conf_measure(\n",
      "  File \"/content/Monotonic-Early-Exit/src/util/skip_conf.py\", line 33, in last_three_hiddens_classifier\n",
      "    assert classifier is not None\n",
      "AssertionError\n",
      "[2024-05-21 03:25:01,517] torch.distributed.elastic.multiprocessing.api: [ERROR] failed (exitcode: 1) local_rank: 0 (pid: 43533) of binary: /usr/bin/python3\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.10/runpy.py\", line 196, in _run_module_as_main\n",
      "    return _run_code(code, main_globals, None,\n",
      "  File \"/usr/lib/python3.10/runpy.py\", line 86, in _run_code\n",
      "    exec(code, run_globals)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/torch/distributed/run.py\", line 816, in <module>\n",
      "    main()\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py\", line 347, in wrapper\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/torch/distributed/run.py\", line 812, in main\n",
      "    run(args)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/torch/distributed/run.py\", line 803, in run\n",
      "    elastic_launch(\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/torch/distributed/launcher/api.py\", line 135, in __call__\n",
      "    return launch_agent(self._config, self._entrypoint, list(args))\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/torch/distributed/launcher/api.py\", line 268, in launch_agent\n",
      "    raise ChildFailedError(\n",
      "torch.distributed.elastic.multiprocessing.errors.ChildFailedError: \n",
      "============================================================\n",
      "src/run_translation.py FAILED\n",
      "------------------------------------------------------------\n",
      "Failures:\n",
      "  <NO_OTHER_FAILURES>\n",
      "------------------------------------------------------------\n",
      "Root Cause (first observed failure):\n",
      "[0]:\n",
      "  time      : 2024-05-21_03:25:01\n",
      "  host      : b56da3cfb72c\n",
      "  rank      : 0 (local_rank: 0)\n",
      "  exitcode  : 1 (pid: 43533)\n",
      "  error_file: <N/A>\n",
      "  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "!python -m torch.distributed.run --nproc_per_node=1 \\\n",
    "    src/run_translation.py \\\n",
    "    --model_name_or_path t5-large \\\n",
    "    --do_eval \\\n",
    "    --dataset_name iwslt2017 \\\n",
    "    --dataset_config_name iwslt2017-de-en \\\n",
    "    --output_dir ./save/iwslt_t5_large/ \\\n",
    "    --per_device_eval_batch_size 1 \\\n",
    "    --deploy_scenario True \\\n",
    "    --use_synchronize True \\\n",
    "    --overwrite_output_dir \\\n",
    "    --predict_with_generate \\\n",
    "    --source_prefix \"translate German to English: \" \\\n",
    "    --output_hidden_states_decoder True \\\n",
    "    --use_early_exit True \\\n",
    "    --source_lang \"de\" \\\n",
    "    --target_lang \"en\" \\\n",
    "    --intermediate_loss_fn weighted_ce \\\n",
    "    --exit_conf_type last_three_hiddens_classifier \\\n",
    "    --exit_position_temp 4 \\\n",
    "    --exit_conf_threshold 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "soBPZsmYKPD_",
    "outputId": "5a97d252-c4ed-4eb9-dcb0-8ee55cafa5a9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-05-21 03:25:06.826232: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-05-21 03:25:06.826285: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-05-21 03:25:06.828142: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-05-21 03:25:08.671527: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n",
      "05/21/2024 03:25:14 - WARNING - __main__ - Process rank: 0, device: cuda:0, n_gpu: 1distributed training: True, 16-bits training: False\n",
      "05/21/2024 03:25:14 - INFO - __main__ - Training/evaluation parameters Seq2SeqTrainingArguments(\n",
      "_n_gpu=1,\n",
      "adafactor=False,\n",
      "adam_beta1=0.9,\n",
      "adam_beta2=0.999,\n",
      "adam_epsilon=1e-08,\n",
      "auto_find_batch_size=False,\n",
      "bf16=False,\n",
      "bf16_full_eval=False,\n",
      "data_seed=None,\n",
      "dataloader_drop_last=False,\n",
      "dataloader_num_workers=0,\n",
      "dataloader_pin_memory=True,\n",
      "ddp_bucket_cap_mb=None,\n",
      "ddp_find_unused_parameters=None,\n",
      "ddp_timeout=1800,\n",
      "debug=[],\n",
      "deepspeed=None,\n",
      "disable_tqdm=False,\n",
      "do_eval=True,\n",
      "do_predict=False,\n",
      "do_train=False,\n",
      "eval_accumulation_steps=None,\n",
      "eval_delay=0,\n",
      "eval_steps=None,\n",
      "evaluation_strategy=no,\n",
      "fp16=False,\n",
      "fp16_backend=auto,\n",
      "fp16_full_eval=False,\n",
      "fp16_opt_level=O1,\n",
      "fsdp=[],\n",
      "fsdp_config={'fsdp_min_num_params': 0, 'xla': False, 'xla_fsdp_grad_ckpt': False},\n",
      "fsdp_min_num_params=0,\n",
      "fsdp_transformer_layer_cls_to_wrap=None,\n",
      "full_determinism=False,\n",
      "generation_config=None,\n",
      "generation_max_length=None,\n",
      "generation_num_beams=None,\n",
      "gradient_accumulation_steps=1,\n",
      "gradient_checkpointing=False,\n",
      "greater_is_better=None,\n",
      "group_by_length=False,\n",
      "half_precision_backend=auto,\n",
      "hub_model_id=None,\n",
      "hub_private_repo=False,\n",
      "hub_strategy=every_save,\n",
      "hub_token=<HUB_TOKEN>,\n",
      "ignore_data_skip=False,\n",
      "include_inputs_for_metrics=False,\n",
      "jit_mode_eval=False,\n",
      "label_names=None,\n",
      "label_smoothing_factor=0.0,\n",
      "learning_rate=5e-05,\n",
      "length_column_name=length,\n",
      "load_best_model_at_end=False,\n",
      "local_rank=0,\n",
      "log_level=passive,\n",
      "log_level_replica=warning,\n",
      "log_on_each_node=True,\n",
      "logging_dir=./save/iwslt_t5_large/runs/May21_03-25-14_b56da3cfb72c,\n",
      "logging_first_step=False,\n",
      "logging_nan_inf_filter=True,\n",
      "logging_steps=500,\n",
      "logging_strategy=steps,\n",
      "lr_scheduler_type=linear,\n",
      "max_grad_norm=1.0,\n",
      "max_steps=-1,\n",
      "metric_for_best_model=None,\n",
      "mp_parameters=,\n",
      "no_cuda=False,\n",
      "num_train_epochs=3.0,\n",
      "optim=adamw_hf,\n",
      "optim_args=None,\n",
      "output_dir=./save/iwslt_t5_large/,\n",
      "overwrite_output_dir=True,\n",
      "past_index=-1,\n",
      "per_device_eval_batch_size=1,\n",
      "per_device_train_batch_size=8,\n",
      "predict_with_generate=True,\n",
      "prediction_loss_only=False,\n",
      "push_to_hub=False,\n",
      "push_to_hub_model_id=None,\n",
      "push_to_hub_organization=None,\n",
      "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
      "ray_scope=last,\n",
      "remove_unused_columns=True,\n",
      "report_to=['tensorboard'],\n",
      "resume_from_checkpoint=None,\n",
      "run_name=./save/iwslt_t5_large/,\n",
      "save_on_each_node=False,\n",
      "save_safetensors=False,\n",
      "save_steps=500,\n",
      "save_strategy=steps,\n",
      "save_total_limit=None,\n",
      "seed=42,\n",
      "sharded_ddp=[],\n",
      "skip_memory_metrics=True,\n",
      "sortish_sampler=False,\n",
      "tf32=None,\n",
      "torch_compile=False,\n",
      "torch_compile_backend=None,\n",
      "torch_compile_mode=None,\n",
      "torchdynamo=None,\n",
      "tpu_metrics_debug=False,\n",
      "tpu_num_cores=None,\n",
      "use_ipex=False,\n",
      "use_legacy_prediction_loop=False,\n",
      "use_mps_device=False,\n",
      "warmup_ratio=0.0,\n",
      "warmup_steps=0,\n",
      "weight_decay=0.0,\n",
      "xpu_backend=None,\n",
      ")\n",
      "/usr/local/lib/python3.10/dist-packages/datasets/load.py:2547: FutureWarning: 'use_auth_token' was deprecated in favor of 'token' in version 2.14.0 and will be removed in 3.0.0.\n",
      "You can remove this warning by passing 'token=<use_auth_token>' instead.\n",
      "  warnings.warn(\n",
      "Overwrite dataset info from restored data version if exists.\n",
      "05/21/2024 03:25:15 - INFO - datasets.builder - Overwrite dataset info from restored data version if exists.\n",
      "Loading Dataset info from /root/.cache/huggingface/datasets/iwslt2017/iwslt2017-de-en/1.0.0/c18a4f81a47ae6fa079fe9d32db288ddde38451d\n",
      "05/21/2024 03:25:15 - INFO - datasets.info - Loading Dataset info from /root/.cache/huggingface/datasets/iwslt2017/iwslt2017-de-en/1.0.0/c18a4f81a47ae6fa079fe9d32db288ddde38451d\n",
      "Found cached dataset iwslt2017 (/root/.cache/huggingface/datasets/iwslt2017/iwslt2017-de-en/1.0.0/c18a4f81a47ae6fa079fe9d32db288ddde38451d)\n",
      "05/21/2024 03:25:15 - INFO - datasets.builder - Found cached dataset iwslt2017 (/root/.cache/huggingface/datasets/iwslt2017/iwslt2017-de-en/1.0.0/c18a4f81a47ae6fa079fe9d32db288ddde38451d)\n",
      "Loading Dataset info from /root/.cache/huggingface/datasets/iwslt2017/iwslt2017-de-en/1.0.0/c18a4f81a47ae6fa079fe9d32db288ddde38451d\n",
      "05/21/2024 03:25:15 - INFO - datasets.info - Loading Dataset info from /root/.cache/huggingface/datasets/iwslt2017/iwslt2017-de-en/1.0.0/c18a4f81a47ae6fa079fe9d32db288ddde38451d\n",
      "/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "[INFO|configuration_utils.py:668] 2024-05-21 03:25:16,115 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--t5-large/snapshots/150ebc2c4b72291e770f58e6057481c8d2ed331a/config.json\n",
      "[INFO|configuration_utils.py:720] 2024-05-21 03:25:16,116 >> Model config T5Config {\n",
      "  \"_name_or_path\": \"t5-large\",\n",
      "  \"architectures\": [\n",
      "    \"T5ForConditionalGeneration\"\n",
      "  ],\n",
      "  \"d_ff\": 4096,\n",
      "  \"d_kv\": 64,\n",
      "  \"d_model\": 1024,\n",
      "  \"decoder_start_token_id\": 0,\n",
      "  \"dense_act_fn\": \"relu\",\n",
      "  \"dropout_rate\": 0.1,\n",
      "  \"eos_token_id\": 1,\n",
      "  \"feed_forward_proj\": \"relu\",\n",
      "  \"initializer_factor\": 1.0,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"is_gated_act\": false,\n",
      "  \"layer_norm_epsilon\": 1e-06,\n",
      "  \"model_type\": \"t5\",\n",
      "  \"n_positions\": 512,\n",
      "  \"num_decoder_layers\": 24,\n",
      "  \"num_heads\": 16,\n",
      "  \"num_layers\": 24,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"relative_attention_max_distance\": 128,\n",
      "  \"relative_attention_num_buckets\": 32,\n",
      "  \"task_specific_params\": {\n",
      "    \"summarization\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"length_penalty\": 2.0,\n",
      "      \"max_length\": 200,\n",
      "      \"min_length\": 30,\n",
      "      \"no_repeat_ngram_size\": 3,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"summarize: \"\n",
      "    },\n",
      "    \"translation_en_to_de\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"max_length\": 300,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"translate English to German: \"\n",
      "    },\n",
      "    \"translation_en_to_fr\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"max_length\": 300,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"translate English to French: \"\n",
      "    },\n",
      "    \"translation_en_to_ro\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"max_length\": 300,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"translate English to Romanian: \"\n",
      "    }\n",
      "  },\n",
      "  \"transformers_version\": \"4.28.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32128\n",
      "}\n",
      "\n",
      "[INFO|tokenization_auto.py:502] 2024-05-21 03:25:16,167 >> Could not locate the tokenizer configuration file, will try to use the model config instead.\n",
      "[INFO|configuration_utils.py:668] 2024-05-21 03:25:16,217 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--t5-large/snapshots/150ebc2c4b72291e770f58e6057481c8d2ed331a/config.json\n",
      "[INFO|configuration_utils.py:720] 2024-05-21 03:25:16,218 >> Model config T5Config {\n",
      "  \"_name_or_path\": \"t5-large\",\n",
      "  \"architectures\": [\n",
      "    \"T5ForConditionalGeneration\"\n",
      "  ],\n",
      "  \"d_ff\": 4096,\n",
      "  \"d_kv\": 64,\n",
      "  \"d_model\": 1024,\n",
      "  \"decoder_start_token_id\": 0,\n",
      "  \"dense_act_fn\": \"relu\",\n",
      "  \"dropout_rate\": 0.1,\n",
      "  \"eos_token_id\": 1,\n",
      "  \"feed_forward_proj\": \"relu\",\n",
      "  \"initializer_factor\": 1.0,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"is_gated_act\": false,\n",
      "  \"layer_norm_epsilon\": 1e-06,\n",
      "  \"model_type\": \"t5\",\n",
      "  \"n_positions\": 512,\n",
      "  \"num_decoder_layers\": 24,\n",
      "  \"num_heads\": 16,\n",
      "  \"num_layers\": 24,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"relative_attention_max_distance\": 128,\n",
      "  \"relative_attention_num_buckets\": 32,\n",
      "  \"task_specific_params\": {\n",
      "    \"summarization\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"length_penalty\": 2.0,\n",
      "      \"max_length\": 200,\n",
      "      \"min_length\": 30,\n",
      "      \"no_repeat_ngram_size\": 3,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"summarize: \"\n",
      "    },\n",
      "    \"translation_en_to_de\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"max_length\": 300,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"translate English to German: \"\n",
      "    },\n",
      "    \"translation_en_to_fr\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"max_length\": 300,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"translate English to French: \"\n",
      "    },\n",
      "    \"translation_en_to_ro\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"max_length\": 300,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"translate English to Romanian: \"\n",
      "    }\n",
      "  },\n",
      "  \"transformers_version\": \"4.28.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32128\n",
      "}\n",
      "\n",
      "[INFO|tokenization_utils_base.py:1809] 2024-05-21 03:25:16,330 >> loading file spiece.model from cache at /root/.cache/huggingface/hub/models--t5-large/snapshots/150ebc2c4b72291e770f58e6057481c8d2ed331a/spiece.model\n",
      "[INFO|tokenization_utils_base.py:1809] 2024-05-21 03:25:16,330 >> loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--t5-large/snapshots/150ebc2c4b72291e770f58e6057481c8d2ed331a/tokenizer.json\n",
      "[INFO|tokenization_utils_base.py:1809] 2024-05-21 03:25:16,330 >> loading file added_tokens.json from cache at None\n",
      "[INFO|tokenization_utils_base.py:1809] 2024-05-21 03:25:16,330 >> loading file special_tokens_map.json from cache at None\n",
      "[INFO|tokenization_utils_base.py:1809] 2024-05-21 03:25:16,330 >> loading file tokenizer_config.json from cache at None\n",
      "[INFO|configuration_utils.py:668] 2024-05-21 03:25:16,330 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--t5-large/snapshots/150ebc2c4b72291e770f58e6057481c8d2ed331a/config.json\n",
      "[INFO|configuration_utils.py:720] 2024-05-21 03:25:16,331 >> Model config T5Config {\n",
      "  \"_name_or_path\": \"t5-large\",\n",
      "  \"architectures\": [\n",
      "    \"T5ForConditionalGeneration\"\n",
      "  ],\n",
      "  \"d_ff\": 4096,\n",
      "  \"d_kv\": 64,\n",
      "  \"d_model\": 1024,\n",
      "  \"decoder_start_token_id\": 0,\n",
      "  \"dense_act_fn\": \"relu\",\n",
      "  \"dropout_rate\": 0.1,\n",
      "  \"eos_token_id\": 1,\n",
      "  \"feed_forward_proj\": \"relu\",\n",
      "  \"initializer_factor\": 1.0,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"is_gated_act\": false,\n",
      "  \"layer_norm_epsilon\": 1e-06,\n",
      "  \"model_type\": \"t5\",\n",
      "  \"n_positions\": 512,\n",
      "  \"num_decoder_layers\": 24,\n",
      "  \"num_heads\": 16,\n",
      "  \"num_layers\": 24,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"relative_attention_max_distance\": 128,\n",
      "  \"relative_attention_num_buckets\": 32,\n",
      "  \"task_specific_params\": {\n",
      "    \"summarization\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"length_penalty\": 2.0,\n",
      "      \"max_length\": 200,\n",
      "      \"min_length\": 30,\n",
      "      \"no_repeat_ngram_size\": 3,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"summarize: \"\n",
      "    },\n",
      "    \"translation_en_to_de\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"max_length\": 300,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"translate English to German: \"\n",
      "    },\n",
      "    \"translation_en_to_fr\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"max_length\": 300,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"translate English to French: \"\n",
      "    },\n",
      "    \"translation_en_to_ro\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"max_length\": 300,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"translate English to Romanian: \"\n",
      "    }\n",
      "  },\n",
      "  \"transformers_version\": \"4.28.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32128\n",
      "}\n",
      "\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/models/t5/tokenization_t5_fast.py:155: FutureWarning: This tokenizer was incorrectly instantiated with a model max length of 512 which will be corrected in Transformers v5.\n",
      "For now, this behavior is kept to avoid breaking backwards compatibility when padding/encoding with `truncation is True`.\n",
      "- Be aware that you SHOULD NOT rely on t5-large automatically truncating your input to 512 when padding/encoding.\n",
      "- If you want to encode/pad to sequences longer than 512 you can either instantiate this tokenizer with `model_max_length` or pass `max_length` when encoding/padding.\n",
      "- To avoid this warning, please instantiate this tokenizer with `model_max_length` set to your preferred value.\n",
      "  warnings.warn(\n",
      "[INFO|modeling_utils.py:2534] 2024-05-21 03:25:16,403 >> loading weights file model.safetensors from cache at /root/.cache/huggingface/hub/models--t5-large/snapshots/150ebc2c4b72291e770f58e6057481c8d2ed331a/model.safetensors\n",
      "[INFO|configuration_utils.py:575] 2024-05-21 03:25:16,536 >> Generate config GenerationConfig {\n",
      "  \"_from_model_config\": true,\n",
      "  \"decoder_start_token_id\": 0,\n",
      "  \"eos_token_id\": 1,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"transformers_version\": \"4.28.1\"\n",
      "}\n",
      "\n",
      "[INFO|modeling_utils.py:3190] 2024-05-21 03:25:57,868 >> All model checkpoint weights were used when initializing DeployT5ForConditionalGeneration.\n",
      "\n",
      "[INFO|modeling_utils.py:3198] 2024-05-21 03:25:57,871 >> All the weights of DeployT5ForConditionalGeneration were initialized from the model checkpoint at t5-large.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use DeployT5ForConditionalGeneration for predictions without further training.\n",
      "[INFO|configuration_utils.py:537] 2024-05-21 03:25:57,940 >> loading configuration file generation_config.json from cache at /root/.cache/huggingface/hub/models--t5-large/snapshots/150ebc2c4b72291e770f58e6057481c8d2ed331a/generation_config.json\n",
      "[INFO|configuration_utils.py:575] 2024-05-21 03:25:57,940 >> Generate config GenerationConfig {\n",
      "  \"_from_model_config\": true,\n",
      "  \"decoder_start_token_id\": 0,\n",
      "  \"eos_token_id\": 1,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"transformers_version\": \"4.28.1\"\n",
      "}\n",
      "\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/iwslt2017/iwslt2017-de-en/1.0.0/c18a4f81a47ae6fa079fe9d32db288ddde38451d/cache-4b87423bce09feab.arrow\n",
      "05/21/2024 03:25:58 - INFO - datasets.arrow_dataset - Loading cached processed dataset at /root/.cache/huggingface/datasets/iwslt2017/iwslt2017-de-en/1.0.0/c18a4f81a47ae6fa079fe9d32db288ddde38451d/cache-4b87423bce09feab.arrow\n",
      "05/21/2024 03:25:59 - INFO - __main__ - *** Evaluate ***\n",
      "[WARNING|logging.py:280] 2024-05-21 03:26:00,023 >> You're using a T5TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "[INFO|configuration_utils.py:575] 2024-05-21 03:26:00,103 >> Generate config GenerationConfig {\n",
      "  \"_from_model_config\": true,\n",
      "  \"decoder_start_token_id\": 0,\n",
      "  \"eos_token_id\": 1,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"transformers_version\": \"4.28.1\"\n",
      "}\n",
      "\n",
      "100% 40/40 [02:18<00:00,  3.54s/it][WARNING|integrations.py:649] 2024-05-21 03:28:24,119 >> Trainer is attempting to log a value of \"0:00:00\" of type <class 'str'> for key \"train/time_encoder_forward\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n",
      "[WARNING|integrations.py:649] 2024-05-21 03:28:24,119 >> Trainer is attempting to log a value of \"0:02:15\" of type <class 'str'> for key \"train/time_decoder_forward\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n",
      "[WARNING|integrations.py:649] 2024-05-21 03:28:24,119 >> Trainer is attempting to log a value of \"['0:00:01', '0:00:00']\" of type <class 'str'> for key \"train/time_key_value_gen\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n",
      "[WARNING|integrations.py:649] 2024-05-21 03:28:24,119 >> Trainer is attempting to log a value of \"['0:00:05', '0:00:03']\" of type <class 'str'> for key \"train/time_attn\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n",
      "[WARNING|integrations.py:649] 2024-05-21 03:28:24,120 >> Trainer is attempting to log a value of \"0:00:01\" of type <class 'str'> for key \"train/time_ffn\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n",
      "[WARNING|integrations.py:649] 2024-05-21 03:28:24,120 >> Trainer is attempting to log a value of \"0:00:05\" of type <class 'str'> for key \"train/time_confidence\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n",
      "[WARNING|integrations.py:649] 2024-05-21 03:28:24,120 >> Trainer is attempting to log a value of \"['0:00:26', '0:00:03']\" of type <class 'str'> for key \"train/time_exit_key_value_gen\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n",
      "[WARNING|integrations.py:649] 2024-05-21 03:28:24,120 >> Trainer is attempting to log a value of \"['0:00:29', '0:00:10']\" of type <class 'str'> for key \"train/time_exit_attn\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n",
      "[WARNING|integrations.py:649] 2024-05-21 03:28:24,120 >> Trainer is attempting to log a value of \"0:00:03\" of type <class 'str'> for key \"train/time_exit_ffn\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n",
      "[WARNING|integrations.py:649] 2024-05-21 03:28:24,120 >> Trainer is attempting to log a value of \"['0:00:00', '0:00:00']\" of type <class 'str'> for key \"train/time_parallel_key_value_gen\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n",
      "[WARNING|integrations.py:649] 2024-05-21 03:28:24,120 >> Trainer is attempting to log a value of \"['0:00:00', '0:00:00']\" of type <class 'str'> for key \"train/time_parallel_attn\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n",
      "[WARNING|integrations.py:649] 2024-05-21 03:28:24,120 >> Trainer is attempting to log a value of \"0:00:00\" of type <class 'str'> for key \"train/time_parallel_ffn\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n",
      "[WARNING|integrations.py:649] 2024-05-21 03:28:24,120 >> Trainer is attempting to log a value of \"0:00:00\" of type <class 'str'> for key \"train/time_estimate_conf\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n",
      "[WARNING|integrations.py:649] 2024-05-21 03:28:24,120 >> Trainer is attempting to log a value of \"0:00:05\" of type <class 'str'> for key \"train/time_others\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n",
      "100% 40/40 [02:18<00:00,  3.46s/it]\n",
      "***** eval metrics *****\n",
      "  eval_bleu                   =                    0.0\n",
      "  eval_block_avg              =                    1.0\n",
      "  eval_gen_len                =                    0.0\n",
      "  eval_runtime                =             0:02:24.10\n",
      "  eval_samples                =                     40\n",
      "  eval_samples_per_second     =                  0.278\n",
      "  eval_steps_per_second       =                  0.278\n",
      "  time_attn                   = ['0:00:05', '0:00:03']\n",
      "  time_confidence             =                0:00:05\n",
      "  time_decoder_forward        =                0:02:15\n",
      "  time_encoder_forward        =                0:00:00\n",
      "  time_estimate_conf          =                0:00:00\n",
      "  time_exit_attn              = ['0:00:29', '0:00:10']\n",
      "  time_exit_ffn               =                0:00:03\n",
      "  time_exit_key_value_gen     = ['0:00:26', '0:00:03']\n",
      "  time_ffn                    =                0:00:01\n",
      "  time_key_value_gen          = ['0:00:01', '0:00:00']\n",
      "  time_others                 =                0:00:05\n",
      "  time_parallel_attn          = ['0:00:00', '0:00:00']\n",
      "  time_parallel_ffn           =                0:00:00\n",
      "  time_parallel_key_value_gen = ['0:00:00', '0:00:00']\n",
      "[INFO|modelcard.py:451] 2024-05-21 03:28:24,227 >> Dropping the following result as it does not have all the necessary fields:\n",
      "{'task': {'name': 'Translation', 'type': 'translation'}, 'dataset': {'name': 'iwslt2017 iwslt2017-de-en', 'type': 'iwslt2017', 'args': 'iwslt2017-de-en'}}\n"
     ]
    }
   ],
   "source": [
    "!python -m torch.distributed.run --nproc_per_node=1 \\\n",
    "    src/run_translation.py \\\n",
    "    --model_name_or_path t5-large \\\n",
    "    --do_eval \\\n",
    "    --dataset_name iwslt2017 \\\n",
    "    --dataset_config_name iwslt2017-de-en \\\n",
    "    --output_dir ./save/iwslt_t5_large/ \\\n",
    "    --per_device_eval_batch_size 1 \\\n",
    "    --deploy_scenario True \\\n",
    "    --use_synchronize True \\\n",
    "    --overwrite_output_dir \\\n",
    "    --predict_with_generate \\\n",
    "    --source_prefix \"translate German to English: \" \\\n",
    "    --output_hidden_states_decoder True \\\n",
    "    --use_early_exit True \\\n",
    "    --source_lang \"de\" \\\n",
    "    --target_lang \"en\" \\\n",
    "    --intermediate_loss_fn weighted_ce \\\n",
    "    --exit_conf_type softmax \\\n",
    "    --exit_position_temp 4 \\\n",
    "    --exit_conf_threshold 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "chx1W8-0KPSp",
    "outputId": "35b2b8e4-836e-4a01-9fb7-7f198540c0e8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-05-21 03:28:36.685752: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-05-21 03:28:36.685810: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-05-21 03:28:36.692269: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-05-21 03:28:38.714724: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n",
      "05/21/2024 03:28:44 - WARNING - __main__ - Process rank: 0, device: cuda:0, n_gpu: 1distributed training: True, 16-bits training: False\n",
      "05/21/2024 03:28:44 - INFO - __main__ - Training/evaluation parameters Seq2SeqTrainingArguments(\n",
      "_n_gpu=1,\n",
      "adafactor=False,\n",
      "adam_beta1=0.9,\n",
      "adam_beta2=0.999,\n",
      "adam_epsilon=1e-08,\n",
      "auto_find_batch_size=False,\n",
      "bf16=False,\n",
      "bf16_full_eval=False,\n",
      "data_seed=None,\n",
      "dataloader_drop_last=False,\n",
      "dataloader_num_workers=0,\n",
      "dataloader_pin_memory=True,\n",
      "ddp_bucket_cap_mb=None,\n",
      "ddp_find_unused_parameters=None,\n",
      "ddp_timeout=1800,\n",
      "debug=[],\n",
      "deepspeed=None,\n",
      "disable_tqdm=False,\n",
      "do_eval=True,\n",
      "do_predict=False,\n",
      "do_train=False,\n",
      "eval_accumulation_steps=None,\n",
      "eval_delay=0,\n",
      "eval_steps=None,\n",
      "evaluation_strategy=no,\n",
      "fp16=False,\n",
      "fp16_backend=auto,\n",
      "fp16_full_eval=False,\n",
      "fp16_opt_level=O1,\n",
      "fsdp=[],\n",
      "fsdp_config={'fsdp_min_num_params': 0, 'xla': False, 'xla_fsdp_grad_ckpt': False},\n",
      "fsdp_min_num_params=0,\n",
      "fsdp_transformer_layer_cls_to_wrap=None,\n",
      "full_determinism=False,\n",
      "generation_config=None,\n",
      "generation_max_length=None,\n",
      "generation_num_beams=None,\n",
      "gradient_accumulation_steps=1,\n",
      "gradient_checkpointing=False,\n",
      "greater_is_better=None,\n",
      "group_by_length=False,\n",
      "half_precision_backend=auto,\n",
      "hub_model_id=None,\n",
      "hub_private_repo=False,\n",
      "hub_strategy=every_save,\n",
      "hub_token=<HUB_TOKEN>,\n",
      "ignore_data_skip=False,\n",
      "include_inputs_for_metrics=False,\n",
      "jit_mode_eval=False,\n",
      "label_names=None,\n",
      "label_smoothing_factor=0.0,\n",
      "learning_rate=5e-05,\n",
      "length_column_name=length,\n",
      "load_best_model_at_end=False,\n",
      "local_rank=0,\n",
      "log_level=passive,\n",
      "log_level_replica=warning,\n",
      "log_on_each_node=True,\n",
      "logging_dir=./save/iwslt_t5_large/runs/May21_03-28-44_b56da3cfb72c,\n",
      "logging_first_step=False,\n",
      "logging_nan_inf_filter=True,\n",
      "logging_steps=500,\n",
      "logging_strategy=steps,\n",
      "lr_scheduler_type=linear,\n",
      "max_grad_norm=1.0,\n",
      "max_steps=-1,\n",
      "metric_for_best_model=None,\n",
      "mp_parameters=,\n",
      "no_cuda=False,\n",
      "num_train_epochs=3.0,\n",
      "optim=adamw_hf,\n",
      "optim_args=None,\n",
      "output_dir=./save/iwslt_t5_large/,\n",
      "overwrite_output_dir=True,\n",
      "past_index=-1,\n",
      "per_device_eval_batch_size=1,\n",
      "per_device_train_batch_size=8,\n",
      "predict_with_generate=True,\n",
      "prediction_loss_only=False,\n",
      "push_to_hub=False,\n",
      "push_to_hub_model_id=None,\n",
      "push_to_hub_organization=None,\n",
      "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
      "ray_scope=last,\n",
      "remove_unused_columns=True,\n",
      "report_to=['tensorboard'],\n",
      "resume_from_checkpoint=None,\n",
      "run_name=./save/iwslt_t5_large/,\n",
      "save_on_each_node=False,\n",
      "save_safetensors=False,\n",
      "save_steps=500,\n",
      "save_strategy=steps,\n",
      "save_total_limit=None,\n",
      "seed=42,\n",
      "sharded_ddp=[],\n",
      "skip_memory_metrics=True,\n",
      "sortish_sampler=False,\n",
      "tf32=None,\n",
      "torch_compile=False,\n",
      "torch_compile_backend=None,\n",
      "torch_compile_mode=None,\n",
      "torchdynamo=None,\n",
      "tpu_metrics_debug=False,\n",
      "tpu_num_cores=None,\n",
      "use_ipex=False,\n",
      "use_legacy_prediction_loop=False,\n",
      "use_mps_device=False,\n",
      "warmup_ratio=0.0,\n",
      "warmup_steps=0,\n",
      "weight_decay=0.0,\n",
      "xpu_backend=None,\n",
      ")\n",
      "/usr/local/lib/python3.10/dist-packages/datasets/load.py:2547: FutureWarning: 'use_auth_token' was deprecated in favor of 'token' in version 2.14.0 and will be removed in 3.0.0.\n",
      "You can remove this warning by passing 'token=<use_auth_token>' instead.\n",
      "  warnings.warn(\n",
      "Overwrite dataset info from restored data version if exists.\n",
      "05/21/2024 03:28:46 - INFO - datasets.builder - Overwrite dataset info from restored data version if exists.\n",
      "Loading Dataset info from /root/.cache/huggingface/datasets/iwslt2017/iwslt2017-de-en/1.0.0/c18a4f81a47ae6fa079fe9d32db288ddde38451d\n",
      "05/21/2024 03:28:46 - INFO - datasets.info - Loading Dataset info from /root/.cache/huggingface/datasets/iwslt2017/iwslt2017-de-en/1.0.0/c18a4f81a47ae6fa079fe9d32db288ddde38451d\n",
      "Found cached dataset iwslt2017 (/root/.cache/huggingface/datasets/iwslt2017/iwslt2017-de-en/1.0.0/c18a4f81a47ae6fa079fe9d32db288ddde38451d)\n",
      "05/21/2024 03:28:46 - INFO - datasets.builder - Found cached dataset iwslt2017 (/root/.cache/huggingface/datasets/iwslt2017/iwslt2017-de-en/1.0.0/c18a4f81a47ae6fa079fe9d32db288ddde38451d)\n",
      "Loading Dataset info from /root/.cache/huggingface/datasets/iwslt2017/iwslt2017-de-en/1.0.0/c18a4f81a47ae6fa079fe9d32db288ddde38451d\n",
      "05/21/2024 03:28:46 - INFO - datasets.info - Loading Dataset info from /root/.cache/huggingface/datasets/iwslt2017/iwslt2017-de-en/1.0.0/c18a4f81a47ae6fa079fe9d32db288ddde38451d\n",
      "/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "[INFO|configuration_utils.py:668] 2024-05-21 03:28:46,484 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--t5-large/snapshots/150ebc2c4b72291e770f58e6057481c8d2ed331a/config.json\n",
      "[INFO|configuration_utils.py:720] 2024-05-21 03:28:46,485 >> Model config T5Config {\n",
      "  \"_name_or_path\": \"t5-large\",\n",
      "  \"architectures\": [\n",
      "    \"T5ForConditionalGeneration\"\n",
      "  ],\n",
      "  \"d_ff\": 4096,\n",
      "  \"d_kv\": 64,\n",
      "  \"d_model\": 1024,\n",
      "  \"decoder_start_token_id\": 0,\n",
      "  \"dense_act_fn\": \"relu\",\n",
      "  \"dropout_rate\": 0.1,\n",
      "  \"eos_token_id\": 1,\n",
      "  \"feed_forward_proj\": \"relu\",\n",
      "  \"initializer_factor\": 1.0,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"is_gated_act\": false,\n",
      "  \"layer_norm_epsilon\": 1e-06,\n",
      "  \"model_type\": \"t5\",\n",
      "  \"n_positions\": 512,\n",
      "  \"num_decoder_layers\": 24,\n",
      "  \"num_heads\": 16,\n",
      "  \"num_layers\": 24,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"relative_attention_max_distance\": 128,\n",
      "  \"relative_attention_num_buckets\": 32,\n",
      "  \"task_specific_params\": {\n",
      "    \"summarization\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"length_penalty\": 2.0,\n",
      "      \"max_length\": 200,\n",
      "      \"min_length\": 30,\n",
      "      \"no_repeat_ngram_size\": 3,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"summarize: \"\n",
      "    },\n",
      "    \"translation_en_to_de\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"max_length\": 300,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"translate English to German: \"\n",
      "    },\n",
      "    \"translation_en_to_fr\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"max_length\": 300,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"translate English to French: \"\n",
      "    },\n",
      "    \"translation_en_to_ro\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"max_length\": 300,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"translate English to Romanian: \"\n",
      "    }\n",
      "  },\n",
      "  \"transformers_version\": \"4.28.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32128\n",
      "}\n",
      "\n",
      "[INFO|tokenization_auto.py:502] 2024-05-21 03:28:46,539 >> Could not locate the tokenizer configuration file, will try to use the model config instead.\n",
      "[INFO|configuration_utils.py:668] 2024-05-21 03:28:46,590 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--t5-large/snapshots/150ebc2c4b72291e770f58e6057481c8d2ed331a/config.json\n",
      "[INFO|configuration_utils.py:720] 2024-05-21 03:28:46,591 >> Model config T5Config {\n",
      "  \"_name_or_path\": \"t5-large\",\n",
      "  \"architectures\": [\n",
      "    \"T5ForConditionalGeneration\"\n",
      "  ],\n",
      "  \"d_ff\": 4096,\n",
      "  \"d_kv\": 64,\n",
      "  \"d_model\": 1024,\n",
      "  \"decoder_start_token_id\": 0,\n",
      "  \"dense_act_fn\": \"relu\",\n",
      "  \"dropout_rate\": 0.1,\n",
      "  \"eos_token_id\": 1,\n",
      "  \"feed_forward_proj\": \"relu\",\n",
      "  \"initializer_factor\": 1.0,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"is_gated_act\": false,\n",
      "  \"layer_norm_epsilon\": 1e-06,\n",
      "  \"model_type\": \"t5\",\n",
      "  \"n_positions\": 512,\n",
      "  \"num_decoder_layers\": 24,\n",
      "  \"num_heads\": 16,\n",
      "  \"num_layers\": 24,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"relative_attention_max_distance\": 128,\n",
      "  \"relative_attention_num_buckets\": 32,\n",
      "  \"task_specific_params\": {\n",
      "    \"summarization\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"length_penalty\": 2.0,\n",
      "      \"max_length\": 200,\n",
      "      \"min_length\": 30,\n",
      "      \"no_repeat_ngram_size\": 3,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"summarize: \"\n",
      "    },\n",
      "    \"translation_en_to_de\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"max_length\": 300,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"translate English to German: \"\n",
      "    },\n",
      "    \"translation_en_to_fr\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"max_length\": 300,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"translate English to French: \"\n",
      "    },\n",
      "    \"translation_en_to_ro\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"max_length\": 300,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"translate English to Romanian: \"\n",
      "    }\n",
      "  },\n",
      "  \"transformers_version\": \"4.28.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32128\n",
      "}\n",
      "\n",
      "[INFO|tokenization_utils_base.py:1809] 2024-05-21 03:28:46,694 >> loading file spiece.model from cache at /root/.cache/huggingface/hub/models--t5-large/snapshots/150ebc2c4b72291e770f58e6057481c8d2ed331a/spiece.model\n",
      "[INFO|tokenization_utils_base.py:1809] 2024-05-21 03:28:46,694 >> loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--t5-large/snapshots/150ebc2c4b72291e770f58e6057481c8d2ed331a/tokenizer.json\n",
      "[INFO|tokenization_utils_base.py:1809] 2024-05-21 03:28:46,694 >> loading file added_tokens.json from cache at None\n",
      "[INFO|tokenization_utils_base.py:1809] 2024-05-21 03:28:46,694 >> loading file special_tokens_map.json from cache at None\n",
      "[INFO|tokenization_utils_base.py:1809] 2024-05-21 03:28:46,694 >> loading file tokenizer_config.json from cache at None\n",
      "[INFO|configuration_utils.py:668] 2024-05-21 03:28:46,694 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--t5-large/snapshots/150ebc2c4b72291e770f58e6057481c8d2ed331a/config.json\n",
      "[INFO|configuration_utils.py:720] 2024-05-21 03:28:46,695 >> Model config T5Config {\n",
      "  \"_name_or_path\": \"t5-large\",\n",
      "  \"architectures\": [\n",
      "    \"T5ForConditionalGeneration\"\n",
      "  ],\n",
      "  \"d_ff\": 4096,\n",
      "  \"d_kv\": 64,\n",
      "  \"d_model\": 1024,\n",
      "  \"decoder_start_token_id\": 0,\n",
      "  \"dense_act_fn\": \"relu\",\n",
      "  \"dropout_rate\": 0.1,\n",
      "  \"eos_token_id\": 1,\n",
      "  \"feed_forward_proj\": \"relu\",\n",
      "  \"initializer_factor\": 1.0,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"is_gated_act\": false,\n",
      "  \"layer_norm_epsilon\": 1e-06,\n",
      "  \"model_type\": \"t5\",\n",
      "  \"n_positions\": 512,\n",
      "  \"num_decoder_layers\": 24,\n",
      "  \"num_heads\": 16,\n",
      "  \"num_layers\": 24,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"relative_attention_max_distance\": 128,\n",
      "  \"relative_attention_num_buckets\": 32,\n",
      "  \"task_specific_params\": {\n",
      "    \"summarization\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"length_penalty\": 2.0,\n",
      "      \"max_length\": 200,\n",
      "      \"min_length\": 30,\n",
      "      \"no_repeat_ngram_size\": 3,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"summarize: \"\n",
      "    },\n",
      "    \"translation_en_to_de\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"max_length\": 300,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"translate English to German: \"\n",
      "    },\n",
      "    \"translation_en_to_fr\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"max_length\": 300,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"translate English to French: \"\n",
      "    },\n",
      "    \"translation_en_to_ro\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"max_length\": 300,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"translate English to Romanian: \"\n",
      "    }\n",
      "  },\n",
      "  \"transformers_version\": \"4.28.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32128\n",
      "}\n",
      "\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/models/t5/tokenization_t5_fast.py:155: FutureWarning: This tokenizer was incorrectly instantiated with a model max length of 512 which will be corrected in Transformers v5.\n",
      "For now, this behavior is kept to avoid breaking backwards compatibility when padding/encoding with `truncation is True`.\n",
      "- Be aware that you SHOULD NOT rely on t5-large automatically truncating your input to 512 when padding/encoding.\n",
      "- If you want to encode/pad to sequences longer than 512 you can either instantiate this tokenizer with `model_max_length` or pass `max_length` when encoding/padding.\n",
      "- To avoid this warning, please instantiate this tokenizer with `model_max_length` set to your preferred value.\n",
      "  warnings.warn(\n",
      "[INFO|modeling_utils.py:2534] 2024-05-21 03:28:46,769 >> loading weights file model.safetensors from cache at /root/.cache/huggingface/hub/models--t5-large/snapshots/150ebc2c4b72291e770f58e6057481c8d2ed331a/model.safetensors\n",
      "[INFO|configuration_utils.py:575] 2024-05-21 03:28:47,210 >> Generate config GenerationConfig {\n",
      "  \"_from_model_config\": true,\n",
      "  \"decoder_start_token_id\": 0,\n",
      "  \"eos_token_id\": 1,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"transformers_version\": \"4.28.1\"\n",
      "}\n",
      "\n",
      "[INFO|modeling_utils.py:3190] 2024-05-21 03:29:28,274 >> All model checkpoint weights were used when initializing DeployT5ForConditionalGeneration.\n",
      "\n",
      "[INFO|modeling_utils.py:3198] 2024-05-21 03:29:28,274 >> All the weights of DeployT5ForConditionalGeneration were initialized from the model checkpoint at t5-large.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use DeployT5ForConditionalGeneration for predictions without further training.\n",
      "[INFO|configuration_utils.py:537] 2024-05-21 03:29:28,341 >> loading configuration file generation_config.json from cache at /root/.cache/huggingface/hub/models--t5-large/snapshots/150ebc2c4b72291e770f58e6057481c8d2ed331a/generation_config.json\n",
      "[INFO|configuration_utils.py:575] 2024-05-21 03:29:28,341 >> Generate config GenerationConfig {\n",
      "  \"_from_model_config\": true,\n",
      "  \"decoder_start_token_id\": 0,\n",
      "  \"eos_token_id\": 1,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"transformers_version\": \"4.28.1\"\n",
      "}\n",
      "\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/iwslt2017/iwslt2017-de-en/1.0.0/c18a4f81a47ae6fa079fe9d32db288ddde38451d/cache-4b87423bce09feab.arrow\n",
      "05/21/2024 03:29:28 - INFO - datasets.arrow_dataset - Loading cached processed dataset at /root/.cache/huggingface/datasets/iwslt2017/iwslt2017-de-en/1.0.0/c18a4f81a47ae6fa079fe9d32db288ddde38451d/cache-4b87423bce09feab.arrow\n",
      "05/21/2024 03:29:30 - INFO - __main__ - *** Evaluate ***\n",
      "[WARNING|logging.py:280] 2024-05-21 03:29:30,079 >> You're using a T5TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "[INFO|configuration_utils.py:575] 2024-05-21 03:29:30,101 >> Generate config GenerationConfig {\n",
      "  \"_from_model_config\": true,\n",
      "  \"decoder_start_token_id\": 0,\n",
      "  \"eos_token_id\": 1,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"transformers_version\": \"4.28.1\"\n",
      "}\n",
      "\n",
      "100% 40/40 [01:16<00:00,  2.78s/it][WARNING|integrations.py:649] 2024-05-21 03:30:52,807 >> Trainer is attempting to log a value of \"0:00:00\" of type <class 'str'> for key \"train/time_encoder_forward\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n",
      "[WARNING|integrations.py:649] 2024-05-21 03:30:52,807 >> Trainer is attempting to log a value of \"0:01:17\" of type <class 'str'> for key \"train/time_decoder_forward\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n",
      "[WARNING|integrations.py:649] 2024-05-21 03:30:52,807 >> Trainer is attempting to log a value of \"['0:00:05', '0:00:00']\" of type <class 'str'> for key \"train/time_key_value_gen\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n",
      "[WARNING|integrations.py:649] 2024-05-21 03:30:52,807 >> Trainer is attempting to log a value of \"['0:00:14', '0:00:14']\" of type <class 'str'> for key \"train/time_attn\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n",
      "[WARNING|integrations.py:649] 2024-05-21 03:30:52,807 >> Trainer is attempting to log a value of \"0:00:09\" of type <class 'str'> for key \"train/time_ffn\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n",
      "[WARNING|integrations.py:649] 2024-05-21 03:30:52,807 >> Trainer is attempting to log a value of \"0:00:18\" of type <class 'str'> for key \"train/time_confidence\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n",
      "[WARNING|integrations.py:649] 2024-05-21 03:30:52,808 >> Trainer is attempting to log a value of \"['0:00:00', '0:00:00']\" of type <class 'str'> for key \"train/time_exit_key_value_gen\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n",
      "[WARNING|integrations.py:649] 2024-05-21 03:30:52,808 >> Trainer is attempting to log a value of \"['0:00:00', '0:00:00']\" of type <class 'str'> for key \"train/time_exit_attn\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n",
      "[WARNING|integrations.py:649] 2024-05-21 03:30:52,808 >> Trainer is attempting to log a value of \"0:00:00\" of type <class 'str'> for key \"train/time_exit_ffn\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n",
      "[WARNING|integrations.py:649] 2024-05-21 03:30:52,808 >> Trainer is attempting to log a value of \"['0:00:00', '0:00:00']\" of type <class 'str'> for key \"train/time_parallel_key_value_gen\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n",
      "[WARNING|integrations.py:649] 2024-05-21 03:30:52,808 >> Trainer is attempting to log a value of \"['0:00:00', '0:00:00']\" of type <class 'str'> for key \"train/time_parallel_attn\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n",
      "[WARNING|integrations.py:649] 2024-05-21 03:30:52,808 >> Trainer is attempting to log a value of \"0:00:00\" of type <class 'str'> for key \"train/time_parallel_ffn\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n",
      "[WARNING|integrations.py:649] 2024-05-21 03:30:52,808 >> Trainer is attempting to log a value of \"0:00:00\" of type <class 'str'> for key \"train/time_estimate_conf\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n",
      "[WARNING|integrations.py:649] 2024-05-21 03:30:52,808 >> Trainer is attempting to log a value of \"0:00:02\" of type <class 'str'> for key \"train/time_others\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n",
      "100% 40/40 [01:16<00:00,  1.90s/it]\n",
      "***** eval metrics *****\n",
      "  eval_bleu                   =                 0.8452\n",
      "  eval_block_avg              =                   24.0\n",
      "  eval_gen_len                =                   27.3\n",
      "  eval_runtime                =             0:01:22.72\n",
      "  eval_samples                =                     40\n",
      "  eval_samples_per_second     =                  0.484\n",
      "  eval_steps_per_second       =                  0.484\n",
      "  time_attn                   = ['0:00:14', '0:00:14']\n",
      "  time_confidence             =                0:00:18\n",
      "  time_decoder_forward        =                0:01:17\n",
      "  time_encoder_forward        =                0:00:00\n",
      "  time_estimate_conf          =                0:00:00\n",
      "  time_exit_attn              = ['0:00:00', '0:00:00']\n",
      "  time_exit_ffn               =                0:00:00\n",
      "  time_exit_key_value_gen     = ['0:00:00', '0:00:00']\n",
      "  time_ffn                    =                0:00:09\n",
      "  time_key_value_gen          = ['0:00:05', '0:00:00']\n",
      "  time_others                 =                0:00:02\n",
      "  time_parallel_attn          = ['0:00:00', '0:00:00']\n",
      "  time_parallel_ffn           =                0:00:00\n",
      "  time_parallel_key_value_gen = ['0:00:00', '0:00:00']\n",
      "[INFO|modelcard.py:451] 2024-05-21 03:30:52,927 >> Dropping the following result as it does not have all the necessary fields:\n",
      "{'task': {'name': 'Translation', 'type': 'translation'}, 'dataset': {'name': 'iwslt2017 iwslt2017-de-en', 'type': 'iwslt2017', 'args': 'iwslt2017-de-en'}}\n"
     ]
    }
   ],
   "source": [
    "!python -m torch.distributed.run --nproc_per_node=1 \\\n",
    "    src/run_translation.py \\\n",
    "    --model_name_or_path t5-large \\\n",
    "    --do_eval \\\n",
    "    --dataset_name iwslt2017 \\\n",
    "    --dataset_config_name iwslt2017-de-en \\\n",
    "    --output_dir ./save/iwslt_t5_large/ \\\n",
    "    --per_device_eval_batch_size 1 \\\n",
    "    --deploy_scenario True \\\n",
    "    --use_synchronize True \\\n",
    "    --overwrite_output_dir \\\n",
    "    --predict_with_generate \\\n",
    "    --source_prefix \"translate German to English: \" \\\n",
    "    --output_hidden_states_decoder True \\\n",
    "    --use_early_exit True \\\n",
    "    --source_lang \"de\" \\\n",
    "    --target_lang \"en\" \\\n",
    "    --intermediate_loss_fn weighted_ce \\\n",
    "    --exit_conf_type hidden_state_saturation \\\n",
    "    --exit_position_temp 4 \\\n",
    "    --exit_conf_threshold 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_HQKxUobKPex",
    "outputId": "0a51df03-fb10-44f3-df62-5d40d5b24c7b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-05-21 03:31:02.663206: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-05-21 03:31:02.663262: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-05-21 03:31:02.664666: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-05-21 03:31:04.014086: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n",
      "05/21/2024 03:31:08 - WARNING - __main__ - Process rank: 0, device: cuda:0, n_gpu: 1distributed training: True, 16-bits training: False\n",
      "05/21/2024 03:31:08 - INFO - __main__ - Training/evaluation parameters Seq2SeqTrainingArguments(\n",
      "_n_gpu=1,\n",
      "adafactor=False,\n",
      "adam_beta1=0.9,\n",
      "adam_beta2=0.999,\n",
      "adam_epsilon=1e-08,\n",
      "auto_find_batch_size=False,\n",
      "bf16=False,\n",
      "bf16_full_eval=False,\n",
      "data_seed=None,\n",
      "dataloader_drop_last=False,\n",
      "dataloader_num_workers=0,\n",
      "dataloader_pin_memory=True,\n",
      "ddp_bucket_cap_mb=None,\n",
      "ddp_find_unused_parameters=None,\n",
      "ddp_timeout=1800,\n",
      "debug=[],\n",
      "deepspeed=None,\n",
      "disable_tqdm=False,\n",
      "do_eval=True,\n",
      "do_predict=False,\n",
      "do_train=False,\n",
      "eval_accumulation_steps=None,\n",
      "eval_delay=0,\n",
      "eval_steps=None,\n",
      "evaluation_strategy=no,\n",
      "fp16=False,\n",
      "fp16_backend=auto,\n",
      "fp16_full_eval=False,\n",
      "fp16_opt_level=O1,\n",
      "fsdp=[],\n",
      "fsdp_config={'fsdp_min_num_params': 0, 'xla': False, 'xla_fsdp_grad_ckpt': False},\n",
      "fsdp_min_num_params=0,\n",
      "fsdp_transformer_layer_cls_to_wrap=None,\n",
      "full_determinism=False,\n",
      "generation_config=None,\n",
      "generation_max_length=None,\n",
      "generation_num_beams=None,\n",
      "gradient_accumulation_steps=1,\n",
      "gradient_checkpointing=False,\n",
      "greater_is_better=None,\n",
      "group_by_length=False,\n",
      "half_precision_backend=auto,\n",
      "hub_model_id=None,\n",
      "hub_private_repo=False,\n",
      "hub_strategy=every_save,\n",
      "hub_token=<HUB_TOKEN>,\n",
      "ignore_data_skip=False,\n",
      "include_inputs_for_metrics=False,\n",
      "jit_mode_eval=False,\n",
      "label_names=None,\n",
      "label_smoothing_factor=0.0,\n",
      "learning_rate=5e-05,\n",
      "length_column_name=length,\n",
      "load_best_model_at_end=False,\n",
      "local_rank=0,\n",
      "log_level=passive,\n",
      "log_level_replica=warning,\n",
      "log_on_each_node=True,\n",
      "logging_dir=./save/iwslt_t5_large/runs/May21_03-31-08_b56da3cfb72c,\n",
      "logging_first_step=False,\n",
      "logging_nan_inf_filter=True,\n",
      "logging_steps=500,\n",
      "logging_strategy=steps,\n",
      "lr_scheduler_type=linear,\n",
      "max_grad_norm=1.0,\n",
      "max_steps=-1,\n",
      "metric_for_best_model=None,\n",
      "mp_parameters=,\n",
      "no_cuda=False,\n",
      "num_train_epochs=3.0,\n",
      "optim=adamw_hf,\n",
      "optim_args=None,\n",
      "output_dir=./save/iwslt_t5_large/,\n",
      "overwrite_output_dir=True,\n",
      "past_index=-1,\n",
      "per_device_eval_batch_size=1,\n",
      "per_device_train_batch_size=8,\n",
      "predict_with_generate=True,\n",
      "prediction_loss_only=False,\n",
      "push_to_hub=False,\n",
      "push_to_hub_model_id=None,\n",
      "push_to_hub_organization=None,\n",
      "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
      "ray_scope=last,\n",
      "remove_unused_columns=True,\n",
      "report_to=['tensorboard'],\n",
      "resume_from_checkpoint=None,\n",
      "run_name=./save/iwslt_t5_large/,\n",
      "save_on_each_node=False,\n",
      "save_safetensors=False,\n",
      "save_steps=500,\n",
      "save_strategy=steps,\n",
      "save_total_limit=None,\n",
      "seed=42,\n",
      "sharded_ddp=[],\n",
      "skip_memory_metrics=True,\n",
      "sortish_sampler=False,\n",
      "tf32=None,\n",
      "torch_compile=False,\n",
      "torch_compile_backend=None,\n",
      "torch_compile_mode=None,\n",
      "torchdynamo=None,\n",
      "tpu_metrics_debug=False,\n",
      "tpu_num_cores=None,\n",
      "use_ipex=False,\n",
      "use_legacy_prediction_loop=False,\n",
      "use_mps_device=False,\n",
      "warmup_ratio=0.0,\n",
      "warmup_steps=0,\n",
      "weight_decay=0.0,\n",
      "xpu_backend=None,\n",
      ")\n",
      "/usr/local/lib/python3.10/dist-packages/datasets/load.py:2547: FutureWarning: 'use_auth_token' was deprecated in favor of 'token' in version 2.14.0 and will be removed in 3.0.0.\n",
      "You can remove this warning by passing 'token=<use_auth_token>' instead.\n",
      "  warnings.warn(\n",
      "Overwrite dataset info from restored data version if exists.\n",
      "05/21/2024 03:31:10 - INFO - datasets.builder - Overwrite dataset info from restored data version if exists.\n",
      "Loading Dataset info from /root/.cache/huggingface/datasets/iwslt2017/iwslt2017-de-en/1.0.0/c18a4f81a47ae6fa079fe9d32db288ddde38451d\n",
      "05/21/2024 03:31:10 - INFO - datasets.info - Loading Dataset info from /root/.cache/huggingface/datasets/iwslt2017/iwslt2017-de-en/1.0.0/c18a4f81a47ae6fa079fe9d32db288ddde38451d\n",
      "Found cached dataset iwslt2017 (/root/.cache/huggingface/datasets/iwslt2017/iwslt2017-de-en/1.0.0/c18a4f81a47ae6fa079fe9d32db288ddde38451d)\n",
      "05/21/2024 03:31:10 - INFO - datasets.builder - Found cached dataset iwslt2017 (/root/.cache/huggingface/datasets/iwslt2017/iwslt2017-de-en/1.0.0/c18a4f81a47ae6fa079fe9d32db288ddde38451d)\n",
      "Loading Dataset info from /root/.cache/huggingface/datasets/iwslt2017/iwslt2017-de-en/1.0.0/c18a4f81a47ae6fa079fe9d32db288ddde38451d\n",
      "05/21/2024 03:31:10 - INFO - datasets.info - Loading Dataset info from /root/.cache/huggingface/datasets/iwslt2017/iwslt2017-de-en/1.0.0/c18a4f81a47ae6fa079fe9d32db288ddde38451d\n",
      "/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "[INFO|configuration_utils.py:668] 2024-05-21 03:31:11,099 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--t5-large/snapshots/150ebc2c4b72291e770f58e6057481c8d2ed331a/config.json\n",
      "[INFO|configuration_utils.py:720] 2024-05-21 03:31:11,100 >> Model config T5Config {\n",
      "  \"_name_or_path\": \"t5-large\",\n",
      "  \"architectures\": [\n",
      "    \"T5ForConditionalGeneration\"\n",
      "  ],\n",
      "  \"d_ff\": 4096,\n",
      "  \"d_kv\": 64,\n",
      "  \"d_model\": 1024,\n",
      "  \"decoder_start_token_id\": 0,\n",
      "  \"dense_act_fn\": \"relu\",\n",
      "  \"dropout_rate\": 0.1,\n",
      "  \"eos_token_id\": 1,\n",
      "  \"feed_forward_proj\": \"relu\",\n",
      "  \"initializer_factor\": 1.0,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"is_gated_act\": false,\n",
      "  \"layer_norm_epsilon\": 1e-06,\n",
      "  \"model_type\": \"t5\",\n",
      "  \"n_positions\": 512,\n",
      "  \"num_decoder_layers\": 24,\n",
      "  \"num_heads\": 16,\n",
      "  \"num_layers\": 24,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"relative_attention_max_distance\": 128,\n",
      "  \"relative_attention_num_buckets\": 32,\n",
      "  \"task_specific_params\": {\n",
      "    \"summarization\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"length_penalty\": 2.0,\n",
      "      \"max_length\": 200,\n",
      "      \"min_length\": 30,\n",
      "      \"no_repeat_ngram_size\": 3,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"summarize: \"\n",
      "    },\n",
      "    \"translation_en_to_de\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"max_length\": 300,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"translate English to German: \"\n",
      "    },\n",
      "    \"translation_en_to_fr\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"max_length\": 300,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"translate English to French: \"\n",
      "    },\n",
      "    \"translation_en_to_ro\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"max_length\": 300,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"translate English to Romanian: \"\n",
      "    }\n",
      "  },\n",
      "  \"transformers_version\": \"4.28.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32128\n",
      "}\n",
      "\n",
      "[INFO|tokenization_auto.py:502] 2024-05-21 03:31:11,165 >> Could not locate the tokenizer configuration file, will try to use the model config instead.\n",
      "[INFO|configuration_utils.py:668] 2024-05-21 03:31:11,245 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--t5-large/snapshots/150ebc2c4b72291e770f58e6057481c8d2ed331a/config.json\n",
      "[INFO|configuration_utils.py:720] 2024-05-21 03:31:11,246 >> Model config T5Config {\n",
      "  \"_name_or_path\": \"t5-large\",\n",
      "  \"architectures\": [\n",
      "    \"T5ForConditionalGeneration\"\n",
      "  ],\n",
      "  \"d_ff\": 4096,\n",
      "  \"d_kv\": 64,\n",
      "  \"d_model\": 1024,\n",
      "  \"decoder_start_token_id\": 0,\n",
      "  \"dense_act_fn\": \"relu\",\n",
      "  \"dropout_rate\": 0.1,\n",
      "  \"eos_token_id\": 1,\n",
      "  \"feed_forward_proj\": \"relu\",\n",
      "  \"initializer_factor\": 1.0,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"is_gated_act\": false,\n",
      "  \"layer_norm_epsilon\": 1e-06,\n",
      "  \"model_type\": \"t5\",\n",
      "  \"n_positions\": 512,\n",
      "  \"num_decoder_layers\": 24,\n",
      "  \"num_heads\": 16,\n",
      "  \"num_layers\": 24,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"relative_attention_max_distance\": 128,\n",
      "  \"relative_attention_num_buckets\": 32,\n",
      "  \"task_specific_params\": {\n",
      "    \"summarization\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"length_penalty\": 2.0,\n",
      "      \"max_length\": 200,\n",
      "      \"min_length\": 30,\n",
      "      \"no_repeat_ngram_size\": 3,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"summarize: \"\n",
      "    },\n",
      "    \"translation_en_to_de\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"max_length\": 300,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"translate English to German: \"\n",
      "    },\n",
      "    \"translation_en_to_fr\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"max_length\": 300,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"translate English to French: \"\n",
      "    },\n",
      "    \"translation_en_to_ro\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"max_length\": 300,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"translate English to Romanian: \"\n",
      "    }\n",
      "  },\n",
      "  \"transformers_version\": \"4.28.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32128\n",
      "}\n",
      "\n",
      "[INFO|tokenization_utils_base.py:1809] 2024-05-21 03:31:11,351 >> loading file spiece.model from cache at /root/.cache/huggingface/hub/models--t5-large/snapshots/150ebc2c4b72291e770f58e6057481c8d2ed331a/spiece.model\n",
      "[INFO|tokenization_utils_base.py:1809] 2024-05-21 03:31:11,351 >> loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--t5-large/snapshots/150ebc2c4b72291e770f58e6057481c8d2ed331a/tokenizer.json\n",
      "[INFO|tokenization_utils_base.py:1809] 2024-05-21 03:31:11,351 >> loading file added_tokens.json from cache at None\n",
      "[INFO|tokenization_utils_base.py:1809] 2024-05-21 03:31:11,351 >> loading file special_tokens_map.json from cache at None\n",
      "[INFO|tokenization_utils_base.py:1809] 2024-05-21 03:31:11,351 >> loading file tokenizer_config.json from cache at None\n",
      "[INFO|configuration_utils.py:668] 2024-05-21 03:31:11,351 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--t5-large/snapshots/150ebc2c4b72291e770f58e6057481c8d2ed331a/config.json\n",
      "[INFO|configuration_utils.py:720] 2024-05-21 03:31:11,352 >> Model config T5Config {\n",
      "  \"_name_or_path\": \"t5-large\",\n",
      "  \"architectures\": [\n",
      "    \"T5ForConditionalGeneration\"\n",
      "  ],\n",
      "  \"d_ff\": 4096,\n",
      "  \"d_kv\": 64,\n",
      "  \"d_model\": 1024,\n",
      "  \"decoder_start_token_id\": 0,\n",
      "  \"dense_act_fn\": \"relu\",\n",
      "  \"dropout_rate\": 0.1,\n",
      "  \"eos_token_id\": 1,\n",
      "  \"feed_forward_proj\": \"relu\",\n",
      "  \"initializer_factor\": 1.0,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"is_gated_act\": false,\n",
      "  \"layer_norm_epsilon\": 1e-06,\n",
      "  \"model_type\": \"t5\",\n",
      "  \"n_positions\": 512,\n",
      "  \"num_decoder_layers\": 24,\n",
      "  \"num_heads\": 16,\n",
      "  \"num_layers\": 24,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"relative_attention_max_distance\": 128,\n",
      "  \"relative_attention_num_buckets\": 32,\n",
      "  \"task_specific_params\": {\n",
      "    \"summarization\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"length_penalty\": 2.0,\n",
      "      \"max_length\": 200,\n",
      "      \"min_length\": 30,\n",
      "      \"no_repeat_ngram_size\": 3,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"summarize: \"\n",
      "    },\n",
      "    \"translation_en_to_de\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"max_length\": 300,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"translate English to German: \"\n",
      "    },\n",
      "    \"translation_en_to_fr\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"max_length\": 300,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"translate English to French: \"\n",
      "    },\n",
      "    \"translation_en_to_ro\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"max_length\": 300,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"translate English to Romanian: \"\n",
      "    }\n",
      "  },\n",
      "  \"transformers_version\": \"4.28.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32128\n",
      "}\n",
      "\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/models/t5/tokenization_t5_fast.py:155: FutureWarning: This tokenizer was incorrectly instantiated with a model max length of 512 which will be corrected in Transformers v5.\n",
      "For now, this behavior is kept to avoid breaking backwards compatibility when padding/encoding with `truncation is True`.\n",
      "- Be aware that you SHOULD NOT rely on t5-large automatically truncating your input to 512 when padding/encoding.\n",
      "- If you want to encode/pad to sequences longer than 512 you can either instantiate this tokenizer with `model_max_length` or pass `max_length` when encoding/padding.\n",
      "- To avoid this warning, please instantiate this tokenizer with `model_max_length` set to your preferred value.\n",
      "  warnings.warn(\n",
      "[INFO|modeling_utils.py:2534] 2024-05-21 03:31:11,428 >> loading weights file model.safetensors from cache at /root/.cache/huggingface/hub/models--t5-large/snapshots/150ebc2c4b72291e770f58e6057481c8d2ed331a/model.safetensors\n",
      "[INFO|configuration_utils.py:575] 2024-05-21 03:31:11,563 >> Generate config GenerationConfig {\n",
      "  \"_from_model_config\": true,\n",
      "  \"decoder_start_token_id\": 0,\n",
      "  \"eos_token_id\": 1,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"transformers_version\": \"4.28.1\"\n",
      "}\n",
      "\n",
      "[INFO|modeling_utils.py:3190] 2024-05-21 03:31:53,081 >> All model checkpoint weights were used when initializing DeployT5ForConditionalGeneration.\n",
      "\n",
      "[INFO|modeling_utils.py:3198] 2024-05-21 03:31:53,084 >> All the weights of DeployT5ForConditionalGeneration were initialized from the model checkpoint at t5-large.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use DeployT5ForConditionalGeneration for predictions without further training.\n",
      "[INFO|configuration_utils.py:537] 2024-05-21 03:31:53,146 >> loading configuration file generation_config.json from cache at /root/.cache/huggingface/hub/models--t5-large/snapshots/150ebc2c4b72291e770f58e6057481c8d2ed331a/generation_config.json\n",
      "[INFO|configuration_utils.py:575] 2024-05-21 03:31:53,147 >> Generate config GenerationConfig {\n",
      "  \"_from_model_config\": true,\n",
      "  \"decoder_start_token_id\": 0,\n",
      "  \"eos_token_id\": 1,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"transformers_version\": \"4.28.1\"\n",
      "}\n",
      "\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/iwslt2017/iwslt2017-de-en/1.0.0/c18a4f81a47ae6fa079fe9d32db288ddde38451d/cache-4b87423bce09feab.arrow\n",
      "05/21/2024 03:31:53 - INFO - datasets.arrow_dataset - Loading cached processed dataset at /root/.cache/huggingface/datasets/iwslt2017/iwslt2017-de-en/1.0.0/c18a4f81a47ae6fa079fe9d32db288ddde38451d/cache-4b87423bce09feab.arrow\n",
      "05/21/2024 03:31:55 - INFO - __main__ - *** Evaluate ***\n",
      "[WARNING|logging.py:280] 2024-05-21 03:31:55,216 >> You're using a T5TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "[INFO|configuration_utils.py:575] 2024-05-21 03:31:55,269 >> Generate config GenerationConfig {\n",
      "  \"_from_model_config\": true,\n",
      "  \"decoder_start_token_id\": 0,\n",
      "  \"eos_token_id\": 1,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"transformers_version\": \"4.28.1\"\n",
      "}\n",
      "\n",
      "100% 40/40 [01:15<00:00,  2.83s/it][WARNING|integrations.py:649] 2024-05-21 03:33:17,929 >> Trainer is attempting to log a value of \"0:00:00\" of type <class 'str'> for key \"train/time_encoder_forward\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n",
      "[WARNING|integrations.py:649] 2024-05-21 03:33:17,930 >> Trainer is attempting to log a value of \"0:01:17\" of type <class 'str'> for key \"train/time_decoder_forward\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n",
      "[WARNING|integrations.py:649] 2024-05-21 03:33:17,930 >> Trainer is attempting to log a value of \"['0:00:06', '0:00:00']\" of type <class 'str'> for key \"train/time_key_value_gen\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n",
      "[WARNING|integrations.py:649] 2024-05-21 03:33:17,930 >> Trainer is attempting to log a value of \"['0:00:14', '0:00:14']\" of type <class 'str'> for key \"train/time_attn\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n",
      "[WARNING|integrations.py:649] 2024-05-21 03:33:17,930 >> Trainer is attempting to log a value of \"0:00:09\" of type <class 'str'> for key \"train/time_ffn\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n",
      "[WARNING|integrations.py:649] 2024-05-21 03:33:17,930 >> Trainer is attempting to log a value of \"0:00:18\" of type <class 'str'> for key \"train/time_confidence\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n",
      "[WARNING|integrations.py:649] 2024-05-21 03:33:17,930 >> Trainer is attempting to log a value of \"['0:00:00', '0:00:00']\" of type <class 'str'> for key \"train/time_exit_key_value_gen\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n",
      "[WARNING|integrations.py:649] 2024-05-21 03:33:17,930 >> Trainer is attempting to log a value of \"['0:00:00', '0:00:00']\" of type <class 'str'> for key \"train/time_exit_attn\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n",
      "[WARNING|integrations.py:649] 2024-05-21 03:33:17,930 >> Trainer is attempting to log a value of \"0:00:00\" of type <class 'str'> for key \"train/time_exit_ffn\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n",
      "[WARNING|integrations.py:649] 2024-05-21 03:33:17,930 >> Trainer is attempting to log a value of \"['0:00:00', '0:00:00']\" of type <class 'str'> for key \"train/time_parallel_key_value_gen\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n",
      "[WARNING|integrations.py:649] 2024-05-21 03:33:17,930 >> Trainer is attempting to log a value of \"['0:00:00', '0:00:00']\" of type <class 'str'> for key \"train/time_parallel_attn\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n",
      "[WARNING|integrations.py:649] 2024-05-21 03:33:17,930 >> Trainer is attempting to log a value of \"0:00:00\" of type <class 'str'> for key \"train/time_parallel_ffn\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n",
      "[WARNING|integrations.py:649] 2024-05-21 03:33:17,930 >> Trainer is attempting to log a value of \"0:00:00\" of type <class 'str'> for key \"train/time_estimate_conf\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n",
      "[WARNING|integrations.py:649] 2024-05-21 03:33:17,930 >> Trainer is attempting to log a value of \"0:00:02\" of type <class 'str'> for key \"train/time_others\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n",
      "100% 40/40 [01:16<00:00,  1.90s/it]\n",
      "***** eval metrics *****\n",
      "  eval_bleu                   =                 0.8452\n",
      "  eval_block_avg              =                   24.0\n",
      "  eval_gen_len                =                   27.3\n",
      "  eval_runtime                =             0:01:22.71\n",
      "  eval_samples                =                     40\n",
      "  eval_samples_per_second     =                  0.484\n",
      "  eval_steps_per_second       =                  0.484\n",
      "  time_attn                   = ['0:00:14', '0:00:14']\n",
      "  time_confidence             =                0:00:18\n",
      "  time_decoder_forward        =                0:01:17\n",
      "  time_encoder_forward        =                0:00:00\n",
      "  time_estimate_conf          =                0:00:00\n",
      "  time_exit_attn              = ['0:00:00', '0:00:00']\n",
      "  time_exit_ffn               =                0:00:00\n",
      "  time_exit_key_value_gen     = ['0:00:00', '0:00:00']\n",
      "  time_ffn                    =                0:00:09\n",
      "  time_key_value_gen          = ['0:00:06', '0:00:00']\n",
      "  time_others                 =                0:00:02\n",
      "  time_parallel_attn          = ['0:00:00', '0:00:00']\n",
      "  time_parallel_ffn           =                0:00:00\n",
      "  time_parallel_key_value_gen = ['0:00:00', '0:00:00']\n",
      "[INFO|modelcard.py:451] 2024-05-21 03:33:18,030 >> Dropping the following result as it does not have all the necessary fields:\n",
      "{'task': {'name': 'Translation', 'type': 'translation'}, 'dataset': {'name': 'iwslt2017 iwslt2017-de-en', 'type': 'iwslt2017', 'args': 'iwslt2017-de-en'}}\n"
     ]
    }
   ],
   "source": [
    "!python -m torch.distributed.run --nproc_per_node=1 \\\n",
    "    src/run_translation.py \\\n",
    "    --model_name_or_path t5-large \\\n",
    "    --do_eval \\\n",
    "    --dataset_name iwslt2017 \\\n",
    "    --dataset_config_name iwslt2017-de-en \\\n",
    "    --output_dir ./save/iwslt_t5_large/ \\\n",
    "    --per_device_eval_batch_size 1 \\\n",
    "    --deploy_scenario True \\\n",
    "    --use_synchronize True \\\n",
    "    --overwrite_output_dir \\\n",
    "    --predict_with_generate \\\n",
    "    --source_prefix \"translate German to English: \" \\\n",
    "    --output_hidden_states_decoder True \\\n",
    "    --use_early_exit True \\\n",
    "    --source_lang \"de\" \\\n",
    "    --target_lang \"en\" \\\n",
    "    --intermediate_loss_fn weighted_ce \\\n",
    "    --exit_conf_type last_three_top_prob_heuristic \\\n",
    "    --exit_position_temp 4 \\\n",
    "    --exit_conf_threshold 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "id": "D4YxzVawKfO7"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
